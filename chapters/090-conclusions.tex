% !TEX root = ../phd-thesis.tex

\chapter{Conclusions}\label{ch:conclusions}
Since proving \emph{incorrectenss} via under-approximation is a concept that recently gained lots of interest, in this thesis we explored the exciting sub-field of combining over and under-approximation in effective ways to improve the ability of both to prove a program correct or incorrect, respectively.

We first surveyed the literature searching for other works doing just that, explicitly or not. We found that most works in this direction (the embedding in KATs, $\LCLA$ and OL) are very recent, but PDR stood out as older. While it wasn't probably understood in this terms at the time, the success of PDR as a model checker shows the potential of this approach.

Since Abstract Interpretation is successful in over-approximation, our first question was whether it could be a good way to perform under-approximation. It turns out the classical formulation based on Galois connection is hard to turn for under-approximation. Given this limitation, we instead consider logical frameworks for under-approximation, and introduce a backward analogous to IL, which we dub SIL. Moreover, by comparing SIL, IL and two over-approximation logics (HL and NC) we further out understanding of the relations between over and under-approximation.

We apply the insight from previous chapters in extending two known techniques.
First, we propose a new framework for $\LCLA$ that is able to prove intensional properties of programs by changing the domain in which (part of) the analysis is performed. We also present a way to circumvent the requirements of best abstraction, basically by performing a sanity check which guarantees the soundness of the analysis. Lastly, we turn $\LCLA$ over for backward analysis by using SIL instead of IL as the underlying program logic: the duality between the two ensures everything works just fine.
Second, we introduce a new PDR-like algorithm which uses an adjoint (roughly corresponding to the backward semantics) to speed up the search for counterexamples. This algorithm accounts for a theory of heuristics, which allows us to schematize, compare and have some reference points for devising new heuristics. Moreover, we lift the assumption that there exists an adjoint by resorting to lower sets, and instantiate it for probabilistic systems (for which there is no right adjoint). The experiments suggest that our approach is overall better than other PDR-like algorithms.

The main message of the thesis is the non-triviality of the over/under-approximation interaction, but also the potential of this approach. As (we hope) shown by our results, the ways to efficiently exchange meaningful information between over and under-approximation are not easy to find. In our opinion, this is partly due to the symmetry between over and under-approximation being less sharp than it appears at first glance. However, if understood correctly, this information sharing can improve the combined search for correctness and incorrectness: under-approximation can guide the refinement process of the over-approximation, while over-approximation prevents under-approximation to forget too much.

In the future, we would like to deepen our understanding of the combination, possibly by studying the details of the interaction in approaches such as Outcome Logic, to be able to apply it in more and more settings.
