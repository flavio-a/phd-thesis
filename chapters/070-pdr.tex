% !TEX root = ../phd-thesis.tex

\chapter{AdjointPDR}\label{ch:pdr}
In this chapter\todo{a bit more connections to other stuff (in the conclusions as well)} we study a new PDR-like algorithm (see Section~\ref{sec:sota:pdr}). Differently than previous approaches, our main tool are \emph{adjunctions}, which we use extensively in our development. We propose a first algorithm, \APDR{}, which exploits an adjoint $g$ to the function $f$ (which roughly identify the backward semantics of $f$) to quicken the counterexample search. This first algorithm allows us to devise a theory of heuristics to better understand and compare them.
However, to apply \APDR{} the right adjoint $g$ to the forward semantics $f$ must exist, and this is not always the case. To get rid of this constraint, we propose \ADPDR{}, a variation of \APDR{} which lift the problem to lower sets, where it is always possible to define this adjoint.
Lastly, we propose yet another variation of the algorithm, \APDRAI{}, which can instantiate both \APDR{} and \ADPDR{}. We implemented this latter algorithm, and compared it against other PDR-like algorithms and state-of-the-art tools with encouraging results.

The content of this chapter is based on~\cite{KABBGH23}.

\section{Overview}
Category theory has recognized adjunctions $f \dashv g$ as fundamental concepts appearing across various mathematical domains~\cite{Lawvere69}. Adjointness is prevalent in various branches of computer science as well, including abstract interpretation and functional programming~\cite{Levy2004}. In our development, we employ adjoints in two distinct ways:
\begin{itemize}
	\item (Forward-Backward Adjoint) $f$ characterizes the \emph{forward semantics} of a transition system, while $g$ represents the \emph{backward} semantics.
	\item (Abstraction-Concretization Adjoint) $C$ denotes a concrete semantic domain, while $A$ is an abstract one, akin to abstract interpretation. An adjoint allows us to translate a fixed-point problem from $C$ to $A$.
\end{itemize}

The problem we address is the standard lattice-theoretical formulation of safety problems, namely whether the least fixed point of a continuous map $b$ over a complete lattice $L$ is below a given element $p \in L$: $\mu b\leq_{?} p$.

The first algorithm we present, \APDR{}, assumes the existence of an element $i \in L$ and two adjoints $f \dashv g \colon L \to L$, representing respectively initial states, forward semantics and backward semantics such that $b(x) = f(x) \lor i$ for all $x \in L$.

\[
\xymatrix{
L \ar@/_1.5ex/[r]_-{g}^-\bot &L\ar@/_1.5ex/[l]_-{f}
}
\]

Under this assumption, Knaster-Tarski Theorem~\ref{th:bg:knaster-tarski} yields the equivalences:
\[
\mu b\le p
\quad \Leftrightarrow\quad
\mu (f\lor i)\le p
\quad \Leftrightarrow\quad
i \le \nu (g \land p),
\]

where $\mu (f\lor i)$  and $\nu (g \land p)$ are, by Kleene Theorem~\ref{th:bg:kleene}, the limits of the \emph{initial} and \emph{final} chains illustrated below.
\[
\bot \le i \le f(i)\lor i \le  \cdots
\qquad\qquad\qquad
\cdots \le g(p)\land p \le p \le \top
\]

The distinguishing feature of \APDR{} is to take as a negative sequence (that is a sequential construction of potential counterexamples) an over-approximation of the final chain. This crucially differs from the negative sequence of other PDR-like algorithm, which is an under-approximation of the computed positive chain.

\APDR{} is sound (Theorem~\ref{th:pdr:soundness}) and does not loop (Proposition~\ref{prop:pdr:progres}), but since the problem $\mu b \le_? p$ is not always decidable, we cannot prove termination. Nevertheless, \APDR{} allows for a formal theory of heuristics that are essential when instantiating the algorithm to concrete problems. The theory prescribes the choices to obtain the boundary executions, using initial and final chains (Proposition~\ref{prop:pdr:negativesequencefinalchain}); it thus identifies a class of heuristics guaranteeing termination when answers are negative (Theorem~\ref{th:pdr:negativetermination}).

In general, \APDR{}'s assumption of a forward-backward adjoint $f \dashv g$ does not hold, especially in probabilistic settings. Our second algorithm \ADPDR{} circumvents this problem by extending the lattice for the negative sequence, from $L$ to the lattice $L^{\downarrow}$ of \emph{lower sets} in $L$. Specifically, by using the second form of adjoints, namely an abstraction-concretization pair, the problem $\mu b \le_{?} p$ in $L$ can be translated to an equivalent problem on $b^{\downarrow}$ in $L^\downarrow$, for which an adjoint $b^\downarrow \dashv b^\downarrow_r$ always exists.
\[
\xymatrix{
L \lloop{b} \ar@/_1.5ex/[r]_-{(-)^\downarrow}^-\bot
&L^\downarrow \rloop{b^\downarrow
	\, \dashv\, b^\downarrow_r
} \ar@/_1.5ex/[l]_-{\bigsqcup}
}
\]
This allows us to run \APDR{} in the lattice $L^\downarrow$. We then notice that the search for a positive chain can be conveniently restricted to principals in $L^\downarrow$, which have representatives in $L$. The resulting algorithm, using $L$ for positive chains and $L^\downarrow$ for negative sequences, is \ADPDR{}.

The use of lower sets for the negative sequence is a key advantage. It not only avoids the restrictive assumption of backward adjoint $g$, but also enables a more thorough search for counterexamples. {\ADPDR} can simulate stepwise LT-PDR (Theorem~\ref{th:pdr:LT-PDR-instance-ADPDR}), but it is more general since a single negative sequence in {\ADPDR} potentially represents multiple (Proposition~\ref{prop:pdr:multipleLTPDR}) or even all (Proposition~\ref{prop:pdr:LTPDRfinal}) negative sequences of LT-PDR.

Our lattice-theoretic algorithms yield many concrete instances: the original IC3/PDR as well as Reverse PDR~\cite{SS17} are instances of \APDR{} with $L$ being the powerset of the state space; since LT-PDR can be simulated by \ADPDR{}, the latter generalizes all instances in~\cite{KUKSH22}.
As a notable instance, we apply \ADPDR{} to MDPs, specifically to decide if the maximum reachability probability \cite{BK08} is below a given threshold. Here the lattice $L=[0,1]^S$ is that of fuzzy predicates over the state space $S$. Our theory provides guidance to devise two heuristics, for which we prove negative termination (Corollary~\ref{cor:pdr:ADPDRtermination}).

We implement this latter instance in Haskell. However, the implementation is not based on \ADPDR{} directly, but rather on a third algorithm, \APDRAI{}. This can be understood as a generalisation of both \APDR{} and \ADPDR{} to a more abstract setting:
\[
\xymatrix{
(L, \le_L) \lloop{b} \ar[r]^-{\gamma}
&(C, \le_C) \rloop{\overline{b} \dashv \overline{b}_r}
}
\]
where $\gamma \colon L \to C$ is an order embedding and $b,\overline{b}$ and $\gamma$ are required to satisfy a condition that is known in the setting of abstract interpretation as \emph{forward completeness}~\cite{GRS00}.

We experimentally evaluate our implementation. We compare it against existing probabilistic PDR algorithms (PrIC3~\cite{BJKKMS20}, LT-PDR~\cite{KUKSH22}) and a non-PDR one (Storm~\cite{DJKV17}). The performance of \ADPDR{} is encouraging---it supports the potential of PDR algorithms in probabilistic model checking. The experiments also indicate the importance of having a variety of heuristics, and thus the value of our adjoint framework that helps in coming up with those.
Additionally, we found that abstraction features of Haskell allow us to code lattice-theoretic algorithms almost literally ($\sim$100 lines). Implementing a few heuristics takes another $\sim$240 lines. This way, we found that mathematical abstraction can directly help in easing implementation effort.

\section{Adjoint PDR}\label{sec:pdr:APDR}

\begin{figure}[t]
	% Syntactic invariants
	\begin{minipage}{.35\linewidth}
		\small
		\begin{align}
			\quad x_0 = \bot \tag{I0}\label{eq:pdr:x0bot} \\
			1\leq k \leq n \tag{I1} \label{eq:pdr:invi}   \\
			\forall j\in[0, n-2]\text{, }x_j \le x_{j+1} \tag{I2}\label{eq:pdr:positivechain}
		\end{align}
	\end{minipage}%
	% General invariants
	\begin{minipage}{.65\linewidth}
		\small
		\begin{align}
			\forall j \in [k, n - 1] \text{, } x_j \not\le y_j \tag{PN} \label{eq:pdr:positivenegative}                                              \\
			\forall j \in [0, n-1] \text{, } (f \lor i)^j (\bot) \le x_j \le (g \land p)^{n-1-j} (\top) \tag{A1} \label{eq:pdr:positiveinitialfinal} \\
			\forall j \in [1, n-1] \text{, } x_{j-1} \le g^{n-1-j}(p) \tag{A2} \label{eq:pdr:positivefinal}                                          \\
			\forall j\in[k,n-1]\text{, }g^{n-1-j}(p) \le y_j \tag{A3} \label{eq:pdr:negativefinal}
		\end{align}
	\end{minipage}
	% Positive chain invariants
	\begin{minipage}{.4\linewidth}
		\small
		\begin{align}
			i \le x_1 \tag{P1} \label{eq:pdr:Ix1}                                            \\
			x_{n-2} \le p \tag{P2}\label{eq:pdr:xP}                                          \\
			\forall j\in[0, n-2]\text{, }f(x_j) \le x_{j+1} \tag{P3}\label{eq:pdr:positiveF} \\
			\forall j\in[0, n-2]\text{, }x_j \le g(x_{j+1}) \tag{P3a} \label{eq:pdr:positiveG}
		\end{align}
	\end{minipage}%
	% Negative sequence invariants
	\begin{minipage}{.6\linewidth}
		\small
		\begin{align}
			\text{If }\vec{y}\neq \varepsilon\text{ then }p \le y_{n-1} \tag{N1}\label{eq:pdr:Pepsilon} \\
			\forall j\in[k,n-2]\text{, }g(y_{j+1}) \le y_j \tag{N2}\label{eq:pdr:negativeG}
		\end{align}
	\end{minipage}

	\vspace*{0.5em}
	\caption{Invariants of {\APDR}.}
	\label{fig:pdr:invariants}
\end{figure}

In this section we introduce {\APDR}, an algorithm that takes in input a tuple $(i,f,g,p)$ with $i,p\in L$ and $f\dashv g \colon L\to L$ and, if it terminates, it returns true whenever $\lfp (f \lor i) \le p$ and false otherwise. The algorithm manipulates two sequences of elements of $L$:
\[
\vec{x} \eqdef x_0, \dots, x_{n-1} \qquad \vec{y} \eqdef y_k, \dots y_{n-1}
\]
of length $n$ and $n-k$, respectively. These satisfy, through the executions of {\APDR}, the invariants in Figure~\ref{fig:pdr:invariants}. By \eqref{eq:pdr:positiveinitialfinal}, $x_j$ over-approximates the $j$-th element of the initial chain, namely $(f \lor i)^j(\bot) \le x_j$, while, by \eqref{eq:pdr:negativefinal}, the $j$-indexed element $y_j$ of $\vec{y}$ over-approximates $g^{n-j-1}(p)$ that, borrowing the terminology of Example~\ref{ex:sota:ts}, is the set of states which are safe in $n-j-1$ transitions.
Moreover, by~\eqref{eq:pdr:positivenegative}, the element $y_j$ witnesses that $x_j$ is unsafe, i.e., that $x_j \nleq g^{n-1-j}(p)$ or equivalently $f^{n-j-1}(x_j) \nleq p$.
Notably, $\vec{x}$ is a positive chain and $\vec{y}$ a negative sequence, according to the definitions below.

\begin{definition}[positive chain] \label{def:pdr:posi_seq}
	A \emph{positive chain} for $\lfp (f \lor i) \le p$ is a finite chain $x_0 \le \dots \le x_{n-1}$ in $L$ of length $n \geq 2$ which satisfies \eqref{eq:pdr:Ix1}, \eqref{eq:pdr:xP}, \eqref{eq:pdr:positiveF} in Figure~\ref{fig:pdr:invariants}.
	It is \emph{conclusive} if $x_{j+1} \le x_j$ for some $j \leq n-2$.
\end{definition}

In a conclusive positive chain, $x_{j+1}$ provides an invariant for $f \lor i$ and thus, by \eqref{eq:bg:coinductionproofprinciple}, $\lfp (f \lor i) \le p$ holds. So, when $\vec{x}$ is conclusive, {\APDR} returns true.

\begin{definition}[negative sequence] \label{def:pdr:neg_seq}
	A \emph{negative sequence} for $\lfp (f \lor i) \le p$ is a finite sequence $ y_k, \dots, y_{n-1}$ in $L$ with $1 \leq k \leq n$ which satisfies \eqref{eq:pdr:Pepsilon} and \eqref{eq:pdr:negativeG} in Figure~\ref{fig:pdr:invariants}.
	It is \emph{conclusive} if $k=1$ and $i \nleq y_1$.
\end{definition}

When $\vec{y}$ is conclusive, {\APDR} returns false as $y_1$ provides a counterexample: \eqref{eq:pdr:Pepsilon} and \eqref{eq:pdr:negativeG} entail \eqref{eq:pdr:negativefinal} and thus $i \nleq y_1 \ge g^{n-2}(p)$, so that $g^{n-2}(p) \ge \gfp (g \land p)$ and thus $i \nleq \gfp(g \land p)$. By~\eqref{eq:bg:adjoint-fixpoint}, $\lfp(f \lor i) \nleq p$.

\begin{figure}[t]
	\begin{center}
		\underline{{\APDR} $(i,f,g,p)$}
		{\small
			\begin{codeNT}
<INITIALISATION>
  $( \vec{x} \| \vec{y} )_{n,k}$ := $(\bot,\top\|\varepsilon)_{2,2}$
<ITERATION>						           % $\vec{x},\vec{y}$  not conclusive
  case $( \vec{x} \| \vec{y} )_{n,k}$ of
	   $\vec{y}=\varepsilon$ And $x_{n-1} \le p$     :                    %(Unfold)
			$( \vec{x} \| \vec{y} )_{n,k}$ := $( \vec{x}, \top \| \varepsilon )_{n+1,n+1}$
	   $\vec{y}=\varepsilon$ And $x_{n-1} \not\le p$    :                     %(Candidate)
			choose $z\in L$ st  $x_{n-1} \not\le z$ And  $p \le z$;
			$( \vec{x} \| \vec{y} )_{n,k}$ := $( \vec{x} \| z )_{n,n-1}$
	   $\vec{y} \neq \varepsilon$ And $f(x_{k-1}) \not \le y_k$ :                        %(Decide)
			choose $z \in L$ st $x_{k-1} \not \le z$ And $g(y_k) \le z$;
			$(\vec{x} \| \vec{y} )_{n,k}$ := $(\vec{x} \| z , \vec{y} )_{n,k-1}$
	   $\vec{y} \neq \varepsilon$ And $f(x_{k-1}) \le y_k$ :                        %(Conflict)
			choose $z \in L$ st $z \le y_k$ And $(f \lor i)(x_{k-1} \land z) \le z$;
			$(\vec{x} \| \vec{y} )_{n,k}$ := $(\vec{x} \land_k z \| \mathsf{tail}(\vec{y}) )_{n,k+1}$
  endcase
<TERMINATION>
	if $\exists j\in [0,n-2]\,.\, x_{j+1} \le x_j$ then return true		 % $\vec{x}$ conclusive
	if $i \not \le y_1$ then return false							% $\vec{y}$ conclusive
\end{codeNT}
		}
	\end{center}
	\caption{{\APDR} algorithm checking $\lfp(f \lor i) \le p$.}\label{fig:pdr:apdr}
\end{figure}

The pseudocode of the algorithm is in Figure~\ref{fig:pdr:apdr}, where we write $( \vec{x} \| \vec{y} )_{n,k}$ to compactly represents the state of the algorithm: the pair $(n,k)$ is called the \emph{index} of the state, with $\vec{x}$ of length $n$ and $\vec{y}$ of length $n-k$. When $k = n$, $\vec{y}$ is the empty sequence $\varepsilon$. For any $z \in L$, we write $\vec{x}, z$ for the chain $x_0, \dots, x_{n-1}, z$ of length $n+1$ and $z, \vec{y}$ for the sequence $z, y_k, \dots y_{n-1}$ of length $n-(k-1)$. Moreover, we write $\vec{x} \land_j z$ for the chain $x_0 \land z, \dots, x_j \land z, x_{j+1}, \dots , x_{n-1}$. Finally, $\mathsf{tail}(\vec{y})$ stands for the tail of $\vec{y}$, namely $y_{k+1}, \dots y_{n-1}$ of length $n-(k+1)$.

The algorithm starts in the initial state $s_0 \eqdef ( \bot, \top \| \varepsilon )_{2,2}$ and, unless one of $\vec{x}$ and $\vec{y}$ is conclusive, iteratively applies one of the four mutually exclusive rules: (Unfold), (Candidate), (Decide) and (Conflict).
The rule (Unfold) extends the positive chain by one element when the negative sequence is empty and the positive chain is under $p$; since the element introduced by (Unfold) is $\top$, its application typically triggers rule (Candidate) that starts the negative sequence with an over-approximation of $p$. Recall that the role of $y_j$ is to witness that $x_j$ is unsafe. After (Candidate) either (Decide) or (Conflict) are possible: if $y_k$ witnesses that, besides $x_k$, also $f(x_{k-1})$ is unsafe, then (Decide) is used to further extend the negative sequence to witness that $x_{k-1}$ is unsafe; otherwise, the rule (Conflict) improves the precision of the positive chain in such a way that $y_k$ no longer witnesses $x_k \land z$ unsafe and, thus, the negative sequence is shortened.
Note that, in (Candidate), (Decide) and (Conflict), the element $z \in L$ is chosen among a set of possibilities, thus {\APDR} is nondeterministic.

To illustrate the executions of the algorithm, we adopt a labeled transition system notation. Let $\states \eqdef \{( \vec{x} \| \vec{y} )_{n,k} \mid n \geq 2$, $k\leq n$, $\vec{x}\in L^n$ and $\vec{y}\in L^{n-k}\}$ be the set of all possible states of {\APDR}. We call $( \vec{x} \| \vec{y} )_{n,k} \in \states$ \emph{conclusive} if $\vec{x}$ or $\vec{y}$ are such.
When $s \in \states$ is not conclusive, we write $s \trz{D}{}$ to mean that $s$ satisfies the guards in the rule (Decide), and $s \trz{D}{z} s'$ to mean that, being (Decide) applicable, {\APDR} moves from state $s$ to $s'$ by choosing $z$. Similarly for the other rules: the labels $\mathit{Ca}$, $\mathit{Co}$ and $U$ stands for (Candidate), (Conflict) and (Unfold), respectively.
When irrelevant we omit to specify labels and choices and we just write $s \tr{} s'$.
As usual $\ttp{}$ stands for the transitive closure of $\tr{}$ and $\ttr{}$ stands for the reflexive and transitive closure of $\tr{}$.

\begin{example}\label{ex:pdr:simple-ts}
	Consider the safety problem in Example~\ref{ex:sota:ts}. Below we illustrate two possible computations of {\APDR} that differ for the choice of $z$ in (Conflict).
	The first run is conveniently represented as the following series of transitions.
	\par\nobreak
	{
		\setlength{\abovedisplayskip}{0pt}
		\setlength{\belowdisplayskip}{6pt}
		\setlength{\abovedisplayshortskip}{0pt}
		\setlength{\belowdisplayshortskip}{3pt}
		\begin{align*}
			                     & ( \emptyset, S \| \varepsilon )_{2,2}
			\tr{\mathit{Ca}}_{P} ( \emptyset, S \| P )_{2,1}
			\tr{\mathit{Co}}_{I} ( \emptyset, I \| \varepsilon )_{2,2}                  \\[-.4em]
			\tr{U}               & ( \emptyset, I, S \| \varepsilon )_{3,3}
			\tr{\mathit{Ca}}_{P} ( \emptyset, I, S \| P )_{3,2}
			\tr{\mathit{Co}}_{S_2} ( \emptyset, I, S_2 \| \varepsilon )_{3,3}           \\[-.4em]
			\tr{U} %
			\tr{\mathit{Ca}}_{P} & ( \emptyset, I, S_2, S \| P )_{4,3}
			\tr{\mathit{Co}}_{S_3} ( \emptyset, I, S_2, S_3 \| \varepsilon )_{4,4}      \\[-.4em]
			\tr{U} %
			\tr{\mathit{Ca}}_{P} & ( \emptyset, I, S_2, S_3, S \| P )_{5,4}
			\tr{\mathit{Co}}_{S_4} ( \emptyset, I, S_2, S_3, S_4 \| \varepsilon )_{5,5} \\[-.4em]
			\tr{U} %
			\tr{\mathit{Ca}}_{P} & ( \emptyset, I, S_2, S_3, S_4, S \| P )_{6,5}
			\tr{\mathit{Co}}_{S_4} ( \emptyset, I, S_2, S_3, S_4, S_4 \| \varepsilon )_{6,6}
		\end{align*}
	}

	\noindent
	The last state returns true since $x_4 = x_5 = S_4$. Observe that the chain $\vec{x}$, with the exception of its last element $x_{n-1}$, is exactly the initial chain of $(T \cup I)$, i.e., $x_j$ is the set of states reachable in at most $j-1$ steps. In the second computation, the elements of $\vec{x}$ are roughly those of the final chain of $(G \cap P)$. More precisely, after (Unfold) or (Candidate), $x_{n-j}$ for $j < n-1$ is the set of states which only reach safe states within $j$ steps.
	\par\nobreak
	{
		\setlength{\abovedisplayskip}{0pt}
		\setlength{\belowdisplayskip}{6pt}
		\setlength{\abovedisplayshortskip}{0pt}
		\setlength{\belowdisplayshortskip}{3pt}
		\begin{align*}
			                     & ( \emptyset, S \| \varepsilon )_{2,2}
			\tr{\mathit{Ca}}_{P} ( \emptyset, S \| P )_{2,1}
			\tr{\mathit{Co}}_{P} ( \emptyset, P \| \varepsilon )_{2,2}      \\[-.4em]
			\tr{U} %
			\tr{\mathit{Ca}}_{P} & ( \emptyset, P, S \| P )_{3,2}
			\tr{D}_{S_4} ( \emptyset, P, S \| S_4, P )_{3,1}
			\tr{\mathit{Co}}_{S_4} ( \emptyset, S_4, S \| P )_{3,2}
			\tr{\mathit{Co}}_{P} ( \emptyset, S_4, P \| \varepsilon )_{3,3} \\[-.4em]
			\tr{U} %
			\tr{\mathit{Ca}}_{P} & ( \emptyset, S_4, P, S \| P )_{4,3}
			\tr{D}_{S_4} ( \emptyset, S_4, P, S \| S_4, P )_{4,2}
			\tr{\mathit{Co}}_{S_4} ( \emptyset, S_4, S_4, S \| P )_{4,3}
		\end{align*}
	}

	\noindent
	Observe that, by invariant \eqref{eq:pdr:positiveinitialfinal}, the values of $\vec{x}$ in the two runs are, respectively, the least and the greatest values for all possible computations of {\APDR}.
\end{example}

\section{Properties of {\APDR}}\label{sec:pdr:properties}

In this section we prove the main properties of {\APDR}: (1) any returned result is valid (soundness); (2) although {\APDR} can diverge, any state is never visited twice (called progression); (3) certain heuristics can be used to guarantee termination when a counterexample exists (called negative termination).

\subsection{Invariants}\label{sec:pdr:soundness}
The proofs of the properties of {\APDR} rely on the properties in Figure~\ref{fig:pdr:invariants}. In this section, we prove that such properties are invariants:
\begin{prop}\label{prop:pdr:invariants-valid}
	For any possible choice performed by {\APDR}, the properties in Figure~\ref{fig:pdr:invariants} hold in all reachable states of the algorithm.
\end{prop}

In proving the invariants, some observations on the choice of element $z$ naturally emerge.
First, the proofs of the three invariants \eqref{eq:pdr:x0bot}, \eqref{eq:pdr:invi} and \eqref{eq:pdr:positivechain} do not rely on the properties of the chosen element $z \in L$.
For proving the invariants of the positive chain (\eqref{eq:pdr:Ix1}, \eqref{eq:pdr:xP}, \eqref{eq:pdr:positiveF} and \eqref{eq:pdr:positiveG}) and of the negative sequence (\eqref{eq:pdr:Pepsilon} and \eqref{eq:pdr:negativeG}) we only exploit the \emph{second} constraints on $z$ of each rule of the algorithm, namely $p \le z$ in (Candidate), $g(y_k) \le z$ in (Decide), and $(f \lor i)(x_{k-1} \land z) \le z$ in (Conflict).
Lastly, the \emph{first} constraint on $z$ in each rule ensures the remaining invariants (\eqref{eq:pdr:positivenegative}, \eqref{eq:pdr:positiveinitialfinal}, \eqref{eq:pdr:positivefinal} and \eqref{eq:pdr:negativefinal}), which in turn are key to the proof of progression.

To make the proofs more uniform and compact, we adopt the following notation: for a state $s$ and a property $(Q)$ we will write $s \models (Q)$ to mean that $(Q)$ holds in $s$. We will often show that $(Q)$ is an invariant inductively: namely, we will prove
\begin{itemize}
	\item[(a)] $s_0 \models (Q)$ and
	\item[(b)] if $s \models (Q)$ and $s \tr{ } s'$, then $s'\models (Q)$.
\end{itemize}
Hereafter, we fix $s=( \vec{x} \| \vec{y} )_{n,k}$ and $s'=( \vec{x}' \| \vec{y}' )_{n',k'}$. As usual we will write $x_j$ and $y_j$ for the elements of $\vec{x}$ and $\vec{y}$. For the elements of $\vec{x}'$ and $\vec{y}'$, we will write $x_j'$ and $y_j'$. Throughout the proofs, we will avoid to repeat every time in (b) that $s \models (Q)$, and we will just write $\stackrel{{(Q)}}{=}$ or $\stackrel{{(Q)}}{{\le}}$ whenever using such hypothesis. Moreover in (b) we will avoid to specify those cases that are trivial: for instance, for the properties that only concerns the positive chain $\vec{x}$, e.g., \eqref{eq:pdr:x0bot} and \eqref{eq:pdr:positiveF}, it is enough to check the property (b) for $s \tr{U} s'$ and $s \tr{\mathit{Co}} s'$, since $s \tr{D} s'$ and $s \tr{\mathit{Ca}} s'$ only modify the negative sequence $\vec{y}$.
We illustrate below only the most interesting cases. The remaining ones are in Appendix~\ref{ch:app:pdr}.

\begin{proof}[Proof sketch]
	\invariantproof{\eqref{eq:pdr:x0bot}}{$x_0 = \bot$}
	\begin{itemize}
		\item[(a)] In $s_0$, $x_0= \bot$.
		\item[(b)] If $s \tr{U} s'$, then $x_0' = x_0 \stackrel{{\eqref{eq:pdr:x0bot}}}{=}\bot$. \\
		      If $s \trz{\mathit{Co}}{z} s'$, then $x_0'=x_0 \land z \stackrel{{\eqref{eq:pdr:x0bot}}}{=} \bot \land z = \bot$.
	\end{itemize}

	\invariantproof{\eqref{eq:pdr:invi}}{$1\leq k \leq n$}
	\begin{itemize}
		\item To prove that $1 \leq k$, observe that $k$ is initialised at $2$ and that it is only decremented by $1$. When $k=1$, $\vec{y}\neq \varepsilon$. By~\eqref{eq:pdr:x0bot} $x_0=\bot$. Since $f$ is a left adjoint, $f(\bot) = \bot$. Thus, $f(x_0) \le y_1$. This means that either the state is conclusive and the algorithm returns, or (Conflict) is enabled and thus $k$ is incremented.
		\item To prove that $k \leq n$, observe that $k$ is incremented only by $1$. When $k=n$, the algorithm does either (Unfold) or (Candidate). In the latter case, $k$ is decremented. In the former, both $n$ and $k$ are incremented.
	\end{itemize}

	\invariantproof{\eqref{eq:pdr:positiveF}}{$\forall j\in[0, n-2] \text{, } f(x_j) \le x_{j+1}$}
	\begin{itemize}
		\item[(a)] In $s_0$, since $n=2$ one needs to check only the case $j=0$: $f(x_0) \le \top = x_1$.
		\item[(b)] If $s \tr{U} s'$, then $f(x_j') =f(x_j) \stackrel{{\eqref{eq:pdr:positivechain}}}{\le} x_{j+1} =x_{j+1}'$ for all $j\in[0,n-2]$. For $j=n-1$, $f(x_{n-1}') = f(x_{n-1}) \le \top = x_{j+1}'$. Since $n'=n+1$, then $\forall j\in [0,n'-2] \text{, } f(x_j')\le x_{j+1}'$. \\
		      If $s \trz{\mathit{Co}}{z} s'$, since $f(x_{k-1} \land z) \le z$, then by \eqref{eq:pdr:positivechain} and monotonicity of $f$ it holds that $\forall j\in [0,k-1]$, $f(x_{j} \land z) \le z$. Since $f(x_j \land z) \le f(x_j) \stackrel{{\eqref{eq:pdr:positiveF}}}{\le} x_{j+1}$, it holds that $f(x_j \land z) \le x_{j+1} \land z $ for all $j\in[0, k-1]$. With this observation is immediate to conclude that $\forall j\in[0,n'-2] \text{, }f(x_j')\le x_{j+1}'$.
	\end{itemize}

	\invariantproof{\eqref{eq:pdr:negativeG}}{$\forall j\in[k,n-2] \text{, } g(y_{j+1}) \le y_j$}\newline
	The case of (Conflict) is trivial: the negative sequence $\vec{y}$ is truncated in the rule (Conflict), and if the invariant holds for $\vec{y}$ then it holds for its tail $\mathsf{tail}(\vec{y})$ as well.
	\begin{itemize}
		\item[(a)] In $s_0$, $k=2$ and $n=2$. Thus \eqref{eq:pdr:negativeG} trivially holds.
		\item[(b)] If $s \tr{\mathit{Ca}} s'$, then $k'=n-1$ and thus \eqref{eq:pdr:negativeG} trivially holds.\\
		      If $s \trz{D}{z} s'$, since $z \ge g(y_k)$ and $k'=k-1$, then $y_{k'}' = y_{k-1}' = z \ge g(y_k) = g(y_k')= g(y_{k'+1}')$. For $j\in[k'+1,n-2]$, namely for $j\in[k,n-2]$, it holds that $y'_j= y_j\stackrel{{\eqref{eq:pdr:negativeG}}}{\ge} g(y_{j+1})=g(y_{j+1}')$. Thus, $\forall j \in [k',n-2] \text{, }g(y_{j+1}') \le y_j'$.
	\end{itemize}

	\invariantproof{\eqref{eq:pdr:positivenegative}}{$\forall j \in [k, n - 1] \text{, } x_j \not\le y_j$}
	\begin{itemize}
		\item[(a)] In $s_0$, $k=n$ and thus \eqref{eq:pdr:positivenegative} trivially holds.
		\item[(b)] If $s \tr{U} s'$, then $k'=n'$ and thus \eqref{eq:pdr:positivenegative} trivially holds.\\
		      If $s \trz{\mathit{Ca}}{z} s'$, since $x_{n-1} \not\le z$, $x'_{n-1}=x_{n-1}$ and $k'=n'-1=n-1$, then $x'_{n'-1} = x_{n-1}\not\le z = y'_{n'-1}$. \\
		      If $s \trz{D}{z} s'$, since $x_{k-1} \not\le z$, then $x_{k-1}'=x_{k-1} \not\le z = y'_{k-1}$. Moreover, $\forall j \in [k, n - 1]$,
		      $x_j' = x_j \stackrel{{\eqref{eq:pdr:positivenegative}}}{ \not\le} y_j = y_j'$. Thus, $\forall j \in [k', n' - 1] \text{, } x_j' \not\le y_j'$.\\
		      If $s \tr{\mathit{Co}} s'$, then $k'=k+1$ and $n'=n$. Observe that for $j \in [k + 1, n - 1]$, $x'_j = x_j \stackrel{{\eqref{eq:pdr:positivenegative}}}{ \not\le} y_j =y_j'$. Thus $\forall j \in [k', n' - 1] \text{, }x_j' \not\le y_j' $.
	\end{itemize}
\end{proof}

\subsection{Soundness}

Once the properties in Figure~\ref{fig:pdr:invariants} are proved to be invariants, the proof of soundness of {\APDR} is rather straightforward: it only appeals to the Knaster-Tarski fixed-point theorem for the positive case, and to the Kleene one for the negative case.

\begin{theorem}[Soundness]\label{th:pdr:soundness}
	\emph {\APDR} is sound, namely,
	\begin{enumerate}
		\item If \emph{\APDR} returns true then $\lfp (f \lor i) \le p$.
		\item If \emph{\APDR} returns false then $\lfp (f \lor i) \nleq p$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	We prove the two items separately.
	\begin{enumerate}
		\item Observe that {\APDR} returns true if $x_{j+1} \le x_j$. By \eqref{eq:pdr:positiveF}, we thus have $f(x_j) \le x_{j+1} \le x_j$. Moreover, by \eqref{eq:pdr:Ix1} and \eqref{eq:pdr:positivechain}, it holds that $i \le x_j$ and $x_j \le p$. Therefore, it holds that
		      \[
		      (f \lor i) x_j \le x_j \le p \text{.}
		      \]
		      By \eqref{eq:bg:coinductionproofprinciple}, we have that $\lfp (f \lor i) \le p$.
		\item Observe that {\APDR} returns false if $i \nleq y_1$. By \eqref{eq:pdr:negativefinal}, $g^{n-2}(p) \le y_1$. Thus $i \nleq g^{n-2}(p)$. Moreover
		      \begin{align*}
			      g^{n-2}p & \le \bigwedge_{j\in \omega} g^{j}(p) \\
			               & = \gfp (g \land p)
		      \end{align*}
		      Thus $i \nleq \gfp(g \land p) $. By \eqref{eq:bg:adjoint-fixpoint}, $\lfp(f \lor i) \nleq p$.
	\end{enumerate}
\end{proof}

\subsection{Progression}\label{sec:pdr:progression}
It is necessary to prove that in any step of the execution, if the algorithm does not return true or false, then it can progress to a new state, not yet visited. To this aim we must deal with the subtleties of the non-deterministic choice of the element $z$ in (Candidate), (Decide) and (Conflict). The following proposition ensures that, for any of these three rules, there is always a possible choice.

\begin{prop}[Canonical choices]\label{prop:pdr:CanonicalChoice}
	The following choices of $z$ are always possible:
	\begin{enumerate}
		\item in (Candidate) $z=p$;
		\item in (Decide) $z= g(y_k)$;
		\item in (Conflict) $z = y_k$;
		\item in (Conflict) $z = (f \lor i)(x_{k-1})$.
	\end{enumerate}
	Thus, for all non-conclusive $s\in \states$, if $s_0 \ttr{} s $ then $s \tr{}$.
\end{prop}
\begin{proof}
	For each rule, we prove that if the guard of the rule is satisfied then the choice of $z$ satisfies the required constraints.
	\begin{enumerate}
		\item The guard of (Candidate) is $x_{n-1} \not\le p$. By choosing $z=p$, one has that $x_{n-1} \not \le z$ and $p \le z$ are trivially satisfied;
		\item The guard of (Decide) is $f(x_{k-1}) \not\le y_k$ thus, by $f \dashv g$, $x_{k-1} \not \le g(y_k)$. By choosing $z= g(y_k)$, one has that $x_{k-1} \not\le z$ and $g(y_k) \le z$;
	\end{enumerate}
	The proofs for the choices in (Conflict) are more subtle. First of all, observe that if $k=1$, then $i\le y_1$ otherwise the algorithm would have returned false. Moreover, for $k\geq 2$, we have that
	$i \le x_{k-1} \le y_k$: the first inequality holds by \eqref{eq:pdr:Ix1} and the second by \eqref{eq:pdr:positivefinal} and \eqref{eq:pdr:negativefinal}. In summary,
	\begin{equation}\label{eq:pdr:yigeqI}
		\forall j \geq 1 \sdot i \le y_j \text{.}
	\end{equation}
	We can then proceed as follows.
	\begin{enumerate}\setcounter{enumi}{2}
		\item The guard of (Conflict) is $f(x_{k-1}) \le y_k$. By choosing $z=y_k$, one has that $z \le y_k$ trivially holds. For $(f \lor i)(x_{k-1} \land z) \le z$ observe that
		      \begin{align*}
			      (f \lor i)(x_{k-1} \land z) & = f(x_{k-1} \land z) \lor i & [\text{def.}]           \\
			                                  & \le f(x_{k-1} ) \lor i      & [\text{monotonicity}]   \\
			                                  & \le z \lor i                & [\text{guard}]          \\
			                                  & = z                         & [\eqref{eq:pdr:yigeqI}]
		      \end{align*}

		\item The guard of (Conflict) is $f(x_{k-1}) \le y_k$. By choosing $z=(f \lor i)(x_{k-1})$, one has that $(f \lor i)(x_{k-1} \land z) \le (f \lor i)(x_{k-1}) =z$ holds by monotonicity.
		      For $z \le y_k$, by using the guard and \eqref{eq:pdr:yigeqI}, we have that $z= (f \lor i)(x_{k-1}) = f(x_{k-1}) \lor i \le y_k$.
	\end{enumerate}
\end{proof}

The following proposition ensures that {\APDR} always traverses new states.
\begin{prop}[Impossibility of loops]\label{prop:pdr:progres}
	If $s_0 \ttr{} s \ttp{ } s'$, then $s\neq s'$.
\end{prop}
\begin{proof}
	Let us consider the following partial order on positive chains: given two sequences $\vec{x}= x_0, \dots x_{n-1}$ and $\vec{x}' = x'_0, \dots, x'_{n'-1}$, we say $\vec{x} \preceq \vec{x}'$ if
	\begin{equation*}
		n \le n' \land x_j \ge x'_j \text{ for each } j\in[0,n - 1]
	\end{equation*}
	We extend the order to states by letting $(\vec{x} \| \vec{y} )_{n, k} \preceq (\vec{x}' \| \vec{y}' )_{n', k'}$ with $\vec{x} \prec \vec{x}'$ or $\vec{x} = \vec{x}'$ and $k \ge k'$.

	We prove the statement by showing that applying a rule strictly increases the state in that partial order.
	As before, we use non-primed variables such as $\vec{x}$ for values before the application of a rule, and primed variables such as $\vec{x}'$ after.

	For (Unfold), we have that $n < n' = n + 1$ and $x_j = x'_j$ for each $j\in[0,n - 1]$.

	For (Candidate), we have $\vec{x}' = \vec{x}$ and $k' = n - 1 < n = k$.

	For (Decide), we have $\vec{x}' = \vec{x}$ and $k' = k - 1 < k$.

	For (Conflict), $n = n'$, and
	\[
	x'_j = \begin{cases*}
		x_j         & if $j > k$   \\
		x_j \land z & if $j \le k$
	\end{cases*}
	\]
	So for $j\in[k+1,n - 1]$ we have $x_j = x'_j$, and for $j\in[0,k]$ we have $x_j \ge x_j \land z = x'_j$. So $\vec{x} \preceq \vec{x}'$. Assume by contradiction that $x'_k = x_k$. Since $x'_k = x_k \land z$, this is equivalent to $x_k \le z$. The choice of $z$ in (Conflict) satisfies $z \le y_k$, that would imply $x_k \le z \le y_k$. However, this is a contradiction, since by \eqref{eq:pdr:positivenegative} we know $x_k \not \le y_k$. Hence $x_k \ge x'_k$, meaning $\vec{x} \prec \vec{x}'$.
\end{proof}

Observe that the above propositions entail that {\APDR} terminates whenever the lattice $L$ is finite, since the set of reachable states is finite in this case.

\begin{example}
	For $(I,T,G,P)$ as in Example~\ref{ex:sota:ts}, {\APDR} behaves essentially as IC3/PDR~\cite{Bradley11}, solving reachability problems for transition systems with finite state space $S$. Since the lattice $\mathcal{P}S$ is also finite, {\APDR} always terminates.
\end{example}

\subsection{Heuristics}\label{sec:pdr:heuristics}
The nondeterministic choices of the algorithm can be resolved by using heuristics. Intuitively, a heuristic chooses for any states $s\in\states$ an element $z\in L$ to be possibly used in (Candidate), (Decide) or (Conflict), so it is just a function $h\colon \states \to L$.
When defining a heuristic, we will avoid to specify its values on conclusive states or in those performing (Unfold), as they are clearly irrelevant.

With a heuristic, one can instantiate {\APDR} by making the choice of $z$ as prescribed by $h$. Syntactically, this means to erase from the code of Figure~\ref{fig:pdr:apdr} the three lines of \texttt{choose} and replace them with $z \texttt{:= } h(\,( \vec{x} \| \vec{c} )_{n,k}\,)$. We call {\APDR}$_h$ the resulting deterministic algorithm and write $s \Htrz{}{h}{} s'$ to mean that {\APDR}$_h$ moves from state $s$ to $s'$. We let $\states^h \eqdef \{s\in \states \mid s_0\Htrz{}{h}{*} s\}$ be the sets of all states reachable by {\APDR}$_h$.

\begin{definition}[legit heuristic]
	A heuristic $h\colon \states \to L$ is called \emph{legit} whenever for all $s,s'\in \states^h$, if $s \Htrz{}{h}{}s'$ then $s\tr{}s'$.
\end{definition}
When $h$ is legit, the only execution of the deterministic algorithm {\APDR}$_h$ is one of the possible executions of the non-deterministic algorithm {\APDR}.

The canonical choices provide two legit heuristics: first, we call \emph{simple} any legit heuristic $h$ that chooses $z$ in (Candidate) and (Decide) as in Proposition \ref{prop:pdr:CanonicalChoice}:
\begin{equation}\label{eq:pdr:simple}
	( \vec{x} \| \vec{y} )_{n,k} \mapsto
	\begin{cases*}
		p      & if $( \vec{x} \| \vec{y} )_{n,k} \tr{ \mathit{Ca} }$ \\
		g(y_k) & if $( \vec{x} \| \vec{y} )_{n,k} \tr{ D }$
	\end{cases*}
\end{equation}
Then, if the choice in (Conflict) is like in Proposition \ref{prop:pdr:CanonicalChoice}.4, we call $h$ \emph{initial}; if it is like in Proposition \ref{prop:pdr:CanonicalChoice}.3, we call $h$ \emph{final}. Shortly, the two legit heuristics are:
\[
\begin{array}{r|ll}
	\quad\emph{simple initial} \quad\
	                   & \quad\eqref{eq:pdr:simple} \text{ and }( \vec{x} \| \vec{y} )_{n,k} \mapsto
	(f\lor i)(x_{k-1}) & \quad\mbox{if $( \vec{x} \| \vec{y} )_{n,k} \in \mathit{Co}$}\quad
	\\[5pt]
	\hline
	\\[-7pt]
	\quad\emph{simple final} \quad\
	                   & \quad\eqref{eq:pdr:simple} \text{ and }
	( \vec{x} \| \vec{y} )_{n,k} \mapsto
	y_k                & \quad\mbox{if $( \vec{x} \| \vec{y} )_{n,k} \in \mathit{Co}$}\quad          \\
\end{array}
\]
Interestingly, with any simple heuristic, the sequence $\vec{y}$ takes a familiar shape:
\begin{prop}\label{prop:pdr:negativesequencefinalchain}
	Let $h\colon \states \to L$ be any simple heuristic. For all $( \vec{x} \| \vec{y} )_{n,k} \in \states^h$, invariant~\eqref{eq:pdr:negativefinal} holds as an equality, namely for all $j\in[k,n-1]$,
	$y_j=g^{n-1-j}(p)$.
\end{prop}
\begin{proof}
	As for the invariants, we prove this equality by induction showing
	\begin{itemize}
		\item[(a)] it holds for $s_0$ and
		\item[(b)] if it holds for $s$ and $s \tr{ } s'$, then it holds for $s'$.
	\end{itemize}

	In $s_0$ and after (Unfold), since $k = n$ there is no $j \in [k, n-1]$.

	For (Conflict), since the property holds on $\vec{y}$ it also holds on $\vec{y}' = \mathsf{tail}(\vec{y})$.

	For (Candidate), $\vec{y}' = p$ and $k' = n - 1$, so the thesis holds because $y_{n-1} = p = g^{n-1-(n-1)} p$.

	For (Decide), $\vec{y}' = g(y_k), \vec{y}$ and $k' = k - 1$. For all $j \in [k' + 1, n-1]$ the thesis holds because $y'_j = y_j$. For $j = k'$, we have $y_{k'} = g(y_k) = g(g^{n - 1 -k}( p)) = g^{n - 1 - k'}(p)$.
\end{proof}

By the above proposition and~\eqref{eq:pdr:negativefinal}, the negative sequence $\vec{y}$ occurring in the execution of {\APDR}$_h$, for a simple heuristic $h$, is the least amongst all the negative sequences occurring in any execution of {\APDR}.
Instead, invariant \eqref{eq:pdr:positiveinitialfinal} informs us that the positive chain $\vec{x}$ is always in between the initial chain of $f\lor i$ and the final chain of $g \land p$. Such values of $\vec{x}$ are obtained by, respectively, simple initial and simple final heuristic.
This is formally shown in Propositions~\ref{prop:pdr:heuristic-initial-chain} and \ref{prop:pdr:heuristic-final-chain} below.

\begin{example}
	Consider the two runs of {\APDR} in Example~\ref{ex:pdr:simple-ts}. The first one exploits the simple initial heuristic and indeed, the positive chain $\vec{x}$ coincides with the initial chain.
	Analogously, the second run uses the simple final heuristic.
\end{example}

\begin{prop}\label{prop:pdr:heuristic-initial-chain}
	Assume $p \neq \top$ and let $h\colon \states \to L$ be any simple initial heuristic.  For all $( \vec{x} \| \vec{y} )_{n,k} \in \states^h$, the first inequality in \eqref{eq:pdr:positiveinitialfinal} holds as an equality for all $j\in[0,n-2]$, namely $x_j=(f\lor i)^{j}(\bot)$.
\end{prop}

\begin{prop}\label{prop:pdr:heuristic-final-chain}
	Assume $p \neq \top$ and let $h\colon \states \to L$ be any simple final heuristic. If $s_0\ttr{}\tr{U} ( \vec{x} \| \vec{y} )_{n,k}$ then the second inequality in \eqref{eq:pdr:positiveinitialfinal} holds as an equality, namely for all $j\in[1,n-1]$, $x_j=(g\land p)^{n-1-j}(\top)$.
\end{prop}

\subsection{Negative Termination}\label{sec:pdr:termination}
When the lattice $L$ is not finite, {\APDR} may not return a result, since checking $\lfp(f \lor i) \le p$ is not always decidable. In this section, we show that the use of certain heuristics can guarantee termination whenever $\lfp(f \lor i) \not\le p$.
The key insight is the following: if $\lfp (f \lor i) \not \le p$ then by~\eqref{th:bg:kleene}, there should exist some $\tilde{n} \in \setN$ such that $(f \lor i)^{\tilde{n}} (\bot) \not \le p$. By \eqref{eq:pdr:positiveinitialfinal}, the rule (Unfold) can be applied only when $(f \lor i)^{n-1} (\bot) \le x_{n-1} \le p$. Since (Unfold) increases $n$ and $n$ is never decreased by other rules, then (Unfold) can be applied at most $\tilde{n}$ times. Therefore, we can guarantee termination whenever the number of steps between two (Unfold) is finite.

The first observation for termination is the following lemma. It states that an element $z$ cannot be added twice to negative sequence until $n$ is increased, i.e., until (Unfold) is applied.
\begin{lemma}\label{lmm:pdr:differentz}
	If $s_0 \ttr{} s \trz{D}{z} \ttr{ } s' \trz{D}{z'} $ and $s$ and $s'$ carry the same index $(n,k)$, then $z' \neq z$.
	Similarly, if $s_0 \ttr{} s \trz{\mathit{Ca}}{z} \ttr{ } s' \trz{\mathit{Ca}}{z'} $ and $s$ and $s'$ carry the same index $(n,k)$, then $z' \neq z$.
\end{lemma}

Elements of negative sequences are introduced by rules (Candidate) and (Decide). If we guarantee that for any index $(n,k)$ the heuristic in such cases returns a finite number of values for $z$, then we can prove termination. To make this formal, we fix $\mathit{CaD}^h_{n,k} \eqdef \{ ( \vec{x} \| \vec{y} )_{n,k}\in \states^h \mid ( \vec{x} \| \vec{y} )_{n,k}\tr{\mathit{Ca}} \text{ or } ( \vec{x} \| \vec{y} )_{n,k}\tr{D}\}$, i.e., the set of all $(n,k)$-indexed states reachable by {\APDR}$_h$ that trigger (Candidate) or (Decide), and $h(\mathit{CaD}^h_{n,k}) \eqdef \{h(s) \mid s\in \mathit{CaD}^h_{n,k}\}$, i.e., the set of all possible values returned by $h$ in such states.

\begin{theorem}[Negative termination]\label{th:pdr:negativetermination}
	Let $h$ be a legit heuristic. If $h(\mathit{CaD}^h_{n,k})$ is finite for all $n,k$ and $\lfp(f\lor i) \not \le p$, then \emph{\APDR}$_h$ terminates.
\end{theorem}

\begin{corollary}\label{cor:pdr:negativetermiantion}
	Let $h$ be a simple heuristic.
	If $\lfp(f\lor i) \not \le p$, then \emph{\APDR}$_h$ terminates.
\end{corollary}

Note that this corollary ensures negative termination whenever we use the canonical choices in (Candidate) and (Decide) \emph{irrespective of the choice for} (Conflict), therefore it holds for both simple initial and simple final heuristics.

\subsection{The meet-semilattice of positive chains and the join-semilattice of negative sequences}
We conclude this section with two results illustrating some algebraic properties of positive chains and negative sequences. These are not necessary for proving properties of {\APDR}, but they will be quite convenient in Section~\ref{sec:pdr:LTPDRvsADPDR}.

We observe that positive chains of a fixed length $n$ form a join-semilattice and negative sequences a meet-semilattices, where joins and meets are defined point-wise, i.e., for two positive chains $\vec{x^1}, \vec{x^2}$ their join is defined as $(\vec{x^1} \lor \vec{x^2})_j \eqdef x^1_j\lor x^2_j$, and similarly for negative sequences. To show this it suffices to prove that the join of an arbitrary set of positive chains (resp. the meet of an arbitrary set of negative sequences) is still a positive chain (resp. negative sequence).

\begin{lemma}\label{lmm:pdr:joinpositive}
	Let $I$ be a set. For all $m \in I$, let $\vec{x^m}=x^m_0, \dots, x^m_{n-1}$ be a positive chain.
	Then, the chain $\bigvee_{m\in I} \vec{x^m}$ defined for all $j \in [0, n-1]$ as
	\[
	(\bigvee_{m\in I} \vec{x^m})_j \eqdef \bigvee_{m\in I}x^m_{j}
	\]
	is a positive chain.
\end{lemma}
\begin{proof}
	Since $ i \le x^m_{1}$ for all $m\in I$, then $ i \le \bigvee_{m\in I}x^m_{1} $.
	Since $ x^m_{n-2} \le p$ for all $m\in I$, then $ \bigvee_{m\in I}x^m_{n-2} \le p$.

	To show that $f( (\bigvee_{m\in I}\vec{x^m})_{j}) \le (\bigvee_{m\in I}\vec{x^m})_{j+1}$ we just observe the following
	\begin{align*}
		f((\bigvee_{m\in I}\vec{x^m})_{j}) & = f(\bigvee_{m\in I}x^m_j)          & [\text{def.}]              \\
		                                   & = \bigvee_{m\in I} f(x^m_{j} )      & [f \dashv g]               \\
		                                   & \le \bigvee_{m\in I} x^m_{j+1}      & [\eqref{eq:pdr:positiveF}] \\
		                                   & = (\bigvee_{m\in I}\vec{x^m})_{j+1} & [\text{def.}]
	\end{align*}
	Thus \eqref{eq:pdr:Ix1}, \eqref{eq:pdr:xP} and \eqref{eq:pdr:positiveF} hold for $\bigvee_{m\in I} \vec{x^m}$.
\end{proof}

\begin{lemma}\label{lmm:pdr:meetneg}
	Let $I$ be a set. For all $m \in I$, let $\vec{y^m}=y^m_k, \dots, y^m_{n-1}$ be a negative sequence. Then, the sequence $\bigwedge_{m\in I} \vec{y^m}$ defined for all $j = 0, \dots n-1$ as
	\[
	(\bigwedge_{m\in I} \vec{y^m})_j \eqdef \bigwedge_{m\in I}y^m_{j}
	\]
	is a negative sequence. Moreover, if $\vec{y^m}$ is conclusive for all $m \in I$, then also $\bigwedge_{m\in I} \vec{y^m}$ is conclusive.
\end{lemma}
\begin{proof}
	Since $p \le y^m_{n-1}$ for all $m\in I$, then $p \le \bigwedge_{m\in I}y^m_{n-1} $.

	To show that $g(\bigwedge_{m\in I}\vec{y^m})_{j+1} \le (\bigwedge_{m\in I}\vec{y^m})_{j}$ we proceed as follows
	\begin{align*}
		g(\bigwedge_{m\in I}\vec{y^m})_{j+1} & = g(\bigwedge_{m\in I}y^m_j)        & [\text{def.}]              \\
		                                     & = \bigwedge_{m\in I} g(y^m_{j+1} )  & [f \dashv g]               \\
		                                     & \le \bigwedge_{m\in I} y^m_{j}      & [\eqref{eq:pdr:negativeG}] \\
		                                     & = (\bigwedge_{m\in I}\vec{y^m})_{j} & [\text{def.}]
	\end{align*}

	For conclusive sequences, observe that, since $i \not \le y_1^m$ for all $m \in I$, then $i \not \le \bigwedge_{m\in I}y_1^m = (\bigwedge_{m\in I}\vec{y^m})_{1}$.
\end{proof}

The bottom element of the meet-semilattice of negative sequences is given by $g^{n-1-j}(p)$ for all $j\in [k,n-1]$, and is exactly the one in invariant \eqref{eq:pdr:negativefinal}. The top element of the join-semilattice of positive chains is the chain defined as $(g \land p)^{n-1-j} (\top)$ for all $j\in [0,n-1]$; its bottom element is the chain $(f \lor i)^j (\bot)$. Again, these are exactly the bounds that appear in invariant \eqref{eq:pdr:positiveinitialfinal}.
Note that, if $\vec{y^1}$ and $\vec{y^2}$ are conclusive, also $\vec{y^1} \land \vec{y^2}$ is conclusive. An analogous property for positive chains does not hold.

\section{{\ADPDR}}\label{sec:pdr:downset}
In Section~\ref{sec:pdr:APDR}, we have introduced an algorithm for checking $\lfp(b) \le p$ whenever $b$ is of the form $f \lor i$ for an element $i\in L$ and a left-adjoint $f\colon L \to L$. This, unfortunately, is not the case for several interesting problems, like the max reachability problem for Markov Decision Processes~\cite{BK08} that we will illustrate in Section~\ref{sec:pdr:MDP}.

The next result informs us that, under standard assumptions, one can transfer the problem of checking $\lfp(b) \le p$ to lower sets, where adjoints can always be defined.
Recall that, for a lattice $(L,\le)$, a \emph{lower set} is a subset $X\subseteq L$ such that if $x\in X$ and $x'\le x$ then $x'\in X$; the set of lower sets of $L$ forms a complete lattice $(L^\downarrow, \subseteq)$ with joins and meets given by  union and intersection; as expected $\bot$ is $\emptyset$ and $\top$ is $L$.
Given $b\colon L\to L$, one can define two functions $b^\downarrow, b^\downarrow_r \colon L^\downarrow \to L^\downarrow$ as $b^\downarrow(X) \eqdef b(X)^\downarrow$ and $b^\downarrow_r(X) \eqdef \{x \mid b(x) \in X\}$. It holds that $b^\downarrow\, \dashv\, b^\downarrow_r$.
\begin{equation}\label{eq:pdr:lowersetadjunction}
	\xymatrix{
	(L, \le) \lloop{b} \ar@/_1.5ex/[r]_-{(-)^\downarrow}^-\bot
	&(L^\downarrow, \subseteq) \rloop{b^\downarrow\, \dashv\, b^\downarrow_r} \ar@/_1.5ex/[l]_-{\bigsqcup}
	}
\end{equation}
In the diagram above, $(-)^\downarrow\colon x \mapsto \{x' \mid x' \le x\}$ and $\bigsqcup \colon L^\downarrow \to L$ maps a lower set $X$ into $\bigsqcup \{x\mid x\in X\}$. The maps $\bigsqcup$ and $(-)^\downarrow$ form a \emph{Galois insertion}, namely $\bigsqcup \dashv (-)^\downarrow$ and $\bigsqcup (-)^\downarrow = id$, and thus one can think of~\eqref{eq:pdr:lowersetadjunction} in terms of abstract interpretation: $L^\downarrow$ represents the concrete domain, $L$ the abstract domain and $b$ is a sound abstraction of $b^\downarrow$. Moreover, $b$ is \emph{forward-complete}~\cite{GRS00,BGGP18} w.r.t. $b^\downarrow$, namely:
\begin{equation}\label{eq:pdr:EMlaw}
	(-)^\downarrow \circ b = b^\downarrow \circ (-)^\downarrow
\end{equation}

\begin{prop}\label{prop:pdr:prob_down_up}
	Let $(L,\le)$ be a complete lattice, $p\in L$ and $b \colon L \to L$ be a $\omega$-continuous map. Then $\lfp(b) \le p$ iff $\lfp(b^\downarrow \cup \bot^\downarrow) \subseteq p^\downarrow$.
\end{prop}
\begin{proof}
	A simple inductive argument using \eqref{eq:pdr:EMlaw} confirms that
	\begin{equation}\label{eq:pdr:hdownn}
		(b^n x)^\downarrow = (b^\downarrow)^n x^\downarrow
	\end{equation}
	for all $x\in L$. The following sequence of logical equivalences
	\begin{align*}
		\lfp(b) \le p & \Leftrightarrow \forall n \in \mathbb{N}.~b^n \bot \le p                                           & [\text{Theorem \ref{th:bg:kleene}}]                                                   \\
		              & \Leftrightarrow \forall n \in \mathbb{N}.~(b^n \bot)^\downarrow \subseteq p^\downarrow             & [\text{mon. of }(-)^\downarrow, \bigsqcup \text{ and } \bigsqcup (-)^{\downarrow}=id] \\
		              & \Leftrightarrow \bigcup_{n \in \mathbb{N}} (b^n \bot)^\downarrow  \subseteq p^\downarrow           & [\text{def. of }\bigcup]                                                              \\
		              & \Leftrightarrow \bigcup_{n \in \mathbb{N}} (b^\downarrow)^n \bot^\downarrow \subseteq p^\downarrow & [\eqref{eq:pdr:hdownn}]                                                               \\
		              & \Leftrightarrow \lfp(b^\downarrow \cup \bot^\downarrow) \subseteq p^\downarrow                     & [\text{Theorem \ref{th:bg:kleene}}].
	\end{align*}
	concludes the proof of the main statement.
\end{proof}

By means of Proposition~\ref{prop:pdr:prob_down_up}, we can thus solve $\lfp(b) \le p$ in $L$ by running {\APDR} on $(\bot^\downarrow, b^\downarrow,b_r^{\downarrow}, p^{\downarrow})$.
Hereafter, we tacitly assume that $b$ is $\omega$-continuous.

\subsection{{\ADPDR}: Positive Chain in $L$, Negative Sequence in \texorpdfstring{$L^\downarrow$}{L↓}}\label{sec:pdr:ADPDR}
\begin{figure}[t]
	\begin{center}
		\underline{{\ADPDR} $(b,p)$}
		{\small
			\begin{codeNT}
<INITIALISATION>
  $( \vec{x} \| \vec{Y} )_{n,k}$ := $(\emptyset,\bot,\top\|\varepsilon)_{3,3}$
<ITERATION>
  case $( \vec{x} \| \vec{Y} )_{n,k}$ of								% $\vec{x}, \vec{Y}$ not conclusive
	   $\vec{Y}=\varepsilon$ And $x_{n-1} \le p$     :                    %(Unfold)
			$( \vec{x} \| \vec{Y} )_{n,k}$ := $( \vec{x}, \top \| \varepsilon )_{n+1,n+1}$
	   $\vec{Y}=\varepsilon$ And $x_{n-1} \not \le p$    :                     %(Candidate)
			choose $Z\in L^{\downarrow}$ st  $x_{n-1} \not \in Z$ And  $p \in Z$;
			$( \vec{x} \| \vec{Y} )_{n,k}$ := $( \vec{x} \| Z )_{n,n-1}$
	   $\vec{Y} \neq \varepsilon$ And $b(x_{k-1}) \not \in Y_k$ :                        %(Decide)
			choose $Z\in L^{\downarrow}$ st $x_{k-1} \not \in Z$ And $b^{\downarrow}_r(Y_k) \subseteq Z$;
			$(\vec{x} \| \vec{Y} )_{n,k}$ := $(\vec{x} \| Z , \vec{Y} )_{n,k-1}$
	   $\vec{Y} \neq \varepsilon$ And $b(x_{k-1}) \in Y_k$ :                        %(Conflict)
			choose $z \in L$ st $z \in Y_k$ And $b(x_{k-1} \land z) \le z$;
			$(\vec{x} \| \vec{Y} )_{n,k}$ := $(\vec{x} \land_k z \| \mathsf{tail}(\vec{Y}) )_{n,k+1}$
  endcase
<TERMINATION>
	if $\exists j\in [0,n-2]\,.\, x_{j+1} \le x_j$ then return true		% $\vec{x}$ conclusive
	if $Y_1=\emptyset$ then return false							% $\vec{Y}$ conclusive
\end{codeNT}
		}
	\end{center}
	\caption{The algorithm {\ADPDR} for checking $\lfp(b) \le p$: the elements of negative sequence are in $L^\downarrow$, while those of the positive chain are in $L$, with the only exception of $x_0$ which is constantly the bottom lower set $\emptyset$. For $x_0$, we fix $b(x_0) = \bot$.}
	\label{fig:pdr:adpdr}
\end{figure}

While {\APDR} on $(\bot^\downarrow, b^\downarrow,b_r^{\downarrow}, p^{\downarrow})$ might be computationally expensive, it is the first step toward an efficient algorithm that exploits a convenient form of the positive chain.

A lower set $X\in L^{\downarrow}$ is said to be a \emph{principal} if $X=x^\downarrow$ for some $x\in L$. Observe that the top of the lattice $(L^\downarrow, \subseteq)$ is a principal, namely $\top^\downarrow$, and that the meet (intersection) of two principals $x^\downarrow$ and $y^\downarrow$ is the principal $(x\land y)^\downarrow$.

Suppose that, in (Conflict), {\APDR}$(\bot^\downarrow, b^\downarrow,b_r^{\downarrow}, p^{\downarrow})$ always chooses principals rather than arbitrary lower sets. This suffices to guarantee that all the elements of $\vec{x}$ are principals (with the only exception of $x_0$ which is constantly the bottom element of $L^\downarrow$ that, note, is $\emptyset$ and not $\bot^\downarrow$). In fact, the elements of $\vec{x}$ are all obtained by (Unfold), that adds the principal $\top^\downarrow$, and by (Conflict), that  takes their meets with the chosen principal.

Since principals are in bijective correspondence with the elements of $L$, by imposing to {\APDR}$(\bot^\downarrow, b^\downarrow,b_r^{\downarrow}, p^{\downarrow})$ to choose  a principal in (Conflict), we obtain an algorithm, named {\ADPDR}, where the elements of the positive chain are drawn from $L$, while the negative sequence is taken in $L^{\downarrow}$. The algorithm is reported in Figure~\ref{fig:pdr:adpdr} where we use the notation $( \vec{x} \| \vec{Y} )_{n,k}$ to emphasize that the elements of the negative sequence are lower sets of elements in $L$.

All definitions and results illustrated in Sections~\ref{sec:pdr:APDR} and \ref{sec:pdr:properties} for {\APDR}  are inherited by {\ADPDR}, with the only exception of Proposition~\ref{prop:pdr:CanonicalChoice}.3. This does not hold because it prescribes a choice for (Conflict) that may not be a principal. In contrast, the choice in Proposition~\ref{prop:pdr:CanonicalChoice}.4 is, thanks to \eqref{eq:pdr:EMlaw}, a principal. This means in particular that the simple initial heuristic is always applicable.

\begin{theorem}\label{th:pdr:ADPDR}
	All results in Section~\ref{sec:pdr:properties}, but Proposition~\ref{prop:pdr:CanonicalChoice}.3, hold for \emph{\ADPDR}.
\end{theorem}
%The proof relies on some transformations of programs from {\APDR}$(\bot^\downarrow, b^\downarrow,b_r^{\downarrow}, p^{\downarrow})$  to {\ADPDR} $(b,p)$.

\subsection{{\ADPDR} simulates LT-PDR}\label{sec:pdr:LTPDRvsADPDR}
The closest approach to {\APDR} and {\ADPDR} is the lattice-theoretic extension of the original PDR, called LT-PDR~\cite{KUKSH22}. While these algorithms exploit essentially the same positive chain to find an invariant, the main difference lies in the sequence used to witness the existence of some counterexamples.
\begin{definition}[Kleene sequence, from~\cite{KUKSH22}]
	A sequence $\vec{c}= c_k,\dots, c_{n-1}$ of elements of $L$ is a \emph{Kleene sequence} if the conditions \emph{(C1)} and \emph{(C2)} below hold.
	It is \emph{conclusive} if also condition \emph{(C0)} holds.
	\[
	\emph{(C0) } c_1 \le b(\bot),
	\qquad
	\emph{(C1) } c_{n-1} \not \le p,
	\qquad
	\emph{(C2) } \forall j\in[k,n-2].~c_{j+1} \le b(c_j)\text{.}
	\]
\end{definition}

LT-PDR tries to construct an under-approximation $c_{n-1}$ of $b^{n-2}(\bot)$ that violates the property $p$. The Kleene sequence is constructed by trial and error, starting by some arbitrary choice of $c_{n-1}$.

	{\APDR} crucially differs from LT-PDR in the search for counterexamples: LT-PDR under-approximates the final chain while {\APDR} over\hyp{}approximates it. However, we can draw a formal correspondence between {\ADPDR} and LT-PDR by showing that {\ADPDR} simulates LT-PDR, but cannot be simulated by LT-PDR.
In fact, {\ADPDR} exploits the existence of the adjoint to start from an over\hyp{}approximation $Y_{n-1}$ of $p^\downarrow$ and computes backward an over-approximation of the set of safe states.
Thus, the key difference comes from the strategy to look for a counterexample: to prove $\lfp(b) \not \le p$, {\ADPDR} tries to find $Y_{n-1}$ satisfying $p \in Y_{n-1}$ and $\lfp(b) \not \in Y_{n-1}$ while LT-PDR tries to find $c_{n-1}$ s.t. $c_{n-1} \not \le p$ and $c_{n-1} \le \lfp(b)$.

Theorem~\ref{th:pdr:LT-PDR-instance-ADPDR} below states that {\ADPDR} can mimic any execution of LT-PDR. The proof exploits a map from LT-PDR's Kleene sequences $\vec{c}$ to {\ADPDR}'s negative sequences $\negation{\vec{c}}$ of a particular form.
Let $(L^{\uparrow}, \supseteq)$  be the complete lattice of upper sets, namely subsets $X \subseteq L$ such that $X=X^\uparrow \eqdef \{x'\in L \mid \exists x\in X \,. \, x\le x'\}$.
There is an isomorphism $\neg \colon {(L^\uparrow, \supseteq)} \stackrel{\cong}{\longleftrightarrow} (L^\downarrow, \subseteq)$ mapping each $X\subseteq S$ into its complement.
For a Kleene sequence $\vec{c} = c_k,\dots, c_{n-1}$ of LT-PDR, the sequence $\negation{\vec{c}} \eqdef \lnot (\{ c_k \}^{\uparrow}), \dots, \lnot (\{ c_{n-1} \}^{\uparrow})$ is a negative sequence, in the sense of Definition~\ref{def:pdr:neg_seq}, for {\ADPDR}.

\begin{prop}\label{prop:pdr:negLTPDR}
	Let $\vec{c}$ be a Kleene sequence. Then $\negation{\vec{c}}$ is a negative sequence for {\ADPDR}.
\end{prop}
\begin{proof}
	First, we show that $p \in \negation{\vec{c}}_{n-1}$. Since $c_{n-1}\not \le p$, by (C1), then $p \not\in \{c_{n-1}\}^\uparrow$. Thus $p \in \neg ( \{c_{n-1}\}^\uparrow)$, that is $p \in \negation{\vec{c}}_{n-1}$.

	Then, we show that $b_r^\downarrow(\negation{\vec{c}}_{j+1}) \subseteq \negation{\vec{c}}_j$.
	\begin{align*}
		b_r^\downarrow(\negation{\vec{c}}_{j+1}) & = b_r^\downarrow(\neg(\{c_{j+1}\}^\uparrow))    & [\text{def.}]       \\
		                                         & = \{x \mid b(x) \notin (\{c_{j+1}\}^\uparrow \} & [\text{def.}]       \\
		                                         & =  \{x \mid c_{j+1} \not \le b(x) \}            & [\text{def.}]       \\
		                                         & \subseteq  \{x \mid b(c_j) \not \le b(x) \}     & [\text{(C2)}]       \\
		                                         & \subseteq  \{x \mid c_j \not \le x \}           & [\text{mon. of } b] \\
		                                         & =  \neg(\{c_j\}^\uparrow)                       & [\text{def.}]       \\
		                                         & =  \negation{\vec{c}}_j                         & [\text{def.}]
	\end{align*}
\end{proof}

Most importantly, the assignment $\vec{c} \mapsto \negation{\vec{c}}$ extends to a function, from the states of LT-PDR to those of {\ADPDR}, that is proved to be a \emph{strong simulation}~\cite{Milner89}.

\begin{theorem}\label{th:pdr:LT-PDR-instance-ADPDR}
	\emph{\ADPDR} simulates LT-PDR.
\end{theorem}

Remarkably, {\ADPDR}'s negative sequences are not limited to the images of LT-PDR's Kleene sequences: they are more general  than the complement of the upper closure of a singleton. In fact, a single negative sequence of {\ADPDR} can represent \emph{multiple} Kleene sequences of LT-PDR at once. Intuitively, this means that a single execution of {\ADPDR} can correspond to multiple runs of LT-PDR. We can make this formal by means of the following result.

\begin{prop}\label{prop:pdr:multipleLTPDR}
	Let $\{\vec{c^m}\}_{m\in M}$ be a family of Kleene sequences.
	Then its pointwise intersection $\bigcap_{m\in M} \negation{\vec{c^m}}$ is a negative sequence.
\end{prop}
\begin{proof}
	Since each of the $\vec{c^m}$ is a Kleene sequence, for all $m\in M$, $\negation{\vec{c^m}}$ is, by Proposition~\ref{prop:pdr:negLTPDR}, a negative sequence.
	Since negative sequences form a meet-semilattice, their intersection is also a negative sequence (Lemma~\ref{lmm:pdr:meetneg}).
\end{proof}

The above intersection is pointwise in the sense that, for all $j\in {[k,n-1]}$, it holds $(\bigcap_{m\in M} \negation{\vec{c^m}})_j \eqdef \bigcap_{m\in M} (\negation{\vec{c^m}})_j = \lnot(\{ c_j^m \mid m \in M \}^{\uparrow})$: intuitively, this is (up to $\negation{\cdot}$) a set containing all the $M$ counterexamples.
Note that, if the negative sequence of {\ADPDR} makes \eqref{eq:pdr:negativefinal} hold as an equality, as it is possible with any simple heuristic (see Proposition~\ref{prop:pdr:negativesequencefinalchain}), then its complement contains \emph{all} Kleene sequences possibly computed by LT-PDR.

\begin{prop}\label{prop:pdr:LTPDRfinal}
	Let $\vec{c}$ be a Kleene sequence and $\vec{Y}$ be the negative sequence s.t. $Y_j= (b_r^\downarrow)^{n-1-j}(p^\downarrow)$ for all $j \in [k,n-1]$.
	Then  $c_j \in \neg(Y_j)$ for all $j \in [k,n-1]$.
\end{prop}
\begin{proof}
	Since $\vec{c} = c_0, \dots, c_{n-1}$ is a Kleene sequence, $\negation{\vec{c}} = \lnot (\{ c_k \}^{\uparrow}), \dots, \lnot (\{ c_{n-1} \}^{\uparrow})$ is, by Proposition~\ref{prop:pdr:negLTPDR}, a negative sequence.
	By~\eqref{eq:pdr:negativefinal}, for all $j \in [k, n - 1]$ we have $(b_r^\downarrow)^{n-1-j}(p^\downarrow) \subseteq  \neg(\{c_j\}^\uparrow)$. Therefore, $\neg (b_r^\downarrow)^{n-1-j}(p^\downarrow) \supseteq \{c_j\}^\uparrow$, so $c_j \in \neg (b_r^\downarrow)^{n-1-j}(p^\downarrow) = \neg (Y_j)$.
\end{proof}

While the previous result suggests that simple heuristics are always the best in theory, as they can carry all counterexamples, this is  often not the case in practice, since they might be computationally hard and outperformed by  some smart over-approximations. An example is given by \eqref{eq:pdr:secondterminatingheuristics} in the next section.

\section{Instantiating {\ADPDR} for MDPs}\label{sec:pdr:MDP}
In this section we illustrate how to use {\ADPDR} to address the max reachability problem \cite{BK08} for Markov Decision Processes.

\subsection{The max reachability problem}
A \emph{Markov Decision Process} (MDP) is a tuple  $(A, S, s_\iota, \delta)$ where $A$ is a set of labels, $S$ is a set of states, $s_\iota \in S$ is an initial state, and $\delta \colon S\times A  \to \mathcal{D}S + 1$ is a transition function.
Here $\mathcal{D}S$ is the set of probability distributions over $S$, namely functions $d\colon S\to [0, 1]$ such that $\sum_{s\in S} d(s)=1$, and $\mathcal{D}S + 1$ is the disjoint union of $\mathcal{D}S$ and $1=\{*\}$. The transition function $\delta$ assigns to every label $a\in A$ and to every state $s\in S$ either a distribution of states or $* \in 1$. We assume that both $S$ and $A$ are finite sets and that the set $\mathit{Act}(s) \eqdef \{ a\in A \mid \delta(s,a)\neq *\}$ of actions enabled at $s$ is non-empty for all states.

An MDP $(A, S, s_\iota, \delta)$ mixes nondeterministic and probabilistic computations. The notion of a \emph{scheduler}, also known as \emph{adversary}, \emph{policy} or \emph{strategy}, is used to resolve nondeterministic choices.
Below, we write $S^+$ for the set of non-empty sequence over $S$, intuitively representing runs of the MDP.
A scheduler is a function $\alpha \colon S^+ \to A$ such that $\alpha(s_0 s_1\dots s_n)\in \mathit{Act}(s_n)$: given the states visited so far, the scheduler decides which action to trigger among the enabled ones so that the MDP behaves as a Markov chain. A scheduler $\alpha$ is called \emph{memoryless} if it always selects the same action in a given state, namely, if $\alpha(s_0 s_1\dots s_n)=\alpha(s_n)$ for any sequence $s_0 s_1\dots s_n\in S^+$. Memoryless schedulers can thus be represented just as functions $\alpha \colon S \to A$ such that $\alpha(s)\in \mathit{Act}(s)$ for any $s\in S$.

Given an MDP, the \emph{max reachability problem} requires to check whether the probability of reaching some bad states $\beta \subseteq S$ is less than or equal to a given threshold $\lambda \in [0, 1]$ for all possible schedulers.
Thus, to solve this problem, one should compute the supremum over infinitely many schedulers. Notably, it is known that there always exists one memoryless scheduler that maximizes the probabilities to reach $\beta$  (see e.g. \cite{BK08}). As the memoryless schedulers are finitely many (although their number can grow exponentially), the supremum can thus be replaced by a maximum.

\subsection{Applying {\ADPDR} to the max reachability problem}
The max-reachability problem, namely checking for a MDP $(A, S, s_\iota, \delta)$ whether the probability of reaching some bad states $\beta\subseteq P$ is less than a threshold $\lambda \in [0, 1]$, enjoys a convenient lattice-theoretical formulation~\cite{BK08}.

Consider the lattice $([0, 1]^S,\leq)$ of all functions $d\colon S\to [0,1]$, often called frames or fuzzy predicates, ordered pointwise. The max reachability problem is equivalent to check that $\mu b \leq p$ for $p\in[0,1]^S$ and $b \colon [0, 1]^S \to [0, 1]^S$, defined for all $d\in [0, 1]^S $ and $s\in S$, as
\begin{displaymath}
	p(s) \eqdef \begin{cases}
		\lambda & \text{ if } s=s_\iota,       \\
		1       & \text{ if }  s \neq s_\iota,
	\end{cases}
	\qquad
	b(d)(s) \eqdef \begin{cases}
		1                                                                                      & \text{ if } s \in \beta,     \\
		\displaystyle \max_{a \in \mathit{Act}(s)} \sum_{s'\in S} d(s') \cdot \delta(s, a)(s') & \text{ if } s \notin \beta .
	\end{cases}
\end{displaymath}

Since $b$ is not of the form $f\lor i$ for a left adjoint $f$ (see e.g.~\cite{KUKSH22}), we can't use {\APDR}, but we can use {\ADPDR}.
Beyond the simple initial heuristic, which is always applicable and enjoys negative termination, we illustrate now two additional heuristics that are experimentally tested in Section~\ref{sec:pdr:experiments}.

The two novel heuristics make the same choices in (Candidate) and (Decide). They exploit memoryless schedulers $\alpha \colon S \to A$, and the function $b_{\alpha} \colon [0, 1]^S \to [0, 1]^S$ defined for all $d\in [0, 1]^S $ and $s\in S$ as follows:
\begin{equation}\label{def:pdr:balpha}
	b_{\alpha}(d)(s) \eqdef \begin{cases}
		1                                                   & \text{ if } s \in \beta, \\
		\sum_{s'\in S} d(s') \cdot \delta(s, \alpha(s))(s') & \text{ otherwise}.
	\end{cases}
\end{equation}
Since for all $D\in ([0,1]^S)^\downarrow$, $b^\downarrow_r (D) = \{d \mid b(d) \in D\} = \bigcap_{\alpha}\{d \mid b_{\alpha} (d)\in D\}$ and since {\ADPDR} executes (Decide) only when $b(x_{k-1}) \notin Y_k$, there should exist some $\alpha$ such that $b_{\alpha} (x_{k-1})\notin Y_k$. One can thus fix
\begin{equation}\label{eq:pdr:secondterminatingheuristics}
	( \vec{x} \| \vec{Y} )_{n,k} \mapsto
	\begin{cases*}
		p^\downarrow                     & if $( \vec{x} \| \vec{Y} )_{n,k} \tr{\mathit{Ca}}$ \\
		\{d \mid b_{\alpha}(d) \in Y_k\} & if $( \vec{x} \| \vec{Y} )_{n,k} \tr{D}$
	\end{cases*}
\end{equation}
Intuitively, such choices are smart refinements of those in \eqref{eq:pdr:simple}: for (Candidate) they are exactly the same; for (Decide) rather than taking $b^\downarrow_r (Y_k)$, we consider a larger lower-set determined by the labels chosen by $\alpha$. This allows to represent each $Y_j$ as a set of $d\in [0, 1]^S $ satisfying a \emph{single} linear inequality, while using $b^\downarrow_r (Y_k)$ would yield a systems of possibly exponentially many inequalities (see Example~\ref{ex:pdr:shortheuristicforDecide} below). Moreover, from Theorem~\ref{th:pdr:negativetermination}, it follows that such choices ensures negative termination.

\begin{corollary}\label{cor:pdr:ADPDRtermination}
	Let $h$ be a legit heuristic defined for (Candidate) and (Decide) as in \eqref{eq:pdr:secondterminatingheuristics}.
	If $\mu b \not \leq p$, then \emph{\ADPDR}$_h$ terminates.
\end{corollary}

\begin{example}\label{ex:pdr:shortheuristicforDecide}
	Consider the maximum reachability problem with threshold $\lambda = \frac{1}{4}$ and $\beta= \{s_3\}$ for the following MDP on alphabet $A=\{a,b\}$ and $s_\iota=s_0$.
	\begin{displaymath}
		\xymatrix@C=30pt{
		s_2\ar@/^1ex/[r]^{b,1} & s_0 \ar@(dr,dl)|{b,\frac{1}{3}} \ar@/^1ex/[l]^{a,\frac{1}{2}\; b,\frac{2}{3}} \ar@/_1ex/[r]_{a,\frac{1}{2}} &s_1 \ar@/_1ex/[l]_{a,\frac{1}{2}} \ar[r]^{a,\frac{1}{2}} &s_3 \ar@(rd, ru)_{a, 1}},
	\end{displaymath}
	Hereafter we write $d\in [0,1]^S$ as column vectors with four entries $v_0\dots v_3$
	and we will use $\cdot$ for the usual matrix multiplication.
	With this notation, the lower set $p^\downarrow \in ([0,1]^S)^\downarrow$ and $b\colon [0,1]^S \to [0,1]^S$ can be written as
	\[
	p^\downarrow = \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {{ \!\begin{bmatrix} 1 & 0 & 0 &0 \end{bmatrix}\!}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {{ \!\begin{bmatrix} \frac{1}{4}  \end{bmatrix}\!} } \} \quad \text{ and } \quad b (\, \cvec{v_0}{v_1}{v_2}{v_3} \,) =\cvec{\max(\frac{v_1+v_2}{2}, \frac{v_0+2v_2}{3})}{\frac{v_0+v_3}{2}}{v_0}{1}
	\textrm{.}
	\]

	Amongst the several memoryless schedulers, only two are relevant for us:
	\begin{align}
		\zeta \eqdef & ( s_0 \mapsto a ,\; s_1 \mapsto a ,\; s_2 \mapsto b ,\; s_3 \mapsto a )
		\text{  and }                                                                                \\
		\xi \eqdef   & (s_0 \mapsto b ,\; s_1 \mapsto a ,\; s_2 \mapsto b ,\; s_3 \mapsto a)\text{.}
	\end{align}

	By using the definition of $b_\alpha \colon [0,1]^S \to [0,1]^S$ in \eqref{def:pdr:balpha}, we have that
	\[
	b_\zeta (\, \cvec{v_0}{v_1}{v_2}{v_3} \,) =\cvec{\frac{v_1+v_2}{2}}{\frac{v_0+v_3}{2}}{v_0}{1}  \qquad \text{ and } \qquad b_\xi (\, \cvec{v_0}{v_1}{v_2}{v_3} \,) =\cvec{ \frac{v_0+2v_2}{3}}{\frac{v_0+v_3}{2}}{v_0}{1}\textrm{.}
	\]

	It is immediate to see that the problem has negative answer, since using $\zeta$ in $4$ steps or less, $s_0$ can reach $s_3$ already with probability $\frac{1}{4}+\frac{1}{8}$.

	\begin{figure}[t]
		\[
		\begin{array}{rc|c|c}
			\mathcal{F}^0 & \eqdef & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} 1 & 0 & 0 &0 \end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix} \frac{1}{4}  \end{bmatrix}\!$} } \}                                                                          & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} 1 & 0 & 0 &0 \end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix} \frac{1}{4}  \end{bmatrix}\!$} } \,\}                                                                         \\
			\mathcal{F}^1 & \eqdef & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} 0 & 1 & 1 &0 \\ 1&0 &2 &0 \end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix} \frac{1}{2} \\ \frac{3}{4} \end{bmatrix}\!$} } \}                                               & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} 0 & \frac{1}{2} & \frac{1}{2} &0 \end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix} \frac{1}{4}  \end{bmatrix}\!$} } \,\}                                                     \\
			\mathcal{F}^2 & \eqdef & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} 3 & 0 & 0 & 1 \\ 2&1 &1 &0 \\ 4&0&2&0\end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix}1 \\ \frac{3}{2} \\ \frac{9}{4}  \end{bmatrix}\!$} } \}                               & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} \frac{3}{4} & 0 & 0 & \frac{1}{4} \end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix} \frac{1}{4}  \end{bmatrix}\!$} } \,\}                                                    \\
			\mathcal{F}^3 & \eqdef & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} 0 & \frac{3}{2} & \frac{3}{2} & 0 \\ 1&0 &2 &0 \\ \frac{3}{2}&1&1&\frac{1}{2} \\ \frac{13}{6} & 0 & \frac{4}{3} & \frac{1}{2}
                \\ 2&2&2&0 \\ \frac{10}{3}&0&\frac{8}{3}&0\end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix}0 \\ 0 \\ \frac{3}{2} \\ \frac{3}{2} \\\frac{9}{4} \\ \frac{9}{4}  \end{bmatrix}\!$} } \}                                                                     & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} 0 & \frac{3}{8} & \frac{3}{8} & 0 \end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix} 0  \end{bmatrix}\!$} } \,\} \\
			\mathcal{F}^4 & \eqdef & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid  {\scalebox{\vecscale}{$ \!\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0&1 &0 &0 \\ 0&0&1&0\\ 0&0&0&1\end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix}0 \\ 0 \\ 0\\0  \end{bmatrix}\!$} }    \}  = \{ \, \cvec{0}{0}{0}{0} \, \} & \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} \frac{9}{16} & 0 & 0 & \frac{3}{16} \end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix} 0  \end{bmatrix}\!$} } \,\}                                                            \\
			\mathcal{F}^5 & \eqdef & \emptyset                                                                                                                                                                                                                                                                                                & \emptyset
		\end{array}
		\]
		\caption{The elements of the negative sequences computed by {\ADPDR} for the MDP in Example~\ref{ex:pdr:shortheuristicforDecide}. In the central column, these elements are computed by means of the simple initial heuristics, that is $\mathcal{F}^i=(b_r^\downarrow)^i(p^\downarrow)$. In the rightmost column, these elements are computed using the heuristic in \eqref{eq:pdr:secondterminatingheuristics}. In particular $\mathcal{F}^i = \{d\mid b_\zeta(d) \in \mathcal{F}^{i-1} \}$ for $i\leq 3$, while for $i\geq 4$ these are computed as $\mathcal{F}^i = \{d\mid b_\xi(d) \in \mathcal{F}^{i-1} \}$.}
		\label{fig:pdr:exDecideheuristics}
	\end{figure}

	To illustrate the advantages of \eqref{eq:pdr:secondterminatingheuristics}, we run {\ADPDR} with the simple initial heuristic and with the heuristic that only differs for the choice in (Decide), taken as in \eqref{eq:pdr:secondterminatingheuristics}. For both heuristics, the first iterations are the same: several repetitions of (Candidate), (Conflict) and (Unfold) exploiting elements of the positive chain that form the initial chain (except for the last element $x_{n-1}$).
		{\footnotesize
			\begin{align*}
				(\emptyset\cvec{0}{0}{0}{0}\cvec{1}{1}{1}{1} \| \varepsilon)_{3,3}
				\tr{\mathit{Ca}} %
				\tr{\mathit{Co}} (\emptyset\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \| \varepsilon)_{3,3}
				\tr{U}\tr{\mathit{Ca}} %
				\tr{\mathit{Co}} %
				\tr{U}\tr{\mathit{Ca}} %
				\tr{\mathit{Co}} %
				\tr{U}\tr{\mathit{Ca}}  %
				\tr{\mathit{Co}} %
				\tr{U}\tr{\mathit{Ca}}  (\emptyset\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{1}{2}}{0}{1}\cvec{\frac{1}{4}}{\frac{1}{2}}{0}{1} \cvec{\frac{1}{4}}{\frac{5}{8}}{\frac{1}{4}}{1} \cvec{1}{1}{1}{1} \|  p^\downarrow )_{7,6} \textrm{.}
			\end{align*}
		}

	In the latter state the algorithm has to perform (Decide), since $b(x_5) \notin p^\downarrow$.
	Now the choice of $z$ in (Decide) is different for the two heuristics: the former uses $b_r^\downarrow(p^\downarrow) = \{d \mid b(d) \in p^\downarrow \}$, the latter uses $\{d \mid b_\zeta(d) \in p^\downarrow \}$. Despite the different choices, both the heuristics proceed with $6$ steps of (Decide):
	{\footnotesize
	\begin{align*}
		(\emptyset\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{1}{2}}{0}{1}\cvec{\frac{1}{4}}{\frac{1}{2}}{0}{1} \cvec{\frac{1}{4}}{\frac{5}{8}}{\frac{1}{4}}{1} \cvec{1}{1}{1}{1} \|  \mathcal{F}^0 )_{7,6} \tr{D}
		\tr{D} %
		\tr{D} %
		\tr{D} %
		\tr{D} (\emptyset\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{1}{2}}{0}{1}\cvec{\frac{1}{4}}{\frac{1}{2}}{0}{1} \cvec{\frac{1}{4}}{\frac{5}{8}}{\frac{1}{4}}{1} \cvec{1}{1}{1}{1} \|  \mathcal{F}^5,\mathcal{F}^4, \mathcal{F}^3,\mathcal{F}^2,\mathcal{F}^1, \mathcal{F}^0 )_{7,1}
	\end{align*}
	}

	The element of the negative sequence $\mathcal{F}^i$ are illustrated in Figure~\ref{fig:pdr:exDecideheuristics} for both the heuristics. In both cases, $\mathcal{F}^5=\emptyset$ and thus {\ADPDR} returns false.

	To appreciate the advantages provided by \eqref{eq:pdr:secondterminatingheuristics}, it is  enough to compare the two columns for the $\mathcal{F}^i$ in Figure~\ref{fig:pdr:exDecideheuristics}:  in the central column, the number of inequalities defining $\mathcal{F}^i$ grows significantly, while in the rightmost column is always $1$.
\end{example}

The fact that using \eqref{eq:pdr:secondterminatingheuristics} ensures that $Y_k$ is generated by a single linear inequality is quite convenient. Indeed, in this case
\[
Y_k=\{d\in [0,1]^S \mid \sum_{s\in S}(r_s \cdot d(s)) \leq r \}
\]
for suitable non-negative real numbers $r$ and $r_s$ for all $s\in S$.
The convex set $Y_k$ is generated by finitely many $d\in [0,1]^S$ enjoying a useful property: $d(s)$ is different from $0$ and $1$ only for at most one $s\in S$. The set of its generators, denoted by $\mathcal{G}_k$, can thus be easily computed. We exploit this property to resolve the choice for (Conflict). We consider  its subset $\mathcal{Z}_k \eqdef \{d \in \mathcal{G}_k \mid b(x_{k-1}) \leq d\}$ and define $z_{B}, z_{01}\in[0,1]^S$ for all $s\in S$ as
\begin{equation}\label{eq:pdr:heuristicsZConflict}
	\hspace{-10pt}z_{B}(s) \!\eqdef\!
	\begin{cases*}
		(\bigwedge \mathcal{Z}_k)(s) & if $r_s \neq 0$, $\mathcal{Z}_k\neq\emptyset$ \\
		b(x_{k-1})(s)                & otherwise
	\end{cases*}
	\hspace{-4pt}z_{01}(s) \!\eqdef\!
	\begin{cases*}
		\lceil z_{B}(s)\rceil & if $r_s = 0, \mathcal{Z}_k\neq\emptyset$ \\
		z_{B}(s)              & otherwise
	\end{cases*}
\end{equation}
where, for $u\in[0,1]$, $\lceil u \rceil$ denotes $0$ if $u=0$ and $1$ otherwise. We call \verb|hCoB|  and \verb|hCo01| the heuristics defined as in \eqref{eq:pdr:secondterminatingheuristics} for (Candidate) and (Decide) and as $z_{B}$, respectively $z_{01}$, for (Conflict). The heuristics \verb|hCo01| can be seen as a Boolean modification of \verb|hCoB|, rounding up positive values to $1$ to accelerate convergence.

\begin{prop}\label{prop:pdr:genlegit}
	The heuristics \verb|hCoB|  and \verb|hCo01| are legit.
\end{prop}

By Corollary~\ref{cor:pdr:ADPDRtermination}, {\ADPDR} terminates for negative answers with both \verb|hCoB| and \verb|hCo01|. We conclude this section with a last example.
\begin{example}\label{ex:pdr:MDPpositive}
	Consider the following MDP with alphabet $A=\{a,b\}$ and $s_\iota=s_0$
	\begin{displaymath}
		\xymatrix{  s_2 \ar@(ld,lu)^{a, 1} & s_0 \ar@(lu,ru)^{a, 1} \ar[l]^{b, \frac{1}{2}} \ar@/_1ex/[r]_{b, \frac{1}{2}} &s_1 \ar@/_1ex/[l]_{a, \frac{1}{3}} \ar[r]^{a, \frac{2}{3}} &s_3 \ar@(rd,ru)_{a, 1}    }
	\end{displaymath}
	and the max reachability problem with threshold $\lambda = \frac{2}{5}$ and $\beta=\{s_3\}$.
	The lower set $p^\downarrow \in ([0,1]^S)^\downarrow$ and $b\colon [0,1]^S \to [0,1]^S$ can be written as
	\[
	p^\downarrow = \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} 1 & 0 & 0 &0 \end{bmatrix}\!$}} \cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix} \frac{2}{5}  \end{bmatrix}\!$} } \}
	\quad \text{ and }  \quad
	b (\, \cvec{v_0}{v_1}{v_2}{v_3} \,) =\cvec{\max(v_0, \frac{v_1+v_2}{2})}{\frac{v_0+2\cdot v_3}{3}}{v_2}{1}
	\]
	Consider also the scheduler $\xi\colon S \to A$ defined as $\xi \eqdef [s_0\mapsto b,s_1\mapsto a,s_2\mapsto a,s_3\mapsto a]$, for which we illustrate
	\begin{equation}\label{eq:pdr:xi}
		b_\xi (\, \cvec{v_0}{v_1}{v_2}{v_3} \,) =\cvec{\frac{v_1+v_2}{2}}{\frac{v_0+2\cdot v_3}{3}}{v_2}{1}
		\quad \text{ and }  \quad
		\mathcal{F}^1_\xi \eqdef \{ d \mid b_\xi(d ) \in p^\downarrow \} = \{\,\cvec{v_0}{v_1}{v_2}{v_3} \mid {\scalebox{\vecscale}{$ \!\begin{bmatrix} 0 & 1 & 1 &0 \end{bmatrix}\!$}}\cdot \cvec{v_0}{v_1}{v_2}{v_3} \leq {\scalebox{\vecscale}{$ \!\begin{bmatrix} \frac{4}{5}  \end{bmatrix}\!$} } \}
	\end{equation}

	\begin{figure}[t]
		{\footnotesize
			\begin{align*}
				                       & (\cvec{0}{0}{0}{0}\cvec{1}{1}{1}{1} \| \varepsilon)_{2,2} \tr{\mathit{Ca}} (\cvec{0}{0}{0}{0}\cvec{1}{1}{1}{1} \| p^\downarrow )_{2,1}
				\tr{\mathit{Co}} (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \| \varepsilon)_{2,2}                                                                                                                                                                         \\
				\tr{U}\tr{\mathit{Ca}} & (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{1}{1}{1}{1} \| p^\downarrow )_{3,2}
				\tr{\mathit{Co}} (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{2}{3}}{0}{1} \| \varepsilon)_{3,3}                                                                                                                                             \\
				\tr{U}\tr{\mathit{Ca}} & (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{2}{3}}{0}{1} \cvec{1}{1}{1}{1} \| p^\downarrow )_{4,3}
				\tr{\mathit{Co}} (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{2}{3}}{0}{1} \cvec{\frac{1}{3}}{\frac{2}{3}}{0}{1} \| \varepsilon )_{4,4}                                                                                                      \\
				\tr{U}\tr{\mathit{Ca}} & (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{2}{3}}{0}{1} \cvec{\frac{1}{3}}{\frac{2}{3}}{0}{1}\cvec{1}{1}{1}{1} \| p^\downarrow )_{5,4}
				\tr{\mathit{Co}} (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{2}{3}}{0}{1} \cvec{\frac{1}{3}}{\frac{2}{3}}{0}{1}\cvec{\frac{1}{3}}{\frac{7}{9}}{0}{1} \| \varepsilon )_{4,4}                                                                 \\
				\tr{U}\tr{\mathit{Ca}} & (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{2}{3}}{0}{1} \cvec{\frac{1}{3}}{\frac{2}{3}}{0}{1}\cvec{\frac{1}{3}}{\frac{7}{9}}{0}{1}\cvec{1}{1}{1}{1} \| p^\downarrow )_{6,5}
				\tr{\mathit{Co}} (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{2}{3}}{0}{1} \cvec{\frac{1}{3}}{\frac{2}{3}}{0}{1}\cvec{\frac{1}{3}}{\frac{7}{9}}{0}{1}\cvec{\frac{7}{18}}{\frac{7}{9}}{0}{1} \| \varepsilon )_{6,6}                           \\
				\tr{U}\tr{\mathit{Ca}} & (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{2}{3}}{0}{1} \cvec{\frac{1}{3}}{\frac{2}{3}}{0}{1}\cvec{\frac{1}{3}}{\frac{7}{9}}{0}{1}\cvec{\frac{7}{18}}{\frac{7}{9}}{0}{1}\cvec{1}{1}{1}{1} \| p^\downarrow )_{7,6}
				\tr{\mathit{Co}} (\cvec{0}{0}{0}{0}\cvec{0}{0}{0}{1} \cvec{0}{\frac{2}{3}}{0}{1} \cvec{\frac{1}{3}}{\frac{2}{3}}{0}{1}\cvec{\frac{1}{3}}{\frac{7}{9}}{0}{1}\cvec{\frac{7}{18}}{\frac{7}{9}}{0}{1}\cvec{\frac{7}{18}}{\frac{43}{54}}{0}{1} \| \varepsilon )_{7,7}
				\cdots
			\end{align*}
		}
		\caption{The non-terminating execution of {\ADPDR} with the simple initial heuristics for the max reachability problem of Example~\ref{ex:pdr:MDPpositive}. The elements of the positive chain, with the exception of the last one $x_{n-1}$ are those of the initial chain.}
		\label{fig:pdr:exsimpleinitialMDPs}
	\end{figure}

	\begin{figure}[t]
		{\footnotesize
			\begin{align*}
				                                                                                                                                                                                    & (\emptyset\cvec{0}{0}{0}{0}\cvec{1}{1}{1}{1} \| \varepsilon)_{3,3}
				\tr{\mathit{Ca}} (\emptyset\cvec{0}{0}{0}{0}\cvec{1}{1}{1}{1} \| p^\downarrow )_{3,2} \tr{\mathit{Co}} (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \| \varepsilon)_{3,3} & b(\cvec{0}{0}{0}{0})=\cvec{0}{0}{0}{1} \quad \mathcal{Z}_2 = \{\cvec{\frac{2}{5}}{0}{0}{1}, \cvec{\frac{2}{5}}{1}{0}{1},\cvec{\frac{2}{5}}{0}{1}{1}, \cvec{\frac{2}{5}}{1}{1}{1} \}                                                                                                                                                                                                                                                                                                                                                \\
				\tr{U}  \tr{\mathit{Ca}}                                                                                                                                                            & (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{1}{1}{1}{1}\| p^\downarrow)_{4,3} \tr{\mathit{Co}} (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{1}{0}{1} \| \varepsilon)_{4,4}                                                                                                                                     & b(\cvec{\frac{2}{5}}{0}{0}{1})= \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \quad \mathcal{Z}_3 = \{ \cvec{\frac{2}{5}}{1}{0}{1}, \cvec{\frac{2}{5}}{1}{1}{1} \}          \\
				\tr{U} \tr{\mathit{Ca}}                                                                                                                                                             & (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{1}{0}{1} \cvec{1}{1}{1}{1} \| p^\downarrow)_{5,4}  \tr{D}  (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{1}{0}{1} \cvec{1}{1}{1}{1} \| \mathcal{F}^1_\xi p^\downarrow)_{5,3}                                                                           & b(\cvec{\frac{2}{5}}{1}{0}{1})= \cvec{max(\frac{2}{5},\frac{1}{2})}{\frac{4}{5}}{0}{1} \not \in p^\downarrow                                                        \\
				\tr{\mathit{Co}}                                                                                                                                                                    & (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \cvec{1}{1}{1}{1} \| p^\downarrow)_{5,4}                                                                                                                                                                                                                        & b(\cvec{\frac{2}{5}}{0}{0}{1})= \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \quad \mathcal{Z}_3 = \{ \cvec{1}{\frac{4}{5}}{0}{1} \}                                       \\
				\tr{\mathit{Co}}                                                                                                                                                                    & (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \cvec{\frac{2}{5}}{1}{0}{1} \| \varepsilon)_{5,5}                                                                                                                                                                                                               & b(\cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1})= \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \quad \mathcal{Z}_4 =\{ \cvec{\frac{2}{5}}{1}{0}{1}, \cvec{\frac{2}{5}}{1}{1}{1} \} \\
				\tr{U} \tr{\mathit{Ca}}                                                                                                                                                             & (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \cvec{\frac{2}{5}}{1}{0}{1} \cvec{1}{1}{1}{1} \| p^\downarrow)_{6,5} \tr{D} (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \cvec{\frac{2}{5}}{1}{0}{1} \cvec{1}{1}{1}{1} \| \mathcal{F}^1_\xi p^\downarrow)_{6,4} & b(\cvec{\frac{2}{5}}{1}{0}{1})= \cvec{max(\frac{2}{5},\frac{1}{2})}{\frac{4}{5}}{0}{1} \not \in p^\downarrow                                                        \\
				\tr{\mathit{Co}}                                                                                                                                                                    & (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \cvec{1}{1}{1}{1} \|  p^\downarrow)_{6,4}                                                                                                                                                                                 & b(\cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1})= \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1}\quad \mathcal{Z}_3 = \{ \cvec{1}{\frac{4}{5}}{0}{1} \}
			\end{align*}
		}
		\caption{On the left, the execution of {\ADPDR}$_{\texttt{hCo01}}$ for the max reachability problem of Example~\ref{ex:pdr:MDPpositive}: in the last state, it returns true since $x_3=x_4$. On the right, the data explaining the choices of (Conflict) and (Decide). Note that, in the two (Decide) steps, the guard $b(x_{k-1}) \notin Y_k$ holds because of the possibility of choosing the label $b$ in state $s_0$. This explain why  $Z$  is taken as $\mathcal{F}^1_\xi(p^{\downarrow})$ for the scheduler $\xi$ defined in~\eqref{eq:pdr:xi}.}
		\label{fig:pdr:exgen}
	\end{figure}

	\begin{figure}[t]
		{\footnotesize
			\begin{align*}
				                                                                                                                                                                                    & (\emptyset\cvec{0}{0}{0}{0}\cvec{1}{1}{1}{1} \| \varepsilon)_{3,3}
				\tr{\mathit{Ca}} (\emptyset\cvec{0}{0}{0}{0}\cvec{1}{1}{1}{1} \| p^\downarrow )_{3,2} \tr{\mathit{Co}} (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \| \varepsilon)_{3,3} & { b(\cvec{0}{0}{0}{0})=\cvec{0}{0}{0}{1} \quad \mathcal{Z}_2 = \{\cvec{\frac{2}{5}}{0}{0}{1}, \cvec{\frac{2}{5}}{1}{0}{1},\cvec{\frac{2}{5}}{0}{1}{1}, \cvec{\frac{2}{5}}{1}{1}{1} \}  }                                                                                                                                                                                                                                                                                           \\
				\tr{U}  \tr{\mathit{Ca}}                                                                                                                                                            & (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{1}{1}{1}{1}\| p^\downarrow)_{4,3} \tr{\mathit{Co}}  (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \| \varepsilon)_{4,4}                                                                                & {b(\cvec{\frac{2}{5}}{0}{0}{1})= \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \quad \mathcal{Z}_3 = \{ \cvec{\frac{2}{5}}{1}{0}{1}, \cvec{\frac{2}{5}}{1}{1}{1} \} } \\
				\tr{U} \tr{\mathit{Ca}}                                                                                                                                                             & (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \cvec{1}{1}{1}{1} \| p^\downarrow)_{5,4}  \tr{\mathit{Co}}  (\emptyset\cvec{0}{0}{0}{0}\cvec{\frac{2}{5}}{0}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \| p^\downarrow)_{5,4} & {b(\cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1})= \cvec{\frac{2}{5}}{\frac{4}{5}}{0}{1} \quad \mathcal{Z}_4 = \mathcal{Z}_3 }
			\end{align*}
		}
		\caption{On the left, the execution of {\ADPDR}$_{\texttt{hCoB}}$ for the max reachability problem of Example~\ref{ex:pdr:MDPpositive}: in the last state, it returns true since $x_3=x_4$. On the right, the data explaining the three choices of (Conflict). }\label{fig:pdr:exgen2}
	\end{figure}

	With the simple initial heuristic, {\ADPDR} does not terminate (Figure~\ref{fig:pdr:exsimpleinitialMDPs}). With the heuristic \verb|hCo01| using scheduler $\xi$, it returns true in 14 steps (Figure~\ref{fig:pdr:exgen}), while with \verb|hCoB| in 8 (Figure~\ref{fig:pdr:exgen2}). The first 4 steps of both \verb|hCoB| and \verb|hCo01| are the same: in the first (Conflict) $z_B = z_{01}$, while in the second $z_{01}(s_1)=1$ and $z_{B}(s_1)=\frac{4}{5}$, leading to the two different executions. Observe that this difference is due to the fact that in $p^\downarrow$, the coefficient corresponding to $s_1$, namely $r_{s_1}$, is $0$ and, since $\mathcal{Z}_3 \neq \emptyset$, then $z_{01}(s_1)= \lceil z_{B}(s_1)\rceil$.
\end{example}

\section{{\APDRAI}}
In Section~\ref{sec:pdr:APDR} we have introduced {\APDR} and in Section~\ref{sec:pdr:downset} {\ADPDR}: the latter is inspired by the former but takes the positive chain in some lattice $L$ and the negative sequence in $L^\downarrow$.
In this section, we introduce {\APDRAI}, a third algorithm that generalises both by allowing to manipulate positive and negative sequences in two different lattices.
The interest in this generalisation is not just theoretical, but it is also convenient in practice since it allows us in the next section to use an implementation of {\APDRAI} as a common template for both {\APDR} and {\ADPDR}.

The framework for {\APDRAI} is a diagram
\begin{equation}
	\xymatrix{
	(L, \le_L) \lloop{b} \ar[r]^-{\gamma}
	&(C, \le_C) \rloop{\overline{b} \dashv \overline{b}_r}
	}
	\label{eq:pdr:a-l}
\end{equation}
where $(L, \le_L)$ and $(C, \le_C)$ are complete lattices, $b\colon L \to L$ is an $\omega$-continuous function, $\overline{b} \dashv \overline{b}_r\colon C \to C$, and $\gamma: L \to C$ is a function satisfying
\begin{enumerate}
	\item \emph{order-embeddingness}: $x \le y$ if and only if $\gamma (x) \le \gamma (y)$ for all $x, y \in L$, and
	\item \emph{forward completeness}: $\overline{b}\gamma = \gamma b$.
\end{enumerate}

In this setting the problem $\mu b \le p$ in $L$ has an equivalent formulation in  $C$:
\begin{prop}\label{prop:pdr:alp_gamma}
	Consider the framework \eqref{eq:pdr:a-l} and let $p \in L$.
	Then $\lfp(b) \le p$ is equivalent to $\lfp(\overline{b} \lor \gamma \bot) \le \gamma p$.
\end{prop}
\begin{proof}
	\begin{align*}
		\lfp(b) \le p
		 & \iff \forall n \in \mathbb{N}.~b^n \bot \le p                                                                              \\
		 & \iff \forall n \in \mathbb{N}.~\gamma b^n \bot \le \gamma p            & [\text{because $\gamma$ is order-embedding}]      \\
		 & \iff \forall n \in \mathbb{N}.~\overline{b}^n \gamma \bot \le \gamma p & [\text{by the forward completeness}]              \\
		 & \iff \lfp(\overline{b} \lor  \gamma \bot) \le \gamma p                 & [\text{since }\overline{b} \dashv \overline{b}_r]
	\end{align*}
\end{proof}
The reader may have recognised some similarities with the proof of Proposition~\ref{prop:pdr:prob_down_up}. Indeed, as we will show in Example~\ref{ex:pdr:cat}, the latter result is an instance of Proposition~\ref{prop:pdr:alp_gamma}.

\begin{example}
	Consider a Galois insertion with a forward complete abstract interpretation $b\colon L \to L$ to a concrete semantic function $\overline{b}\colon C \to C$. If the function $\overline{b}$ has a right adjoint, then this is an instance of \eqref{eq:pdr:a-l}.
\end{example}

\begin{example}\label{ex:pdr:cat}
	It is easy to check that \eqref{eq:pdr:lowersetadjunction} is an instance of \eqref{eq:pdr:a-l}. Hereafter, we illustrate how this can be understood in categorical terms. For a complete lattice $L$ and an $\omega$-continuous function $b\colon L \to L$, we shall see $L$ as a $\mathbf{Bool}$-enriched category where $\mathbf{Bool}$ is the monoidal category with two objects $\bot, \top$, one arrow $\bot \rightarrow \top$ and monoidal product given by $\land$.
	Then the left Kan extension $\mathrm{Lan}_{y} (y \circ b)$ along the Yoneda embedding $y\colon L \to \mathbf{Bool}^{L^\op}$ is a left adjoint of the induced functor $b^* \eqdef (-) \circ b^\op$.
	\begin{displaymath}
		\xymatrix{
		L \lloop{b} \ar[r]^-{y}
		&\mathbf{Bool}^{L^\op} \rloop{\mathrm{Lan}_{y} (y \circ b) \dashv b^*}
		}
	\end{displaymath}
	The above diagram is the same as \eqref{eq:pdr:lowersetadjunction} under the isomorphism $\mathbf{Bool}^{L^\op} \cong L^\downarrow$. It is an instance of \eqref{eq:pdr:a-l} because:
	\begin{enumerate}
		\item $y$ is order-embedding since $y$ is full and faithful and
		\item $y$ is forward complete since the unit $b \Rightarrow \mathrm{Lan}_y (y \circ b) \circ y$ is an isomorphism by \cite[Proposition~4.23]{Kelly82} and full and faithfulness of $y$.
	\end{enumerate}
\end{example}

The algorithm \APDRAI checking $\mu b \le p$ in $L$ is illustrated in Figure~\ref{fig:pdr:adjai}. It is an adaptation of {\APDR} algorithm checking $\lfp(\overline{b} \lor \gamma \bot) \le \gamma p$ in $C$ (that, by Proposition~\ref{prop:pdr:alp_gamma}, is equivalent to $\lfp(b) \le p$) that takes positive sequences in $L$.

More precisely, \APDRAI manipulates pairs $(\vec{x} \| \vec{y})_{n, k}$ of sequences $\vec{x}$ in $L$ and $\vec{y}$ in $C$ while \APDR manipulates pairs of sequences in $C$. The pairs in {\APDRAI} satisfy invariants \eqref{eq:pdr:x0bot}, \eqref{eq:pdr:invi}, \eqref{eq:pdr:positivechain} and (PN'): $\forall j \in [k, n - 1] \text{, } \gamma x_j \not\le y_j$. Observe that $(\bot,\gamma \vec{x}) \eqdef (\bot,\gamma x_0, \dots , \gamma x_{n-1})$ forms a positive chain (Definition~\ref{def:pdr:posi_seq}) and $\vec{y}$ forms a negative sequence (Definition~\ref{def:pdr:neg_seq}) for $\lfp(\overline{b} \lor \gamma \bot) \le \gamma p$ in $C$.
The algorithm returns true if $\vec{x}$ is conclusive (i.e., $\exists j\in [0,n-2]\,.\, x_{j+1} \le x_j$) which implies $(\bot, \gamma \vec{x})$ is also conclusive. It returns false if $y_0$ is defined, or equivalently, if $\vec{y}$ in the pair $(\bot, \gamma \vec{x} \| \vec{y})$ is conclusive. This equivalence is deduced from \eqref{eq:pdr:x0bot} and $\gamma x_0 \not \le y_0$ by (PN').

The above discussion immediately yields the soundness of \APDRAI. One can also prove the properties of canonical choices, impossibility of loops, and negative termination for \APDRAI{} in the same way as for \ADPDR{}, yielding the following result.
\begin{theorem}
	All results in Section~\ref{sec:pdr:properties}, but Proposition~\ref{prop:pdr:CanonicalChoice}.3, hold for \APDRAI{}.
\end{theorem}

\begin{figure}[t]
	\centering
	\underline{{\APDRAI} $(b,p, \gamma\colon L \to C, \overline{b}, \overline{b}_r)$}
	\begin{codeNT}
<INITIALISATION>
  $( \vec{x} \| \vec{y} )_{n,k}$ := $(\bot,\top\|\varepsilon)_{2,2}$
<ITERATION>			                % $\vec{x},\vec{y}$  not conclusive
  case $( \vec{x} \| \vec{y} )_{n,k}$ of
	  $\vec{y}=\varepsilon$ And $x_{n-1} \le p$     :                    %(Unfold)
		$( \vec{x} \| \vec{y} )_{n,k}$ := $( \vec{x}, \top \| \varepsilon )_{n+1,n+1}$
	  $\vec{y}=\varepsilon$ And $x_{n-1} \not \le p$    :                     %(Candidate)
		choose $z\in C$ st $\gamma x_{n-1} \not \le z$ And  $\gamma p \le z$;
		$( \vec{x} \| \vec{y} )_{n,k}$ := $( \vec{x} \| z )_{n,n-1}$
	  $\vec{y} \neq \varepsilon$ And $\overline{b}\gamma x_{k-1} \not \le y_k$ :                        %(Decide)
		choose $z \in C$ st $\gamma x_{k-1} \not \le z$ And $\overline{b}_r(y_k) \le z$;
		$(\vec{x} \| \vec{y} )_{n,k}$ := $(\vec{x} \| z , \vec{y} )_{n,k-1}$
	  $\vec{y} \neq \varepsilon$ And $\gamma b x_{k-1} \le y_k$ :                        %(Conflict)
		choose $z \in L$ st $\gamma z \le y_k$ And $b(x_{k-1} \land z) \le z$;
		$(\vec{x} \| \vec{y} )_{n,k}$ := $(\vec{x} \land_k z \| \mathsf{tail}(\vec{y}) )_{n,k+1}$
  endcase
<TERMINATION>
  if $\exists j\in [0,n-2]\,.\, x_{j+1} \le x_j$ then return true		 % $\vec{x}$ conclusive
  if $y_0$ is defined then return false	  	% $\vec{y}$ conclusive
\end{codeNT}
	\caption{{\APDRAI} algorithm checking $\mu b \le p$.}\label{fig:pdr:adjai}
\end{figure}

We conclude this section by illustrating how the algorithm \APDRAI{} generalises both \APDR{} and \ADPDR{}.
\begin{prop}\label{prop:pdr:instanceAI}
	\APDR{} is an instance of \APDRAI{}.
\end{prop}
\begin{proof}
	Consider the setting for \APDR with $(i, f, g, p)$, i.e.~$i, p \in L$ and $f \dashv g \colon L \to L$. Let $\Li$ be the complete lattice $\{ x \in L \mid i \le x \}$ (with the same order as $L$) and let $s \colon \Li \hookrightarrow L$ be the inclusion function. Then \APDRAI with parameters $(f \lor i, p, s\colon \Li \hookrightarrow L, f, g)$ defines exactly \APDR{} with parameters $(i, f, g, p)$ and starting state $(\bot, i, \top \| \epsilon)_{3, 3}$ (reachable in \APDR{} applying (Candidate), (Conflict) and (Unfold)).
\end{proof}

\begin{prop}\label{prop:pdr:instanceAI2}
	\ADPDR{} is an instance of \APDRAI{}.
\end{prop}
\begin{proof}
	Consider the setting for \ADPDR{}, i.e.~$(b, p)$ with an $\omega$-continuous function $b\colon L \to L$ and $p \in L$.
	Then \APDRAI{} with parameters $(b,p, (-)^\downarrow \colon L \to L^\downarrow,b^\downarrow, b^\downarrow_r)$ defines exactly \ADPDR{} with parameters $(b, p)$ under the correspondence between intermediate data $(\vec{x} \| \vec{y})_{n, k}$ in \APDRAI{} and $(\emptyset, \vec{x} \| \vec{Y})$ in \ADPDR{} where $Y_{i+1} \coloneqq y_{i}$ for each $i \in \{k, \dots, n-1\}$. Please note that $x^\downarrow \subseteq Z$ if and only if $x \in Z$ for a lower set $Z$.
\end{proof}

\section{Implementation}\label{sec:pdr:experiments}
We first developed, using Haskell and exploiting its abstraction features, a common template of {\APDRAI} that accommodates both {\APDR} and {\ADPDR}.
It is a  program parametrized by two lattices---used for positive chains and negative sequences, respectively---and by a heuristic.

For our experiments, we instantiated the template to \ADPDR{} for MDPs (letting $L=[0,1]^{S}$), with three different heuristics: \verb|hCoB| and \verb|hCo01| from Proposition~\ref{prop:pdr:genlegit}; and \verb|hCoS| introduced below. Besides the template ($\sim$100 lines),  we needed $\sim$140 lines to account for \verb|hCoB| and \verb|hCo01|, and additional $\sim$100 lines to further obtain \verb|hCoS|. All this indicates a clear benefit of our abstract theory: a general template can itself be coded succinctly; instantiation to concrete problems is easy, too, thanks to an explicitly specified interface of heuristics.

Our implementation accepts MDPs  expressed in a symbolic format inspired by Prism models~\cite{KNP11}, in which states are variable valuations and  transitions are described by symbolic functions (they can be segmented with symbolic guards $\{\text{guard}_i\}_{i}$). We use rational arithmetic (\verb|Rational| in Haskell) for probabilities to avoid rounding errors.

\paragraph{Heuristics.}
The three heuristics (\verb|hCoB|, \verb|hCo01|, \verb|hCoS|) use the same choices in (Candidate) and (Decide), as defined in \eqref{eq:pdr:secondterminatingheuristics}, but different ones in (Conflict).

The third heuristics \verb|hCoS| is a \emph{symbolic} variant of \verb|hCoB|; it relies on our symbolic model format. It uses $z_{S}$ for $z$ in (Conflict), where $z_{S}(s)=z_{B}(s)$ if $r_{s}\neq 0$ or $\mathcal{Z}_k=\emptyset$. The definition of $z_{S}(s)$ otherwise is notable: we use a piecewise affine function $(t_{i}\cdot s + u_{i})_{i}$ for $z_{S}(s)$, where the affine functions $(t_{i}\cdot s + u_{i})_{i}$ are guarded by the same guards $\{\text{guard}_i\}_{i}$ of the MDP's transition function. We let the SMT solver Z3~\cite{MB08} search for the values of the coefficients $t_{i}, u_{i}$, so that $z_{S}$ satisfies the requirements of (Conflict) (namely $b(x_{k-1})(s) \leq z_{S}(s) \leq 1$ for each $s\in S$ with $r_s=0$), together with the condition $b (z_{S}) \leq z_{S}$ for faster convergence. If the search is unsuccessful, we give up \verb|hCoS| and fall back on the \verb|hCoB|.

As a task common to the three heuristics, we need to calculate $\mathcal{Z}_k = \{d \in \mathcal{G}_k \mid b(x_{k-1}) \leq d\}$ in (Conflict) (see~(\ref{eq:pdr:heuristicsZConflict})). Rather than computing the whole set $\mathcal{G}_k$ of generating points of the linear inequality that defines $Y_{k}$, we implemented an ad-hoc algorithm that crucially exploits the condition $b(x_{k-1}) \leq d$ for pruning.

\paragraph{Experiment Settings.}
We conducted the experiments on Ubuntu 18.04 and AWS t2.xlarge (4 CPUs, 16 GB memory, up to 3.0 GHz Intel Scalable Processor). We used several Markov chain (MC) benchmarks and a couple of MDP ones.

%\begin{table}[tb]\scriptsize
%  \caption{Experimental results on MC benchmarks.
% $|S|$ is the number of states, $P$ is the
%reachability probability (calculated by manual inspection), $\lambda$ is the threshold in the problem $P\le_{?} \lambda$ (shaded if the answer is no). The other columns show the average execution time in seconds; TO is timeout (\SI{900}{\second}); MO is out-of-memory.
%For \ADPDR{} and LT-PDR
%we used the \texttt{tasty-bench} Haskell package and repeated executions until std.\ dev.\ is $<$ 5\% (at least three execs). For PrIC3 and Storm, we made five executions. Storm's execution does not depend on $\lambda$: it seems to answer queries of the form $P\le_{?} \lambda$ by calculating $P$. We observed a wrong answer for the entry with $(\dagger)$ (Storm, sp.-num., Haddad-Monmege); see the discussion of RQ2.
%} \label{tb:mc}
%\begin{lrbox}{\lstbox}\begin{minipage}{\textwidth}
%  \npdecimalsign{.}
%  \nprounddigits{3}
%	       \begin{tabular}{ccccccccccccccc}
%    	       \toprule
%    	       Benchmark & $|S|$ &
%    	       %
%    	       $P$
%    	       & $\lambda$
%    	       &\multicolumn{3}{c}{\ADPDR}
%    	       &%
%     	       LT-PDR
%    	       &\multicolumn{4}{c}{PrIC3}
%    	       &\multicolumn{3}{c}{Storm}
%    	       \\\cmidrule(lr){5-7} \cmidrule(lr){9-12} \cmidrule(lr){13-15}
%    	       &&&&\verb|hCoB| &\verb|hCo01| &\verb|hCoS|
%    	       & &none & lin. & pol. & hyb. &sp.-num. &sp.-rat. &sp.-sd. \\
%    	       \midrule\midrule
%    	       \multirow{4}{*}{Grid} & \multirow{2}{*}{$10^2$} & \multirow{2}{*}{0.033} & 0.3 & 0.013 &0.022 & 0.659 & 0.343 & 1.383 &23.301 &MO &MO &\multirow{2}{*}{0.010} &\multirow{2}{*}{0.010} &\multirow{2}{*}{0.010} \\
%               &   &   & 0.2 & 0.013 &0.031 & 0.657 & 0.519 & 1.571 &26.668 &TO &MO \\
%    	       \cmidrule[0.5pt](r){2-15}
%     	       & \multirow{2}{*}{$10^3$} & \multirow{2}{*}{$<$0.001} & 0.3 & 1.156 &2.187 & 5.633 & 126.441 & TO &TO &TO &MO &\multirow{2}{*}{0.010} &\multirow{2}{*}{0.017} &\multirow{2}{*}{0.011} \\
%               &   &   & 0.2 & 1.146 &2.133 & 5.632 & 161.667 & TO &TO &TO &MO \\
%    	       \midrule
%    	       \multirow{3}{*}{BRP} & \multirow{3}{*}{$10^3$} & \multirow{3}{*}{0.035} & 0.1 & 12.909 &7.969 & 55.788 & TO & TO &TO &MO &MO &\multirow{3}{*}{0.012} &\multirow{3}{*}{0.018} &\multirow{3}{*}{0.011} \\
%               &   &   & \cellcolor{gray!25}0.01  & 1.977 &8.111 & 5.645 & 21.078 & 60.738 &626.052 &524.373 &823.082  \\
%               &   &   & \cellcolor{gray!25}0.005 & 0.604 &2.261  & 2.709 & 1.429 & 12.171 &254.000 &197.940 &318.840  \\
%      	       \midrule
%    	       \multirow{8}{*}{\parbox{4em}{Zero- Conf}} & \multirow{4}{*}{$10^2$} & \multirow{4}{*}{0.5} & 0.9 &1.217 &68.937  & 0.196 & TO & 19.765 &136.491 &0.630 &0.468 &\multirow{4}{*}{0.010} &\multirow{4}{*}{0.018} &\multirow{4}{*}{0.011} \\
%               &   &   & 0.75 & 1.223 & 68.394 & 0.636 & TO & 19.782 &132.780 &0.602 &0.467  \\
%               &   &   & 0.52 & 1.228 & 60.024 & 0.739& TO & 19.852 &136.533 &0.608 &0.474  \\
%               &   &   & \cellcolor{gray!25}0.45 &$<$0.001 & 0.001 & 0.001 & $<$0.001 & 0.035 &0.043 &0.043 &0.043 \\
%    	       \cmidrule[0.5pt](r){2-15}
%     	       & \multirow{4}{*}{$10^4$} & \multirow{4}{*}{0.5} & 0.9 &MO & TO & 7.443 & TO & TO &TO &0.602 &0.465 &\multirow{4}{*}{0.037} &\multirow{4}{*}{262.193} &\multirow{4}{*}{0.031} \\
%               &   &   & 0.75 & MO & TO & 15.223 & TO & TO &TO &0.599 &0.470  \\
%               &   &   & 0.52 & MO & TO & TO & TO &TO &TO &0.488 & 0.475 \\
%               &   &   & \cellcolor{gray!25}0.45 &0.108 & 0.119 & 0.169 & 0.016 & 0.035 &0.040 &0.040 &0.040  \\
%    	       \midrule
%    	       \multirow{4}{*}{Chain} & \multirow{4}{*}{$10^3$} & \multirow{4}{*}{0.394} & 0.9 & 36.083 & TO & 0.478 & TO & 269.801 &TO &0.938 &0.686 &\multirow{4}{*}{0.010} &\multirow{4}{*}{0.014} &\multirow{4}{*}{0.011} \\
%               &   &   & 0.4 & 35.961 & TO & 394.955 & TO & 271.885 &TO &0.920 &TO\\
%               &   &   & \cellcolor{gray!25}0.35 & 101.351 & TO & 454.892 & 435.199 & 238.613 &TO &TO &TO  \\
%               &   &   & \cellcolor{gray!25}0.3 & 62.036 & 463.981 & 120.557 & 209.346 & 124.829 &746.595 &TO &TO \\
%    	       \midrule
%    	       \multirow{4}{*}{\parbox{4em}{Double- Chain}} & \multirow{4}{*}{$10^3$} & \multirow{4}{*}{0.215} & 0.9 & 12.122 & 7.318 & TO & TO & TO &TO &1.878 &2.053 &\multirow{4}{*}{0.011} &\multirow{4}{*}{0.018} &\multirow{4}{*}{0.010} \\
%               &   &   & 0.3 & 12.120 & 20.424 & TO & TO & TO &TO &1.953 &2.058 \\
%               &   &   & 0.216 & 12.096 & 19.540 & TO & TO & TO &TO &172.170 &TO \\
%               &   &   & \cellcolor{gray!25}0.15 & 12.344 & 16.172 & TO & 16.963 & TO &TO &TO &TO \\
%    	       \midrule
%    	       \multirow{4}{*}{\parbox{4em}{Haddad- Monmege}} & \multirow{2}{*}{$41$} & \multirow{2}{*}{0.7} & 0.9 & 0.004 & 0.009 &8.528 & TO & 1.188 &31.915 &TO &MO &\multirow{2}{*}{0.011} &\multirow{2}{*}{0.011} &\multirow{2}{*}{1.560} \\
%               &   &   & 0.75 & 0.004 & 0.011 & 2.357 & TO & 1.209 &32.143 &TO &712.086 \\
%    	       \cmidrule[0.5pt](r){2-15}
%     	       & \multirow{2}{*}{$10^3$} & \multirow{2}{*}{0.7} & 0.9 & 59.721 & 61.777 &TO & TO & TO &TO &TO &TO &\multirow{2}{*}{0.013 $(\dagger)$} &\multirow{2}{*}{0.043} &\multirow{2}{*}{TO} \\
%               &   &   & 0.75 & 60.413 & 63.050 & TO & TO & TO &TO &TO &TO \\
%    	       \bottomrule
%  	       \end{tabular}
%\end{minipage}\end{lrbox}
%\scalebox{.71}{\centering\usebox\lstbox}
%
%\end{table}
\begin{sidewaystable}
	\caption{\small Experimental results on MC benchmarks.
		$|S|$ is the number of states, $P$ is the reachability probability (calculated by manual inspection), $\lambda$ is the threshold in the problem $P\le_{?} \lambda$ (shaded if the answer is no). The other columns show the average execution time in seconds; TO is timeout (\SI{900}{\second}); MO is out-of-memory.
		For \ADPDR{} and LT-PDR we used the \texttt{tasty-bench} Haskell package and repeated executions until std.\ dev.\ is $<$ 5\% (at least three execs). For PrIC3 and Storm, we made five executions. Storm's execution does not depend on $\lambda$: it seems to answer queries of the form $P\le_{?} \lambda$ by calculating $P$. We observed a wrong answer for the entry with $(\dagger)$ (Storm, sp.-num., Haddad-Monmege); see the discussion of RQ2.
	}
	\label{tb:pdr:mc}
	\centering
	\begin{lrbox}{\lstbox}\begin{minipage}{\textheight}
			\npdecimalsign{.}
			\nprounddigits{3}
			\begin{tabular}{ccccccccccccccc}
				\toprule
				Benchmark                                      & $|S|$                      &
				%
				$P$
				                                               & $\lambda$
				                                               & \multicolumn{3}{c}{\ADPDR}
				                                               &                                                                                                                                                                                                                                                                          %
				LT-PDR
				                                               & \multicolumn{4}{c}{PrIC3}
				                                               & \multicolumn{3}{c}{Storm}
				\\\cmidrule(lr){5-7} \cmidrule(lr){9-12} \cmidrule(lr){13-15}
				                                               &                            &                           &                          & \verb|hCoB| & \verb|hCo01| & \verb|hCoS|
				                                               &                            & none                      & lin.                     & pol.        & hyb.         & sp.-num.    & sp.-rat. & sp.-sd.                                                                                                                        \\
				\midrule\midrule
				\multirow{4}{*}{Grid}                          & \multirow{2}{*}{$10^2$}    & \multirow{2}{*}{0.033}    & 0.3                      & 0.013       & 0.022        & 0.659       & 0.343    & 1.383   & 23.301  & MO      & MO      & \multirow{2}{*}{0.010}             & \multirow{2}{*}{0.010}   & \multirow{2}{*}{0.010} \\
				                                               &                            &                           & 0.2                      & 0.013       & 0.031        & 0.657       & 0.519    & 1.571   & 26.668  & TO      & MO                                                                                               \\
				\cmidrule[0.5pt](r){2-15}
				                                               & \multirow{2}{*}{$10^3$}    & \multirow{2}{*}{$<$0.001} & 0.3                      & 1.156       & 2.187        & 5.633       & 126.441  & TO      & TO      & TO      & MO      & \multirow{2}{*}{0.010}             & \multirow{2}{*}{0.017}   & \multirow{2}{*}{0.011} \\
				                                               &                            &                           & 0.2                      & 1.146       & 2.133        & 5.632       & 161.667  & TO      & TO      & TO      & MO                                                                                               \\
				\midrule
				\multirow{3}{*}{BRP}                           & \multirow{3}{*}{$10^3$}    & \multirow{3}{*}{0.035}    & 0.1                      & 12.909      & 7.969        & 55.788      & TO       & TO      & TO      & MO      & MO      & \multirow{3}{*}{0.012}             & \multirow{3}{*}{0.018}   & \multirow{3}{*}{0.011} \\
				                                               &                            &                           & \cellcolor{gray!25}0.01  & 1.977       & 8.111        & 5.645       & 21.078   & 60.738  & 626.052 & 524.373 & 823.082                                                                                          \\
				                                               &                            &                           & \cellcolor{gray!25}0.005 & 0.604       & 2.261        & 2.709       & 1.429    & 12.171  & 254.000 & 197.940 & 318.840                                                                                          \\
				\midrule
				\multirow{8}{*}{\parbox{4em}{Zero- Conf}}      & \multirow{4}{*}{$10^2$}    & \multirow{4}{*}{0.5}      & 0.9                      & 1.217       & 68.937       & 0.196       & TO       & 19.765  & 136.491 & 0.630   & 0.468   & \multirow{4}{*}{0.010}             & \multirow{4}{*}{0.018}   & \multirow{4}{*}{0.011} \\
				                                               &                            &                           & 0.75                     & 1.223       & 68.394       & 0.636       & TO       & 19.782  & 132.780 & 0.602   & 0.467                                                                                            \\
				                                               &                            &                           & 0.52                     & 1.228       & 60.024       & 0.739       & TO       & 19.852  & 136.533 & 0.608   & 0.474                                                                                            \\
				                                               &                            &                           & \cellcolor{gray!25}0.45  & $<$0.001    & 0.001        & 0.001       & $<$0.001 & 0.035   & 0.043   & 0.043   & 0.043                                                                                            \\
				\cmidrule[0.5pt](r){2-15}
				                                               & \multirow{4}{*}{$10^4$}    & \multirow{4}{*}{0.5}      & 0.9                      & MO          & TO           & 7.443       & TO       & TO      & TO      & 0.602   & 0.465   & \multirow{4}{*}{0.037}             & \multirow{4}{*}{262.193} & \multirow{4}{*}{0.031} \\
				                                               &                            &                           & 0.75                     & MO          & TO           & 15.223      & TO       & TO      & TO      & 0.599   & 0.470                                                                                            \\
				                                               &                            &                           & 0.52                     & MO          & TO           & TO          & TO       & TO      & TO      & 0.488   & 0.475                                                                                            \\
				                                               &                            &                           & \cellcolor{gray!25}0.45  & 0.108       & 0.119        & 0.169       & 0.016    & 0.035   & 0.040   & 0.040   & 0.040                                                                                            \\
				\midrule
				\multirow{4}{*}{Chain}                         & \multirow{4}{*}{$10^3$}    & \multirow{4}{*}{0.394}    & 0.9                      & 36.083      & TO           & 0.478       & TO       & 269.801 & TO      & 0.938   & 0.686   & \multirow{4}{*}{0.010}             & \multirow{4}{*}{0.014}   & \multirow{4}{*}{0.011} \\
				                                               &                            &                           & 0.4                      & 35.961      & TO           & 394.955     & TO       & 271.885 & TO      & 0.920   & TO                                                                                               \\
				                                               &                            &                           & \cellcolor{gray!25}0.35  & 101.351     & TO           & 454.892     & 435.199  & 238.613 & TO      & TO      & TO                                                                                               \\
				                                               &                            &                           & \cellcolor{gray!25}0.3   & 62.036      & 463.981      & 120.557     & 209.346  & 124.829 & 746.595 & TO      & TO                                                                                               \\
				\midrule
				\multirow{4}{*}{\parbox{4em}{Double- Chain}}   & \multirow{4}{*}{$10^3$}    & \multirow{4}{*}{0.215}    & 0.9                      & 12.122      & 7.318        & TO          & TO       & TO      & TO      & 1.878   & 2.053   & \multirow{4}{*}{0.011}             & \multirow{4}{*}{0.018}   & \multirow{4}{*}{0.010} \\
				                                               &                            &                           & 0.3                      & 12.120      & 20.424       & TO          & TO       & TO      & TO      & 1.953   & 2.058                                                                                            \\
				                                               &                            &                           & 0.216                    & 12.096      & 19.540       & TO          & TO       & TO      & TO      & 172.170 & TO                                                                                               \\
				                                               &                            &                           & \cellcolor{gray!25}0.15  & 12.344      & 16.172       & TO          & 16.963   & TO      & TO      & TO      & TO                                                                                               \\
				\midrule
				\multirow{4}{*}{\parbox{4em}{Haddad- Monmege}} & \multirow{2}{*}{$41$}      & \multirow{2}{*}{0.7}      & 0.9                      & 0.004       & 0.009        & 8.528       & TO       & 1.188   & 31.915  & TO      & MO      & \multirow{2}{*}{0.011}             & \multirow{2}{*}{0.011}   & \multirow{2}{*}{1.560} \\
				                                               &                            &                           & 0.75                     & 0.004       & 0.011        & 2.357       & TO       & 1.209   & 32.143  & TO      & 712.086                                                                                          \\
				\cmidrule[0.5pt](r){2-15}
				                                               & \multirow{2}{*}{$10^3$}    & \multirow{2}{*}{0.7}      & 0.9                      & 59.721      & 61.777       & TO          & TO       & TO      & TO      & TO      & TO      & \multirow{2}{*}{0.013 $(\dagger)$} & \multirow{2}{*}{0.043}   & \multirow{2}{*}{TO}    \\
				                                               &                            &                           & 0.75                     & 60.413      & 63.050       & TO          & TO       & TO      & TO      & TO      & TO                                                                                               \\
				\bottomrule
			\end{tabular}
		\end{minipage}\end{lrbox}
	\scalebox{.85}{\centering\usebox\lstbox}
\end{sidewaystable}

\paragraph{Research Questions.} We wish to address the following questions.
\begin{description}
	\item[RQ1] Does \ADPDR{} advance the state-of-the-art performance of \emph{PDR} algorithms for probabilistic model checking?
	\item[RQ2] How does \ADPDR{}'s performance compare against \emph{non-PDR} algorithms for probabilistic model checking?
	\item[RQ3] Does the  theoretical framework of \ADPDR{} successfully guide the discovery of various heuristics with practical performance?
	\item[RQ4] Does \ADPDR{} successfully manage nondeterminism in MDPs (that is absent in MCs)?
\end{description}

\paragraph{Experiments on MCs (Table~\ref{tb:pdr:mc}).}
We used six benchmarks: Haddad\hyp{}Monmege is from~\cite{HKPQR19}; the others are from~\cite{BJKKMS20,KUKSH22}. We compared \ADPDR (with three heuristics) against LT-PDR~\cite{KUKSH22}, PrIC3 (with four  heuristics \emph{none}, \emph{lin.}, \emph{pol.}, \emph{hyb.}, see~\cite{BJKKMS20}), and Storm 1.5~\cite{DJKV17}. Storm is a recent comprehensive toolsuite that implements different algorithms and solvers. Among them, our comparison is against \emph{sparse-numeric}, \emph{sparse-rational}, and \emph{sparse-sound}.
The \emph{sparse} engine uses explicit state space representation by sparse matrices; this is unlike another representative \emph{dd} engine that uses symbolic BDDs. (We did not use \emph{dd} since it often reported errors, and was overall slower than \emph{sparse}.)
\emph{Sparse-numeric}  is a value-iteration (VI) algorithm;  \emph{sparse-rational}  solves linear (in)equations using rational arithmetic;
\emph{sparse-sound} is a sound VI algorithm~\cite{QK18}.\footnote{There are two more sound algorithms in Storm: one that utilizes interval iteration~\cite{BKLPW17} and the other does optimistic VI~\cite{HK20}. We have excluded them from the results since we observed that they returned incorrect answers.}

\begin{table}[tb]
	\caption{Experimental results on MDP benchmarks. The legend is the same as Table~\ref{tb:pdr:mc}, except that $P$ is now the maximum reachability probability.}
	\label{tb:pdr:mdp}
	\centering
	{\small
		\begin{tabular}{cccccccccc}
			\toprule
			Benchmark                  & $|S|$                      &
			$P$\
			                           & $\lambda$
			                           & \multicolumn{3}{c}{\ADPDR}
			                           & \multicolumn{3}{c}{Storm}
			\\\cmidrule(lr){5-7}\cmidrule{8-10}
			                           &                            &                        &                         & \verb|hCoB| & \verb|hCo01| & \verb|hCoS| & sp.-num                & sp.-rat.               & sp.-sd.                \\
			\midrule\midrule
			\multirow{3}{*}{CDrive2}   & \multirow{3}{*}{38}        & \multirow{3}{*}{0.865} & 0.9                     & MO          & 0.172        & TO          & \multirow{3}{*}{0.019} & \multirow{3}{*}{0.019} & \multirow{3}{*}{0.018} \\
			                           &                            &                        & \cellcolor{gray!25}0.75 & MO          & 0.058        & TO                                                                                     \\
			                           &                            &                        & \cellcolor{gray!25}0.5  & 0.015       & 0.029        & 86.798                                                                                 \\
			\midrule
			\multirow{3}{*}{TireWorld} & \multirow{3}{*}{8670}      & \multirow{3}{*}{0.233} & 0.9                     & MO          & 3.346        & TO          & \multirow{4}{*}{0.070} & \multirow{4}{*}{0.164} & \multirow{4}{*}{0.069} \\
			                           &                            &                        & 0.75                    & MO          & 3.337        & TO                                                                                     \\
			                           &                            &                        & 0.5                     & MO          & 6.928        & TO                                                                                     \\
			                           &                            &                        & \cellcolor{gray!25}0.2  & 4.246       & 24.538       & TO                                                                                     \\
			\bottomrule
		\end{tabular}
	}
\end{table}

\paragraph{Experiments on MDPs (Table~\ref{tb:pdr:mdp}).}
We used two benchmarks from~\cite{HKPQR19}. We compared \ADPDR only against Storm, since RQ1 is already addressed using MCs (besides, PrIC3 did not run for MDPs).

\paragraph{Discussion.}
The experimental results suggest the following answers to the RQs.

\textbf{RQ1}.
The performance advantage of \ADPDR{}, over both LT-PDR and PrIC3, was clearly observed throughout the benchmarks.
\ADPDR{} outperformed LT-PDR, thus confirming empirically the theoretical observation in Section~\ref{sec:pdr:LTPDRvsADPDR}. The profit is particularly evident in those instances whose answer is positive.
\ADPDR{} generally outperformed PrIC3, too.
Exceptions are in ZeroConf, Chain and DoubleChain, where PrIC3 with  polynomial (pol.) and hybrid (hyb.) heuristics performs well. This seems to be thanks to the expressivity of the polynomial template in PrIC3, which is a possible enhancement we are yet to implement (currently our symbolic heuristic \verb|hCoS| uses only the affine template).

\textbf{RQ2}.
The comparison with Storm is interesting. Note first that Storm's \emph{sparse-numeric} algorithm is a VI algorithm that gives a guaranteed lower bound \emph{without guaranteed convergence}. Therefore  its positive answer to $P\le_{?}\lambda$ may not be correct. Indeed, for Haddad-Monmege with $|S|\sim 10^{3}$, it answered $P=0.5$ which is wrong ($(\dagger)$ in Table~\ref{tb:pdr:mc}). This is in contrast with PDR algorithms that discovers an explicit witness for  $P\le\lambda$ via their positive chain.

Storm's \emph{sparse-rational} algorithm is precise.
It was faster than PDR algorithms in many benchmarks, although \ADPDR was better or comparable in ZeroConf ($10^4$) and Haddad-Monmege ($41$), for $\lambda$ such that $P\le\lambda$ is true. We believe this suggests a general advantage of PDR algorithms, namely to accelerate the search for an invariant-like witness for safety.

Storm's \emph{sparse-sound} algorithm is a sound VI algorithm that returns correct answers aside numerical errors.
Its performance was similar to that of sparse-numeric, except for the two instances of Haddad-Monmege: sparse-sound returned correct answers but was much slower than sparse-numeric.
For these two instances, \ADPDR{} outperformed sparse-sound.

It seems that a big part of Storm's good performance is attributed to the sparsity of state representation. This is notable in the comparison of the two instances of Haddad-Monmege ($41$ vs.\ $10^3$): while Storm handles both of them easily, \ADPDR{} struggles a bit in the bigger instance. Our implementation can be extended to use sparse representation, too; this is future work.

\textbf{RQ3}.
We derived the three heuristics (\verb|hCoB|, \verb|hCo01|, \verb|hCoS|) exploiting the theory of \ADPDR{}. The experiments show that each heuristic has its own strength. For example, \verb|hCo01| is slower than \verb|hCoB| for MCs, but it is much better for MDPs. In general, there is no silver bullet heuristic, so coming up with a variety of them is important. The experiments suggest that our theory of \ADPDR{} provides great help in doing so.

\textbf{RQ4}.
Table~\ref{tb:pdr:mdp} shows that \ADPDR{} can handle nondeterminism well: once a suitable heuristic is chosen, its performances on MDPs and on MCs of similar size are comparable. It is also interesting that better-performing heuristics vary, as we discussed above.

\section{Summary}\label{sec:pdr:conclusions}
In this chapter, we presents \APDR{}, an algorithm that generalizes Bradley's PDR \cite{Bradley11} to address the least fixpoint problem $\lfp(f\lor i) \le p$. The novelty in the algorithm lies in the use of a right adjoint $g\colon L \to L$ of the map $f$ to search for counterexamples: the function $f$ is used in the search of an over-approximation to show that the least fixpoint is below $p$, while the adjoint $g$ produces candidate counterexamples -- that is, under-approximations -- to witness the violation of the property.

Similar to other PDR-like algorithms, \APDR{} depends on some non-deterministic choices of elements $z \in L$. Therefore, not only we proved soundness of the algorithm for any possible resolution of nondeterminism, but we also showed that the algorithm always progresses to a new state. In general, \APDR{} is not guaranteed to terminate since the problem $\lfp(f\lor i) \le p$ is undecidable. However, we showed that certain well-behaved heuristics, which essentially fix a legitimate choice of $z$, ensure termination whenever the problem has a negative answer.

The assumption that $f$ possesses a right adjoint $g$ is not satisfied in several interesting cases, particularly those involving probabilistic systems. To address these, we introduce a variation of the algorithm, called \ADPDR{}, that employs lower sets to guarantee the presence of a right adjoint. In this algorithm, a witness for the positive answer is sought in the lattice $L$, while a counterexample in the lattice of lower sets $L^\downarrow$. We demonstrated that most properties of \APDR{} hold for \ADPDR{} and that \ADPDR{} simulates LT-PDR~\cite{KUKSH22}, a preceding lattice-theoretical generalization of PDR. Conversely, LT-PDR cannot simulate \ADPDR{}: indeed, the use of $L^\downarrow$ enables \ADPDR{} to simultaneously search for multiple -- even all -- counterexamples computed by LT-PDR at the same time.

We instantiated \ADPDR{} to address the max-reachability problem of Markov Decision Processes. Specifically, we devised two heuristics, named \verb|hCoB| and \verb|hCo01|, which cleverly reduce the search space and are guaranteed to terminate in the case of a negative answer. We conducted experimental comparisons, using \APDRAI{} as a basis for implementation, of the two heuristics with other tools, yielding promising results: our implementation clearly outperforms other PDR-based algorithms in many benchmarks, and even compares favourably with Storm---a highly sophisticated toolsuite---in a couple of benchmarks. These are notable especially given that \ADPDR{} currently lacks enhancing features such as richer symbolic templates and sparse representation (adding which is future work). Overall, we believe that \ADPDR{} \emph{confirms the potential of PDR algorithms in probabilistic model checking}. Through the three heuristics, we also observed the value of an abstract general theory in devising heuristics in PDR, which is probably true of verification algorithms in general besides PDR.
