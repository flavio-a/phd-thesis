% !TEX root = ../phd-thesis.tex

\chapter{Related work}\label{ch:sota}
This chapter surveys related work about combining over and under-approximations.
Section~\ref{sec:sota:kat} analyses KATs as a possible formalism to describe together over and under-approximation, represented respectively by HL and IL.
Section~\ref{sec:sota:lcl} present $\LCLA$, a logic that exploits under-approximation to ensure precision of an (over-approximating) abstraction and is able to prove either program correctness or incorrectness with any triple.
Section~\ref{sec:sota:pdr} discusses \code{ic3}, a model-checking algorithm that combines over and under-approximations in a non-trivial way, and its generalizations.
Section~\ref{sec:sota:ol} introduces Outcome Logic, a triple based program logic that can express both over and under-approximation properties.

\section{Unifying formalism}\label{sec:sota:kat}
HL and IL operate on dual principles. Because of this, some properties are shared among the two, others are dualized and others still are just different. In order to highlight these similarities and differences, it is interesting to embed HL and IL in a common formalism. As recalled in Section~\ref{sec:bg:algebra}, KATs are able to encode regular commands \cite{Kozen97} as well as HL \cite{Kozen00} making it a purely equational theory.
It would be interesting to do the same for IL, and in principle it seems reasonable to do so given the duality with HL. However, the symmetry is at the level of images of functions (precisely, strongest postconditions), something that is not explicit in the syntax of KAT. Therefore, it is interesting to study which additional algebraic properties are needed to encode IL in KATs.

In \cite{MOH21}, the authors propose modal KATs as the diamond modality can be used to represent strongest postconditions (our collecting semantics $\fwsem{\cdot}$) \cite{DMS06}. This way, they are able to encode all IL rules but the infinitary \ilrule{iter}, that is needed for completeness of the proof system\footnote{Kleene star can be handled using finite unrolling, as we will show for SIL and IL in Figure~\ref{fig:sil:rules-comparison}. While this approach is not complete, it is sufficient to prove many triples. We refer the reader to \cite[Figure~2]{MOH21} for finitary rules dealing with Kleene star in KATs.} but requires the existence of countable joins of tests. The authors add this requirement explicitly to the algebraic structure, defining countably-test complete (CTC for short) modal KATs.
\begin{definition}[Backward diamond]
	A backward diamond modality on a KAT $A$ is an operator $\bdiamond{\cdot} : A \rightarrow \test(A) \rightarrow \test(A)$ satisfying, for all $a, b \in A$, $p, q \in \test(A)$
	\begin{align*}
		\bdiamond{a} p \le q \iff p \cdot a \le a \cdot q \\
		\bdiamond{a \cdot b} p = \bdiamond{b} (\bdiamond{a} p)
	\end{align*}
\end{definition}
Intuitively, $\bdiamond{a} p$ is the strongest postcondition of $a$ on input $p$. This correspondence is exact in relational KATs, and so it is natural to use it in all modal KATs. Moreover, the standard soundness of HL in KATs is defined by the equation $p \cdot a = p \cdot a \cdot q$, that can be proved equivalent to $\bdiamond{a} p \le q$ in a modal KAT. This further justifies the usage of backward diamond as strongest postcondition, given that $\spost(a, P) \le Q$ is the classical soundness condition of HL.

\begin{figure}[t]
	\centering
	\begin{framed}
		%		\resizebox{\textwidth}{!}{
		\(
		\begin{array}{cc}
			\infer[\ilrule{divergence}]
			{\iltriple{p}{0}{0}}
			{}
			\quad                     &
			\infer[\ilrule{skip}]
			{\iltriple{p}{1}{p}}
			{}
			\\[7.5pt]
			\infer[\ilrule{atom}]
			{\iltriple{p}{a}{\bdiamond{a}p}}
			{}
			\quad                     &
			\infer[\ilrule{cons}]
			{\iltriple{p}{a}{q}}
			{p \geq p'                & \iltriple{p'}{a}{q'}     & q' \geq q}
			\\[7.5pt]
			\infer[\ilrule{seq}]
			{\iltriple{p}{a \cdot b}{q}}
			{\iltriple{p}{a}{r}       & \iltriple{r}{b}{q}}
			\qquad                    &
			\infer[\ilrule{choice}]
			{\iltriple{p}{a_1 + a_2}{q}}
			{\exists i \in \{ 1, 2 \} & \iltriple{p}{a_i}{q}}
			\\[7.5pt]
			\infer[\ilrule{disj}]
			{\iltriple{p_1 \lor p_2}{a}{q_1 \lor q_2}}
			{\iltriple{p_1}{a}{q_1}   & \iltriple{p_2}{a}{q_2} }
			\qquad                    &
			\infer[\ilrule{iter}]
			{\iltriple{p_0}{a^\kstar}{\bigvee\limits_{n \ge 0} p_n}}
			{\forall n \ge 0 \sdot \iltriple{p_n}{a}{p_{n+1}}}
		\end{array}
		\)
		%		}
	\end{framed}
	\vspace{-1ex}
	\caption{CTC modal KAT encoding of IL, from \cite{MOH21}}\label{fig:sota:il-kat}
\end{figure}

Since a CTC modal KAT satisfies all the KAT axioms, it has for free the standard encoding of HL.
IL embedding in CTC modal KAT is detailed in Figure~\ref{fig:sota:il-kat}, taken from \cite{MOH21}. The rules are close to standard IL (Figure~\ref{fig:bg:il}), replacing the semantics $\denot{\expe}$ with the backward modality $\bdiamond{a}$, sequential composition with $\cdot$ and choice with $+$. This is thanks to closeness of regular command to the syntax of KATs. The two additional rules for $0$ and $1$ are, just as for HL, subsumed by \ilrule{atom}, but we add them explicitly since $0$ and $1$ are part of the syntax of KATs.

In this algebraic setting, the author derive soundness and completeness theorems for both IL and HL, that we summarise below. We remark that soundness and completeness are obtained with respect to a validity notion based on backward diamond, precisely $\bdiamond{a} p \le q$ for HL and $\bdiamond{a} p \ge q$ for IL. However, these notions coincide with the usual one when our expected interpretation of $\bdiamond{\cdot}$ as the semantics holds.
What is really interesting about those are the hypotheses required by each:
\begin{theorem}[cf. \cite{MOH21}]
	\leavevmode
	\begin{enumerate}
		\item HL is sound and complete in any modal KAT.
		\item IL is sound in any CTC modal KAT without the \textit{(Star induction)} axiom.
		\item IL is complete in any CTC modal KAT.
	\end{enumerate}
\end{theorem}
This theorem is interesting because it shows the symmetry between over and under-approximation is less sharp than it appears at first glance.
On the one hand, over-approximation requires the KAT axioms for both soundness and completeness (modality is used to define the validity of a triple, but we can do without changing the notion of validity). The fact that the latter has the same requirements as the former is essentially because loop invariants are both sound \emph{and} complete for over-approximation.
On the other hand, for under-approximation no such tool is known, so completeness has stronger hypotheses than soundness. Particularly, soundness of IL does not require \textit{(Star induction)}, the KAT rephrasing of loop invariants. Intuitively, to obtain an under-approximation there is no need for the loop fixpoint but it is enough to consider any (finite) number of iterations. This is no longer the case for completeness, since we do need the loop semantics to prove that it is a sound under-approximation of itself. This motivate the need for all the KAT axioms (including \textit{(Star induction)}) in point $(3)$ above. In other words, we can prove soundness of finite iterations for any fixpoint, not necessarily the minimal one (ie. requiring \textit{(Star unfold)} only), but only the minimal one is reached by the limit of such iteration, thus we can only prove completeness when we force the semantics to be the least fixpoint (ie. adding \textit{(Star induction)}, too).
In terms of fixpoint, the symmetry is broken because we use least fixpoints for the semantics both in over and under-approximation, while a fully dual theory would also use greatest fixpoints.

In CTC modal KAT the authors are also able to formally prove an intuitive connection between HL and IL.
\begin{theorem}[cf. \cite{MOH21}]\label{th:sota:falsify-il-hl-topkat}
	In any CTC modal KAT
	\[
	\nvDash \hltriple{p}{a}{q} \iff \exists p', q' \sdot p' \le p, q' \nleq q, \vDash \iltriple{p'}{a}{q'}
	\]
\end{theorem}
On the one hand, this theorem means that whenever a specification (ie. an Hoare triple) is not met there is a valid incorrectness triple showing a violation of that specification. In the theorem, $q'$ is the violation since $q' \nleq q$, and is in the post of $p$ because we can prove $\iltriple{p'}{a}{q'}$ with $p' \le p$.
On the other hand, when the specification is true, we don't need incorrectness reasoning since we already know all valid triples (hence all those we can prove) starting from $p' \le p$ are bound to have a postcondition $q'$ satisfying the specification $q$ (ie. $q' \le q$).
%We remark that the statement involves valid triples, but since both HL and IL are complete in CTC modal KATs this is the same as considering provable triples.
This theorem states a connection between over- and under-approximation that is obvious in the concrete interpretation with programs and states, but that it is not in a general algebraic model. Being able to prove it means the encoding meets the desired intuition, rising confidence in its correctness. Moreover, this theorem is a first (trivial) example of how over- and under-approximation can positively interact.

To conclude this section, we mention the concurrent work \cite{ZAG22}, that addresses the same problem. In this work, the authors show that KAT alone are not enough to express IL (cf. Theorem~1 of their paper). To enrich the algebraic structure, they follow the idea of expressing (to some extent) the codomain, too, and propose to add a top element $\top$ to the KAT, defining the so-called TopKAT.
With this addition, given any element $a$ of the TopKAT we have that $\top a$ represents the ``codomain" of $a$. This is formal in relational TopKATs:
\begin{prop}[cf. \cite{ZAG22}]
	In any relational TopKAT $\katR$, for any two elements $p, q \in \katR$ and letting $\text{cod}$ extract the codomain of a relation,
	\begin{align*}
		\top p = \top q   & \iff \text{cod}(p) = \text{cod}(q)         \\
		\top p \le \top q & \iff \text{cod}(p) \subseteq \text{cod}(q)
	\end{align*}
\end{prop}
The authors then use this definition to embed IL in a generic TopKAT.
We remark that in this article the focus is not on completeness of the logic, so there is not the CTC assumption on the algebraic structure, but only the those join needed to apply \ilrule{iter} are required to exist. Even with this focus shift, the work already shows that a top element is a viable alternative to modality for encoding IL in Kleene algebras.

\section{Local completeness}\label{sec:sota:lcl}
Local Completeness Logic on an abstract domain $A$ ($\LCLA$ for short) \cite{BGGR21} is a first example of non-trivial over and under-approximation interaction, with the former embodied by abstract interpretation and the latter by IL.

As described in Section~\ref{sec:bg:absint}, abstract interpretation is always sound, but in general it is not complete: composition of best correct abstractions (bcas) is not the bca of the composition. Also widening and narrowing operators introduce incompleteness, and the former are required to ensure termination of the analysis on infinite abstract domains. While in theory completeness ensures no precision is lost, it is a very uncommon situation in general. One of the causes is its requirement to hold \emph{for all inputs}. To weaken this condition, in \cite{BGGR21} the authors propose a notion of \emph{local} completeness, that depends on a specific input.
\begin{definition}[Local completeness, cf.\cite{BGGR21}]
	Let $f: C \rightarrow C$ be a concrete function, $c \in C$ a concrete point and $A \in \Abs(C)$ and abstract domain for $C$. Then $A$ is \emph{locally complete} for $f$ on $c$, written $\complete{A}{c}{f}$ iff
	\[
	Af(c) = AfA(c) .
	\]
\end{definition}

A remarkable difference between global and local completeness is that, while the former can be proved compositionally on the command only~\cite{GLR15}, the latter needs the input to each fragment of the program. Consequently, to carry on a compositional proof of local completeness, information on the input to each subpart of the program is also required, i.e., all traversed states are important. However, local completeness enjoys an ``abstract convexity" property: if $f$ is locally complete on a point $c$, then it is locally complete on any point $d$ between $c$ and $A(c)$:

\begin{lemma}[Abstract convexity, cf.~\cite{BGGR21}]\label{lmm:sota:abstract-convexity}
	If $\complete{A}{c}{f}$ and $c \le d \le A(c)$, then $\complete{A}{d}{f}$.
\end{lemma}

This observation has been crucial in the design of the proof system $\LCLA$. The proof system depends on an abstract domain $A$, and is able to prove triples $\lcl{A}{P}{\regr}{Q}$  ensuring that:

\begin{enumerate}
	\item $Q$ is an under-approximation of the concrete semantics $\denot{\regr}P$,
	\item $Q$ and $\denot{\regr}P$ have the same over-approximation in $A$,
	\item $A$ is locally complete for $\denot{\regr}$ on input $P$.
\end{enumerate}

Point (2) means that, given a specification \textit{Spec} expressible in $A$, any provable triple $\lcl{A}{P}{\regr}{Q}$ either proves correctness of $\regr$ with respect to \textit{Spec} or expose an alert in $Q \setminus \textit{Spec}$. This in turns correspond to a true one because, by point (1), $Q$ is an under-approximation of the concrete semantics $\denot{\regr}P$, as pointed out by Corollary~\ref{th:sota:corollary-verification} below.

\begin{figure*}[t]
	\centering
	\begin{framed}
		\resizebox{\textwidth}{!}{
			\(
			\begin{array}{l}
				\infer[\lclrule{transfer}]
				{\lcl{A}{P}{\expe}{\denot{\expe}P}}
				{\complete{A}{P}{\denot{\expe}}}
				\quad %&
				\infer[\lclrule{relax}]
				{\lcl{A}{P}{\regr}{Q}}
				{P'\leq P \leq A(P')       & \lcl{A}{P'}{\regr}{Q'}              & Q \leq Q' \leq A(Q)}
				\\[7.5pt]
				\infer[\lclrule{seq}]
				{\lcl{A}{P}{\regr_1;\regr_2}{Q}}
				{\lcl{A}{P}{\regr_1}{R}    &
					\lcl{A}{R}{\regr_2}{Q}}
				\qquad %&
				\infer[\lclrule{join}]
				{\lcl{A}{P}{\regr_1 \regplus \regr_2}{Q_1\vee Q_2}}
				{ \lcl{A}{P}{\regr_1}{Q_1} & \lcl{A}{P}{\regr_2}{Q_2}}
				\\[7.5pt]
				\infer[\lclrule{rec}]
				{\lcl{A}{P}{\regr^\kstar}{Q}}
				{\lcl{A}{P}{\regr}{R}      & \lcl{A}{P\vee R}{\regr^\kstar}{Q} }
				\qquad %&
				\infer[\lclrule{iterate}]
				{\lcl{A}{P}{\regr^\kstar}{P\vee Q}}
				{\lcl{A}{P}{\regr}{Q}      & Q\leq A(P)}
			\end{array}
			\)
		}
	\end{framed}
	\vspace{-1ex}
	\caption{The proof system $\LCLA$, from \cite{BGGR21}.}\label{fig:sota:lcla-rules}
	%	\vspace{-4ex}
\end{figure*}
The proof system is defined in Figure~\ref{fig:sota:lcla-rules}. It is a logic of under-approximations, much like IL (in fact, IL is an instance of $\LCLA$ using the trivial abstract domain $A$ containing only the $\top$ element), but with one additional constraint: the under approximation $Q$ must have the same abstraction of the concrete semantics $\denot{\regr} P$, as for instance explicitly required in rule \lclrule{relax}. This, by the abstract convexity property mentioned above, means that local completeness of $\denot{\regr}$ on the \emph{under-approximation} $P$ of the concrete store is enough to prove local completeness.
This way, $\LCLA$ exploits the interaction of over- and under-approximation. The latter is used to ensure the abstraction is locally complete, ie. guarantees precision of the over-approximation. Conversely, the presence of the abstraction in rule \lclrule{iterate} speeds up the computation as it allows to stop as soon as $A(P)$ is an abstract loop invariant, not having to deal with (possible infinitary) concrete invariants. Note that this doesn't guarantee termination, since an infinite number of iterations may be needed to reach the abstract invariant.

Formally, the three key properties (1--3) above are subsumed by the following result:

\begin{theorem}[Soundness, cf.\cite{BGGR21}]\label{th:sota:lcl-soundness}
	Let $A_{\alpha, \gamma} \in \Abs(C)$.
	If $\lcl{A}{P}{\regr}{Q}$ then:
	\begin{enumerate}
		\item $Q \le \denot{\regr} P$,
		\item $\alpha(\denot{\regr} P) = \alpha(Q)$,
		\item $\denot{\regr}^{\sharp}_{A} \alpha(P) = \alpha(Q)$.
	\end{enumerate}
\end{theorem}

We say that a triple satisfying these three conditions is \emph{valid}, written $\lclvalid{A}{P}{\regr}{Q}$.
As a consequence of this theorem, given a specification expressible in the abstract domain $A$, a provable triple $\lcl{A}{P}{\regr}{Q}$ can determine both correctness and incorrectness of the program $\regr$:

\begin{corollary}[Proofs of Verification, cf. \cite{BGGR21}]\label{th:sota:corollary-verification}
	Let $A_{\alpha, \gamma} \in \Abs(C)$ and $a \in A$. If $\lcl{A}{P}{\regr}{Q}$ then
	\[
	\denot{\regr} P \le \gamma(a) \iff Q \le \gamma(a) .
	\]
\end{corollary}
The corollary is useful in program analysis and verification because, given a specification expressible in $A$ and a provable triple $\lcl{A}{P}{\regr}{Q}$, it allows to distinguish two cases.
\begin{itemize}
	\item If $Q \subseteq \gamma(a)$, then we have also $\denot{\regr} P \subseteq \gamma(a)$, so that the program is correct with respect to the specification.
	\item If $Q \nsubseteq \gamma(a)$, then also $\denot{\regr} P \nsubseteq \gamma(a)$, that means $\denot{\regr} P \setminus \gamma(a)$ is not empty and thus contains a true alert of the program. Moreover, since $Q \subseteq \denot{\regr} P$ we have that $Q \setminus \gamma(a) \subseteq \denot{\regr} P \setminus \gamma(a)$, so that already $Q$ is able to pinpoint some issues.
\end{itemize}
To better show how this work, we briefly introduce the following example (discussed also in \cite{BGGR21} where it is possible to find all details of the derivation).
\begin{example}
	Consider the concrete domain $C = \pow(\setZ)$, the abstract domain $\Int$ of intervals, the precondition $P = \{ 1; 999 \} \in C$ and the command $\regr \eqdef (\regr_1 \oplus \regr_2)^{\kstar}$, where
	\begin{align*}
		\regr_1 & \eqdef \code{(x > 0)?; x := x - 1}    \\
		\regr_2 & \eqdef \code{(x < 1000)?; x := x + 1}
	\end{align*}
	In $\LCLA$ it is possible to prove the triple $\lcl{\Int}{P}{\regr}{Q}$, whose postcondition is $Q = \{ 0; 2; 1000 \} \in C$. Consider the two specification $\text{Spec}= (x \le 1000)$ and $\text{Spec}' = (x \ge 100)$.
	The triple is then able to prove correctness of $\text{Spec}$ and incorrectness of $\text{Spec}'$.
	For the former, observe that $Q \subseteq \text{Spec}$. By Corollary~\ref{th:sota:corollary-verification} we then know $\denot{\regr} P \subseteq \text{Spec}$, that is correctness.
	For the latter, $Q$ exhibits two witnesses to the violation of $\text{Spec}'$, that are $0, 2 \in Q \setminus \text{Spec}'$. By point (1) of soundness we then know that $0, 2 \in Q \subseteq \denot{\regr} P$ are true alerts.
\end{example}

In $\LCLA$, being able to prove any triple $\lcl{A}{P}{\regr}{Q}$ allows to show both correctness and incorrectness of $\regr$. However, if $\regr$ is not locally complete on $P$, or more in general any of the local completeness proof obligations introduced by rule \lclrule{transfer} (the only axiom of the logic) fails, the proof cannot be completed.
To handle this issue, \cite{BGGR22} proposes the idea of changing the abstract domain in which the derivation is performed. Following what had been done for completeness \cite{GRS00}, they propose to minimally (in the lattice of abstract interpretations) refine the abstract domain. Unluckily, such a minimal refinement in general does not exists, so that the authors propose a different notion of ``best" refinement. They consider pointed refinements, that are defined by the addition of a single point to the abstract domain (followed by a Moore closure operation). Then they compare these pointed refinements not in the lattice of abstract interpretations but by the precision of the additional point. When there exists a most abstract point whose pointed refinement is locally complete, they call this domain \emph{pointed (locally complete) shell}:
\begin{definition}[Pointed shell, cfr. \cite{BGGR22}]
	Let $f : C \rightarrow C$ be a monotone concrete function, $A \in \Abs(C)$ be an abstract domain and $c \in C$ a concrete point. The pointed shell of $A$ on $c$ w.r.t. $f$ exists when the maximum of the set
	\[
	\{ x \in C \svert \complete{A_x}{c}{f} \}
	\]
	exists and, letting $u$ be such maximum, the pointed shell is $A_u \in \Abs(C)$.
\end{definition}

Other than characterizing the existence of pointed shell, they propose two strategies to repair the abstract domain using pointed shells. One of the two, the so-called backward repair, mostly operates on the abstract and so does not fit our goal of combining over and under-approximation.
The other, forward repair, instead operates on under-approximation of concrete points. It processes local completeness proof obligations in order, starting from the input and following the control flow. Thanks to abstract convexity of local completeness, this strategy works even on under-approximations of concrete stores, so that it integrates well with $\LCLA$. The strategy computes local completeness proof obligations in order, either reaching the end of the program (thus completing the analysis) or finding a failed one. In this case, it repairs the abstract domain to the pointed shell (using the under-approximation) and then restart the analysis in the refined domain.

To conclude this section, we point out there exists an algebraic formulation of $\LCLA$. In \cite{MR22}, the authors take inspiration from the works discussed in the previous Section~\ref{sec:sota:kat} and embed $\LCLA$ in (a suitable extension of) KAT. Their first contribution is the definition of an abstract interpretation of KAT, a problem not studied before. Exploiting this, they embed $\LCLA$ in both modal KATs and TopKATs. The technical development of these embeddings is similar to that of \cite{MOH21} and \cite{ZAG22}, but it shows that such an embedding is effective as it preserves al property of $\LCLA$. Lastly, we remark that just as $\LCLA$ generalizes IL, in \cite{MR22} they recover the embedding of IL in modal KATs/TopKATs as a special case of $\LCLA$'s.

\section{IC3/PDR}\label{sec:sota:pdr}
\code{ic3} (``Incremental Construction of Inductive Clauses for Indubitable Correctness"), also called PDR (``Property directed reachability"), was first proposed by Bradley as a model checking algorithm \cite{Bradley11}. Given a safety property $P$ and a finite transition system, it either proves the property or outputs a counterexample. In this sense, \code{ic3} operates both as prover and a bug finder \cite{Bradley12}. Its ingenuity consists in using an over-approximation to guide the search for counterexamples, that in turn are used to guide the progressive refinement of the over-approximation. This means that the core of \code{ic3} is a combination of over- and under-approximations.
Thanks to this, it quickly became one of the best hardware model checker. Moreover, the algorithm was later applied to other settings, such as probabilistic transition systems, software model checking or generic complete lattices.

In its original formulation, \code{ic3} operates on a finite transition system. Let $\states$ be the (finite) set of states, $I \subseteq \states$ the set of initial states and $T : \pow(\states) \rightarrow \pow(\states)$ the transition function: given a set of states $S$, it returns the sets of states reachable from $S$ in one step. If $\rightarrow \subseteq \pow(\states \times \states)$ is the transition relation, $T(S) \eqdef \{ t \svert \exists s \in S \sdot s \rightarrow t \}$. In this case, $\lfp(T \cup I)$ is the set of all states reachable from $I$.
Actually, $I$ and $T$ are represented by propositional formulas and a SAT solver is employed to prove implications and satisfiability queries.

\begin{example}[Safety problem for transition systems]\label{ex:sota:ts}
	\begin{figure}[t]
		\begin{displaymath}
			\xymatrix@R=5pt{
			& s_1 \ar@/^1ex/[rd] \ar@/^1ex/[ld] &  s_5 \ar@(l,u)\ar[r] & s_6 \ar@(r,u)\\
			s_0 \ar@/^1ex/[ur] \ar@/_1ex/[rd] & &s_3 \ar[r]& s_4\ar@(rd,ur) \\
			& s_2 \ar@(ur,ul) \ar@/_1ex/[ru] &
			}
		\end{displaymath}
		\caption{The transition system of Example~\ref{ex:sota:ts}, with $S = \{ s_0, \dots s_6 \}$ and $I=\{s_0\}$.}\label{fig:sota:ts}
	\end{figure}

	Consider the transition system in Figure~\ref{fig:sota:ts}. Hereafter we write $S_{j}$ for the set of states $\{s_0, s_1, \dots, s_j\}$ and we fix the set of safe states to be $P = S_5$.
	We know that the transition system is safe if and only if $\lfp(T \cup I) \subseteq P$, and by Kleene Theorem~\ref{th:bg:kleene} this correspond to the limit of the intial chain
	\[
	\emptyset \subseteq  I \subseteq S_2 \subseteq S_3 \subseteq S_4 \subseteq S_4 \subseteq \cdots
	\]
	which stabilizes at $S_4$ and is therefore below $P$. Note that the $(j+1)$-th element of the initial chain contains all the states that can be reached from $I$ in at most $j$ transitions.

	It is worth to remark that $T$ has a right adjoint $G \colon \pow(S) \to \pow(S)$ defined for all $X \in \pow(S)$ as $G(X) \eqdef \{s \mid \forall t. s \rightarrow t \implies t \in X \}$ (the right adjoint is also called $\widetilde{\text{pre}}$). Thus by~\eqref{eq:bg:adjoint-fixpoint}, $\lfp (T \cup I) \subseteq P$ iff $I \subseteq \gfp (G \cap P)$. We can check this by computing the final chain
	\[
	\cdots \subseteq S_4 \subseteq S_4 \subseteq P \subseteq S
	\]
	which again stabilizes at $S_4$ and is therefore below $P$. Note that the $(j+1)$-th element of the final chain contains all the states that in at most $j$ transitions reach safe states only.
\end{example}

\code{ic3} fundamental data structures is a sequence $(X_i)_{0 \le i \le k + 1}$ of over-approximations of states reachable in at most $i$ steps. $k$ is the number of so-called \emph{major iterations} the algorithm has performed.
In \code{ic3}, $X_i$ are logical formulas in CNF. Moreover, the following invariants are kept: (1)~$\clause(X_{i+1}) \subseteq \clause(X_i)$, (2)~$T(X_i) \timplies X_{i+1}$, (3)~$X_{k} \timplies P$.
As a consequence, (1) implies $X_i \timplies X_{i+1}$, and together with (3) this means all elements of the sequence (but possibly $X_{k+1}$) are strengthening of $P$. The over-approximation $X_k$ is the ``frontier" of the analysis. $k$ is increased by major iterations. Being at major iteration $k$ means the algorithm has proved no violation of $P$ is reachable within $k$ steps, and is working on $k + 1$. Once the algorithm can prove $T(X_k) \timplies P$, it refines $X_{k+1}$ by conjoining it with $P$, increases $k$ and sets $X_{k+2} = \true$ (the empty set of clauses).

At this point, the algorithm also performs what is called clauses propagation. Basically, it considers any clause $c$ in any of the $X_i$ and checks whether $T(X_i) \timplies c$. If this is the case, $c$ is conjoined to $X_{i+1}$, as this preserves all the invariants. In this sense, the clause $c$ is ``propagated" from $X_i$ to $X_{i+1}$, and the algorithm goes on with other clauses in $X_i$, then with $X_{i+1}$ and so on. If during this step, at any point $X_i = X_{i+1}$, the algorithm proved the specification $P$ because $T(X_i) \timplies X_{i+1} = X_i$, so $X_i$ is an invariant, and $X_i \timplies P$.

However, in general the implication $T(X_k) \timplies P$ will not be satisfied. In this case, the SAT solver produces a counterexample $s$, that is a state in $X_k$ such that one of its successors does not satisfy $P$. This means that not only $X_k$ is not an invariant, but that it contains some states which violate (after one step) the specification. The idea of \code{ic3} is then to see whether this state is introduced by the over-approximation, so that it can be ruled out, or is really reachable, so that a true counterexample is found.
So the algorithm looks for a clause $c$ to rule out $s$ from $X_k$. It takes $c$ whose literals are a subset of those in $\lnot s$ (so that $c \timplies \lnot s$) and checks whether $I \timplies c$ and $T(X_k \land c) \timplies c$. If it can find any such $c$, it conjoins it to all $X_i$ up to $X_{k+1}$ (it's easy to check this preserves all three invariants). Doing so, the algorithm removes $s$ from $X_k$, so it tries again the implication $T(X_k) \timplies P$, that will either be satisfied or find a different counterexample.
If instead no $c$ satisfies this implication, the algorithm does the same for $X_{k-1}$, and then for $X_{k-2}$. Suppose it finds it at this last check (it can't go further because $s$ is not in $T(X_{k-2})$, as it has a successor violating $P$ while $T(X_{k-2}) \timplies X_{k-1}$ and $T(X_{k-1}) \timplies P$, hence $T(X_{k-2}) \timplies \lnot s$). So the algorithm found a clause $c$ such that $c \timplies \lnot s$, $I \timplies c$ and $T(X_{k-2} \land c) \timplies c$: it can conjoin it to all $X_i$ up to $X_{k-1}$. This rules $s$ out of $X_{k-1}$ but does not solve the issue of $s$ being in $X_{k}$. However, here's the ingenuity of the algorithm: now the query $T(X_{k-1}) \timplies \lnot s$ is either satisfied, so we can conjoin $\lnot s$ to $X_k$, or its failure pinpoints a predecessor $t$ of $s$ in $X_{k-1}$.
That is, it either shows $s$ is a spurious counterexample introduced by the over-approximation of $X_k$ or it traces it back to a possible counterexample in $X_{k-1}$.

The procedure restarts from $t$ and $X_{k-1}$. This recursive call will either prove $t$ spurious, so that we can turn back to $T(X_{k-1}) \timplies \lnot s$, or find a predecessor $u$ of $t$ in $X_{k-2}$. This going up and down the sequence of over-approximation, refining it along the way, in the end will either prove $T(X_k) \timplies P$ or find a chain of true counterexample starting from $I$, disproving $P$.
In this exploration, going up the chain means the over-approximation allows to discard a counterexample, while going down means the counterexample (that is an under-approximation) guides the refinement of the over-approximation.

Our description is high level and leaves out many details. We are not interested in discussing them here, and refer the reader to \cite{Bradley11}, as we will detail the more general algorithm LT-PDR later in this Section. The one thing we want to point out is that the choice of the clause $c$ used to rule out states violating the specification is minimal, ie. no strict subclause (made of a subset of the literals in $c$) satisfies the properties required. This in turn means that $c$ includes less states (removing a literal from a disjunction makes it smaller), that is it removes from $F_i$ as much states as possible. This choice is done in order to remove spurious counterexamples as quickly as possible. We will discuss this issue later.

\code{ic3} was developed for hardware model checking, that means variables are boolean and the transition system is finite. However, its core ideas are deep and not tied to the specific domain, which lead to a host of derived work in other fields. One of the main challenges to generalize it is moving to infinite states space, since termination of the algorithm relies on finiteness of the domain. To cope with this issue, the crucial step is ``generalization", that in \code{ic3} is the choice of $c \timplies \lnot s$ to remove $s$. Taking $c$ as general as possible removes many (unreachable) states at once. Clearly this does not impact termination if the state space is finite, but is crucial whenever it is infinite. Hence, while generalization is not needed for soundness, it becomes fundamental for termination of infinite generalizations.
This notwithstanding, \code{ic3} has been successfully generalized. One example is \code{PrIC3} (``Probabilistic \code{ic3}") \cite{BJKKMS20}, for model checking Markov decision processes (MDPs). These are basically transition systems where, fixed the action, the state to which the system transitions is not determined but is chosen with a given probability distribution. Without entering in too much details, the key difference is that a system configuration is not a single state but a probability distribution, meaning the state space is infinite. The authors propose a first algorithm which depends on an heuristic\todo{Cousot: dual narrowing?} for generalization and always terminates, but gives up correctness when the system detects a counterexample: it may very well return a false alarm. Then, to recover correctness, they propose an effective way to find a ``good" heuristics, that basically amount to run their algorithm repeatedly, using the false counterexample to refine the heuristic every step until either safety is proved or a true counterexample is found.
Another generalization is software \code{ic3}, that uses the same core algorithm to model check software systems~\cite{CG12,LNNK20}. Given their state spaces are infinite and variables are not just boolean, software \code{ic3} relies on SMT instead of SAT solvers. Moreover, they exploit explicitly the control-flow structure of programs, as doing so implicitly has been shown far less effective~\cite{CG12}. This makes the algorithms more involved, but shows that \code{ic3} has indeed the potential to scale to infinite state spaces.

\subsection{LT-PDR}\label{sec:sota:lt-pdr}
\begin{figure}[t]
	\begin{center}
		\underline{{LT-PDR} $(F,\alpha)$}
		{\small
			\begin{codeNT}
<INITIALISATION>
  $( \vec{x} \| \vec{c} )_{n,k}$ := $(\bot, F \bot \| \varepsilon)_{2,2}$
<ITERATION>						            % $\vec{X},\vec{C}$  not conclusive
  case:
       $k \ge 2$ :               	         	%(Induction)
			choose $j \ge 2$ And $z \in L$ st $x_{j} \nleq z$ And $F(x_{j-1} \land z) \leq z$;
			$(\vec{x} \| \vec{c} )_{n,k}$ := $(\vec{x} \land_j z \| \vec{c} )_{n,k}$
       $\vec{c}=\varepsilon$ And $x_{n-1} \leq \alpha$     :                    %(Unfold)
			$( \vec{x} \| \vec{c} )_{n,k}$ := $( \vec{x}, \top \| \varepsilon )_{n+1,n+1}$
       $\vec{c}=\varepsilon$ And $x_{n-1} \nleq \alpha$    :                     %(Candidate)
			choose $z \in L$ st $z \le x_{n-1}$ And $z \nleq \alpha$;
			$( \vec{x} \| \vec{c} )_{n,k}$ := $( \vec{x} \| z )_{n,n-1}$
       $\vec{c} \neq \varepsilon$ And $c_k \le F(x_{k-1})$ :                        %(Decide)
			choose $z \in L$ st $z \le x_{k-1}$ And $c_{k} \le F(z)$;
			$( \vec{x} \| \vec{c} )_{n,k}$ := $(\vec{x} \| z , \vec{c} )_{n,k-1}$
       $\vec{c} \neq \varepsilon$ And $c_{k} \nleq F(x_{k-1})$ :                        %(Conflict)
			choose $z \in L$ st $c_k \nleq z$ And $F(x_{k-1} \land z) \le z$;
			$( \vec{x} \| \vec{c} )_{n,k}$ := $(\vec{x} \land_k z \| \mathsf{tail}(\vec{c}) )_{n,k+1}$
  endcase
<TERMINATION>
  if $\exists j\in [0,n-2]\,.\, x_{j+1} \leq x_j$ then return true		 % $\vec{X}$ conclusive
  if $k = 1$ then return false						 % $\vec{C}$ conclusive
\end{codeNT}
		}
	\end{center}
	\caption{LT-PDR, from~\cite{KUKSH22}}\label{fig:sota:lt-pdr}
\end{figure}
An interesting point of view is taken by \cite{KUKSH22}. In this article, the authors propose a generalization of \code{ic3} whose only constraint is that the space state is a complete lattice, that they call LT-PDR (``Lattice theoretic property directed reachability"). While LT-PDR is extremely generic and needs heuristics to be instantiated for particular domains, it captures the essence of the algorithm, showing in particular which properties are needed for soundness and termination. In fact, the paper highlights how \code{ic3} is based on Knaster-Tarski (for proving safety) and Kleene (for finding counterexamples) fixed-point theorems (see Section~\ref{sec:bg:posets}).
LT-PDR is presented in Figure~\ref{fig:sota:lt-pdr}. Its notation differs a little from \code{ic3}. Given a complete lattice $L$, a monotone function $F: L \rightarrow L$ and a property $\alpha \in L$, the goal of the algorithm is to either prove or find a counterexample to $\lfp(F) \le \alpha$. To encode for instance the original \code{ic3} problem in this settings, it is sufficient to take $L = \pow(\states)$, $F(S) = I \cup T(S)$ and $\alpha = P$; it is not hard to verify that this gives exactly the model checking problem.
With this in mind, $F$ broadly correspond to the transition relation $T$; the sequence of over-approximations is $( x_i )_{0 \le i \le n}$, while $\vec{c}$ is the so-called \emph{negative sequence}, and is more or less the call stack for the recursion on predecessors of error states.
Intuitively, the algorithm is building an over-approximating sequence $\vec{x}$ of the fixpoint iterates, hoping to reach a safe abstract fixpoint. However, when the next over-approximating iterate is not safe, it must contain a counterexample, that is then either traced back toward initial states or identified as spurious and hence removed.

The algorithm keeps the following invariants on $\vec{x}$, analogously to \code{ic3}: (1)~$x_i \le x_{i+1}$, (2)~$F(x_i) \le x_{i+1}$, (3)~$x_k \le \alpha$.
\textbf{Unfold} increases $n$, advancing the major iteration. It does not perform clause propagation, though: this is the duty of \textbf{Induction}. The algorithm is nevertheless sound, as it basically already propagates clauses in \textbf{Conflict}.
\textbf{Candidate} broadly correspond to the case when $T(X_k) \timplies P$ is not satisfied, with a slight difference. Here $z$ is a counterexample to $x_{n-1} \le \alpha$, not an element of $x_{n-2}$ that is predecessor of a bad state. This difference is insubstantial, though, as the algorithm just recur on this element going to its predecessors.
\textbf{Decide} and \textbf{Conflict} are, respectively, a new recursive call and the conclusion of a previous one. On the one hand, when $c_k$, the current predecessor of a bad state, is contained in $F (x_{k-1})$, it means it has a predecessor in the over-approximation. This predecessor is $z$, and the algorithm goes on recursing on it. On the other hand, if $c_k$ is not in $F (x_{k-1})$ it's a spurious counterexample, introduced by the over-approximation: it is then removed refining $x_{k-1}$ by conjoining with a suitable $z$ that excludes $c_k$, just like the clause $c$ is conjoined to $X_i$ in \code{ic3}.
The two termination conditions correspond to safety and unsafety respectively: the first one checks whether it reached an invariant (that is $X_{i+1} = X_i$); the second one verifies whether the predecessors reached an initial state (as $F(\bot) = I$ with the given definition of $L$ and $F$).

The choice of $z$ in \textbf{Induction}, \textbf{Candidate}, \textbf{Decide} and \textbf{Conflict} is left unspecified by the algorithm. Save for induction (that is anyway not necessary for neither soundness nor termination), there are canonical choices for $z$ in the rules, but better solutions can be provided by heuristics. This allows LT-PDR to accommodate for other known instances of the algorithm, such as \code{ic3} and \code{PrIC3}, just fixing the right lattice and heuristics.
While in general LT-PDR does not help with this choice, it is able to highlight pros and cons behind them. As briefly remarked above, in general we can look for either a larger or smaller $z$ in each of the rules. There are two different kind of choices here: \textbf{Induction} and \textbf{Conflict}, which pick a $z$ to \emph{refine} the over-approximation, and \textbf{Candidate} and \textbf{Decide}, whose goal is identify a counterexample.
For the latter, a larger $z$ means that we are possibly examining more counterexamples at once. However, if any of these counterexample is spurious we have to apply \textbf{Conflict} to remove that $z$, and possibly restart with a smaller one containing all the counterexamples we did not discard with the refinement (that are all the true ones, but possibly also some of the false ones). Moreover, this requires to work on many states (e.g., application of $F$, that must be exact for the algorithm to work) at once, that may be costly. On the other hand, a smaller $z$ means considering just a few counterexamples at a time. While a lucky choice of such an $z$ may lead very quickly to a true counterexample, if the safety property is satisfied this may cause the removal of counterexample one by one, possibly taking a lot of time or preventing termination. We note that \code{ic3} follows the second path, only examining the single counterexample returned by the SAT solver.
Considering instead the $z$ used to refine the over-approximation, there are two conflicting, driving forces guiding the choice. On the one hand, a smaller $z$ removes more counterexamples (this is the path chosen by \code{ic3} with its generalization to a minimal subclause). On the other hand, a larger $z$ ensures more abstract over-approximations, yielding less expensive computations and, when there are no counterexamples, a faster fixpoint convergence.

The authors discuss the termination of LT-PDR. As many choices are left unspecified, the best they were able to prove unconditionally is the \emph{existence} of a sequence of choices that make the algorithm end. In addition, the authors propose a set of sufficient but very restrictive conditions that guarantee termination for all possible choices of the algorithm:
\begin{itemize}
	\item the complete lattice $L$ is well-founded, and
	\item either $\lfp(F) \nleq \alpha$ (i.e., there is a counterexample) or $\lfp(F) \le \alpha$ (the property is satisfied) and there are no strictly increasing infinite chains bounded by $\alpha$.
\end{itemize}
Intuitively, the first condition means that eventually the algorithm proceeds to the next major iteration (i.e., increases $n$). The second instead limit the number of major iterations to a finite number: either it finds a counterexample at some point, or it does not but then it cannot increase $n$ arbitrarily while staying below $\alpha$. While these conditions are not necessary for termination, nor they are satisfied by many domains, they give a possible starting direction to investigate termination strategies for LT-PDR.

\section{Outcome Logic}\label{sec:sota:ol}
Outcome Logic (OL)~\cite{ZDS23} is a recent triple-based program logic, inspired by IL and its ability to search for true bugs. However, it is based on the key insight that under-approximation (of the program behaviour) and reachability (of true error states) are distinct concepts. To handle them separately, OL starts back from HL and generalizes it to use as assertions not properties of states but properties on an \emph{outcome monoid}, which for instance can be sets of states or probability distributions. This allows OL to unify safety and reachability, as well as over and under-approximation in the same logic.

An OL triple has the familiar shape $\oltriple{P}{\regr}{Q}$. However, $P$ and $Q$ are not assertions on states in $\Sigma$. Rather, they are properties over an outcome monoid $M \Sigma$. Particularly, this allows the assertion language to include a new connective $\oplus$, called \emph{outcome conjunction}, that correspond to the monoidal operation on $M$.
To understand the differences, consider the three formulae $x = 0 \land y = 1$, $x = 0 \lor y = 1$ and $x = 0 \oplus y = 1$ and the powerset monoid. In this instance, $S \in M \Sigma = \pow(\Sigma)$ is a \emph{set of states} rather than just a state $\sigma \in \Sigma$. A set of states $S$ is a model of $x = 0 \land y = 1$ if, for all states $\sigma \in S$, $\sigma(x) = 0$ and $\sigma(y) = 1$. This is in line with the intuition we have from HL of this assertion. Things are already a bit different for $x = 0 \lor y = 1$: a set of states $S$ satisfy this formula if either $\sigma(x) = 0$ for all states $\sigma \in S$ or $\sigma(y) =1$ for all states $\sigma \in S$. Therefore, for instance the set ${[x \mapsto 0, y \mapsto 0], [x \mapsto 1, y \mapsto 1]}$ does \emph{not} satisfy $x = 0 \lor y = 1$ because it is not the case that neither all states have $x = 0$ nor that all states have $y = 1$. Note that this notion of having all states satisfying either one or the other disjunct has no correspondence in HL because properties are satisfied only by single states, not sets of states. Lastly, the outcome conjunction $x = 0 \oplus y = 1$ is satisfied by a set of states $S$ if it possible to partition $S = S_1 \cup S_2$ in two \emph{non-empty} sets $S_1$, $S_2$ such that $S_1$ satisfies $x = 0$ and $S_2$ satisfies $y = 1$. For instance,  ${[x \mapsto 0, y \mapsto 0], [x \mapsto 1, y \mapsto 1]}$ does satisfy $x = 0 \oplus y = 1$ because we can split it as ${[x \mapsto 0, y \mapsto 0]} \uplus {[x \mapsto 1, y \mapsto 1]}$ where the first set trivially satisfies $x = 0$ and the second $y = 1$. Differently than disjunction though, the set ${[x \mapsto 0, y \mapsto 0], [x \mapsto 0, y \mapsto 7], [x \mapsto 0, y \mapsto 42]}$ does not satisfy $x = 0 \oplus y = 1$ because we cannot split it in two non-empty subsets such that one satisfies $y = 1$: this non-emptiness requirement ensures that all outcomes separated by $\oplus$ are reachable in $S$.

Given this assertion language, an outcome triple has a validity condition similar to HL. Given a lifting $\fwsem{\regr}^{\dagger}$ of the semantics of $\regr$ to the monoid $M \Sigma$ (this is, for instance, the Kleisli lifting of $\fwsem{\regr}$ when $M$ is a monad) the triple $\oltriple{P}{\regr}{Q}$ is valid if and only, for all elements of the monoid $m \in M \Sigma$ that satisfy the precondition ($m \models P)$ then the final outcomes satisfy $Q$: $\fwsem{\regr}^{\dagger} m \models Q$. For instance, when $M$ is the powerset monad, the lifted semantics $\fwsem{\regr}^{\dagger}$ is simply the collecting semantics from Figure~\ref{fig:bg:regcom-sem}. This gives a natural way to encode over-approximation (i.e., HL triples), with the addition of the outcome conjunction to prescribe that all outcomes mentioned in the postcondition are truly reachable.

However, the authors want to describe under-approximation as well, that is, be able to specify only a subset of the program behaviours. To achieve this, they use a very specific kind of over-approximation: they introduce a special assertion $\top$ that is satisfied by any element $m \in M \Sigma$. Since all outcomes in the post must be reachable, to specify only some program behaviour instead of all it is enough to over-approximate with $\top$ all the outcomes you do not want to describe. This is a sound over-approximation, but it only specifies that some of the outcomes are truly reachable, thus ignoring some program behaviours and performing under-approximation.

\begin{example}\label{ex:sota:ol}
	Consider the motivating example from~\cite[\S~2.1]{ZDS23}.
	\begin{align*}
		\regr \eqdef \code{x := malloc(); *x := 1}
	\end{align*}
	The above program, written in C syntax, allocates a pointer with \code{malloc} and then tries to dereference it, forgetting that \code{malloc} can fail and return \code{null}. Therefore, the HL triple $\hltriple{\true}{\regr}{x \mapsto 1}$ is not valid: if \code{malloc} fails, the program ends in an error state.\footnote{This is actually undefined behaviour in C. We won't deal with that here and just assume this actually causes a recognizable error, such as a segmentation fault.} The correct HL triple is then $\hltriple{\true}{\regr}{(x \mapsto 1) \lor \errstate}$, where $\errstate$ describes that some error occurred. However, this HL triple does not tell that both states where $(x \mapsto 1)$ and $\errstate$ are reachable: they could have been added by the over-approximation.

	On the other hand, IL can show reachability of these states: $\iltriple{\true}{\regr}{(x \mapsto 1) \lor \errstate}$ is a valid IL triple. Moreover, we may not be interested in the $x \mapsto 1$ outcomes since we already found an error, and for efficiency reasons a tool may want to drop it. IL accounts for this with its consequence rule, that allows to derive the triple $\iltriple{\true}{\regr}{\errstate}$.

	OL is able to do both, via the outcome conjunction $\oplus$ and the trivial outcome $\top$. The OL triple $\oltriple{\true}{\regr}{(x \mapsto 1) \oplus \errstate}$ is valid, stating the safety property that all reachable outcomes satisfy $(x \mapsto 1) \oplus \errstate$. However, it also tells a reachability property, namely that both $(x \mapsto 1)$ and $\errstate$ are reachable outcomes of the program. To account for under-approximation, OL can use its consequence rule to weaken the postcondition according to the implication $(x \mapsto 1) \oplus \errstate \implies \top \oplus \errstate$: therefore, the OL triple $\oltriple{\true}{\regr}{\top \oplus \errstate}$ is valid. Since $\top$ is a trivial assertion, satisfied by any outcome, this triple means that $\errstate$ is a reachable outcome, and other reachable outcomes (if any) are unconstrained because they only have to satisfy $\top$.
\end{example}

The proof system for OL~\cite[Figure~4]{ZDS23} is parametric in the chosen outcome monoid. The authors also show how to enrich this proof system with rules specific for probabilistic programs~\cite[Figure~7]{ZDS23} and heap-manipulating programs~\cite[Figure~6]{ZDS23}. This last instance is further explored in subsequent work~\cite{ZSS24}, which propose Outcome Separation Logic, a logic where the heap manipulation is baked into the logic itself and can be further composed with the outcome monoid to get, for instance, a probabilistic separation OL. This allows to explore a new tri-abduction algorithm and to prove a more general frame rule that is valid for any outcome monoid.\footnote{The OL instance already admitted a frame rule but it was limited to deterministic computations.}

Another result proved for OL is the ability to disprove triples within the logic itself. As show for instance in Theorem~\ref{th:sota:falsify-il-hl-topkat}, IL can disprove HL triples, i.e., prove that a given HL triple is not valid. However, neither HL nor IL can disprove triples of the same logic: no (set of) valid HL triple can show that an HL triple is not valid. Instead, OL is able to do just that. If we consider sets of elements of $M \Sigma$ instead of syntactic assertions (see Section~\ref{sec:bg:hl}), any OL triple is not valid if and only if some other OL triple is valid. Thus, encoding HL triples as OL ones, it is possible to disprove them within the logic. Moreover, this result is made syntactic (i.e., using formulae in an assertion language) for the nondeterministic and probabilistic instance of OL presented in the paper. Therefore, OL can use under-approximation to disprove over-approximation and vice versa.

We conclude pointing out that \cite{Zilberstein24} propose yet another generalization of OL (using weighted computations) for which they provide a complete proof system, at the cost of using semantics assertions (in that case, arbitrary weighted collections of elements of $\Sigma$) instead of syntactic ones.

\section{Summary}
In this chapter, we showed some works that exploits both over and under-approximation. They pinpoint symmetries as well as fundamental differences between the two, and combines them so that they help each other, in order to get the best out of both. We discussed an algebraic formulation that incorporate both, then three techniques - namely $\LCLA$, \code{ic3}/PDR and OL - which are able to exploit this combination in different and non trivial ways. In the next chapters, we first deepen our understanding of the relations between over and under-approximation, and then try to combine them in more effective ways.
