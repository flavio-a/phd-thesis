% !TEX root = ../phd-thesis.tex

\chapter{Introduction}\label{ch:intro}
Static program analyses are techniques used to infer properties of programs directly from their source code, without executing them. They have been studied and successfully applied for over 50 years to produce effective methods and tools to support the development of correct software. For instance, some of these tools include the Astr√©e static analyzer~\cite{BCCFMMMR03}, the SLAM model checker~\cite{BR01}, the certified C compiler CompCert~\cite{Leroy09}, and VCC using the calculus of weakest precondition to verify safety properties~\cite{CDHLMSST09}.
For all these years, the main focus of static analysis was to prove \emph{correctness} property of software such as, for instance, the absence of bugs or security requirements. To this end, static analyses compute \emph{over-approximations}, ie. supersets of all possible behaviours, of the semantics of programs: the absence of unwanted behaviour in the over-approximation guarantees the correctness of the program. However, over-approximation cannot be used to disprove correctness, eg. by exposing real bugs, since any alert raised by the analyser may be caused by the over-approximation rather than by the program: it can be a so called false alarm.

Hoare logic~\cite{Hoare69} is perhaps the first example of formal static analysis whose goal is proving the absence of errors, and indeed it is an over-approximation one.
%Maybe early works like this and influential opinions such as Dijkstra's renown quote ``\textit{Program testing can be used to show the presence of bugs, but never to show their absence!}" \cite{EWD249} directed the focus towards over-approximation.
It is possible that early works on over-approximation, together with influential opinions such as Dijkstra's renown quote ``\textit{Program testing can be used to show the presence of bugs, but never to show their absence!}" \cite{EWD249}, focused the theoretical interest on over-approximation.
However, from the point of view of a software developer, false alarms are undesirable because they undermine the credibility and usefulness of the analysis. Therefore, in practice static analysis technique are deployed to \emph{find true bugs} rather than proving their absence~\cite{BKY05}.
This idea gained mainstream interest with O'Hearn's introduction of Incorrectness Logic~\cite{OHearn20}. In his paper, he argues for the relevance of formal methods for bug catching, and more in general for disproving correctness -- in other words, proving \emph{incorrectness}. He proposes Incorrectness Logic, a dual version of Hoare logic thought from the ground up for scalable bug-finding, which serves as the theoretical basis for Pulse, a tool in production at Meta which catches thousands of bugs on programs with millions of lines of code~\cite{DFLO19}.
Incorrectness Logic computes an \emph{under-approximation}, ie. a subset of all possible behaviours of a program. Dually to over-approximation, under-approximation can then expose defects in the code, but it is unable to show their absence.
Given the recent interest in this idea (eg. \cite{RBDDOV20,RBDO22,LRVBDO22,CZKON22,Cousot24} and the first Workshop on ``Formal Methods for Incorrectness'' affiliated with POPL'24), we believe the field poses many interesting research questions that can improve the software development process in many ways.

\paragraph*{The problem.}
Even accounting for the rising interest in under-approximation techniques in the last years, ``\textit{there is a rich variety of problems [...] to bring the foundations of reasoning about program incorrectness onto a par with the extensively developed foundations for correctness}''~\cite{OHearn20}. To fill this gap, instead of starting from scratch, it seems reasonable to start from the vast literature on over-approximation techniques and try to recast it to the new under-approximation context. However, this process is not straightforward.

\noindent \textbf{Question Q1}: which of these techniques can be effectively transferred from the current over-approximation literature to under-approximation methodologies?

Other than a ``static'' transfer of techniques to derive new under-approximation methods from over-approximation ones, we envisage a more ``dynamic'' cooperation: an interesting research avenue lies in the possibility of combining over and under-approximation together, exploiting a synergic information transfer to improve on both. However, the details of this interaction are non-trivial.

\noindent \textbf{Question Q2}: how can we effectively combine the two paradigms?

\noindent \textbf{Question Q3}: how can the information produced by over-approximation help the precision and speed of an under-approximation analysis, and vice versa?

\paragraph*{Contributions.}
The goal of the thesis is to study the effect of over/under-approximation interaction.
The fundamental insight is that even partial information about correctness can help the search for an incorrectness proof and vice versa. This means that a technique employing both over and under-approximation at the same time can, in principle, be more effective than a disjoint application of the two for deciding correctness and incorrectness of a program.
However, designing an effective cooperation of over and under-approximation techniques is not a trivial task. It requires an instantiation for the over and under-approximation analysis, and insightful understanding of both to identify the kind of information that can be taken from one to help the other.
To this end, we first consider over and under-approximation separately to understand how they can effectively communicate: we do so by drawing analogies and differences between the two. Then, we consider some known approaches that combine over and under-approximation and try to extend them using the insight we gained from the previous study.

We first try to address Q1.
We consider the possibility of using \textbf{abstract interpretation} for under\hyp{}approximation. While in principle this should be an easy task thanks to duality, it turns out the apparent symmetry between over and under-approximation in not so sharp. Thus, defining an effective under-approximation abstract domain is a challenging task. We explore intuitive and technical reasons behind this, pointing out non-trivial asymmetries between over and under-approximation.
Given the difficulties in using abstract interpretation for under\hyp{}approximation, we then study \textbf{logical frameworks} instead, such as Incorrectness Logic. Particularly, we follow \emph{duality} considerations along two main axes--direction of the approximation and of the computation---to draw connections between known program logics. Moreover, this leads us to define the new \textbf{Sufficient Incorrectness Logic}, a \emph{backward under-approximation} logic intended to start from some output errors and track back their origins in the input. We study its properties and its extension to handle pointers and dynamic memory allocation.
Our investigation shows that some over-approximation techniques can be adapted easily to under-approximation, while other are less effective.

After identifying the formalisms we want to use for under-approximation, we focus on approaches that combines the two paradigms.

Addressing Q2, we extend \textbf{$\LCLA$}~\cite{BGGR23}, a technique that combines the under-approximating Incorrectness Logic and over-approximating abstract interpretation to ensure that the analysis, if it terminates, can decide \emph{both correctness and incorrectness} of a program. Particularly, we extend it in different ways to relax the hypothesis of the original formulation. First, we allow the use of \emph{different abstract domains} for different parts of the program, which let the logic prove all the properties that are true of the semantics of the program, \emph{irrespective of its syntax}. Second, we propose a framework to apply it even when there is \emph{no best abstraction} in the abstract domain (a feature that some practical domains lack). Third, we combine it with Sufficient Incorrectness Logic to obtain an $\LCLA$ that is suitable for \emph{backward analysis}.

For Q3, we introduce a new \textbf{PDR-like algorithm}~\cite{Bradley11}, which combines an over\hyp{}approximation of the initial Kleene chain with an under-approximation of a set of possible counterexamples to the safety of the chain. The novelty of our algorithm lies in the thorough use of \emph{adjoints}, both in the forward/backward duality that in the form of abstraction/concretization functions. Our approach yields a framework to understand and compare heuristics as a trade-off between precision and performances. Moreover, we propose and extension which relaxes some of the hypothesis, and we compare our algorithm with state-of-the-art tools.

These two results about $\LCLA$ and PDR provide examples of how over and under-approximation analysis can effectively share information, and shows that this interaction is non-trivial and requires thorough understanding of both approaches.

\paragraph*{Structure of the thesis.}
After giving some background (to be used as a reference) in Chapter~\ref{ch:background}, we examine other works that combine over and under-approximations in Chapter~\ref{ch:sota} to get a baseline.
Chapter~\ref{ch:uai} shows some difficulties in using abstract interpretation for under\hyp{}approximation for program analysis.
Chapter~\ref{ch:sil} proposes a duality-driven taxonomy of program logics and introduce Sufficient Incorrectness Logic and its extension to handle memory errors.
Chapter~\ref{ch:lcla} proposes various approaches to extend $\LCLA$ capabilities based on domain refinement and simplification.
Chapter~\ref{ch:pdr} details our new PDR-like algorithm and its experimentation.
We draw conclusions in Chapter~\ref{ch:conclusions}.
For presentation purposes not all proofs and technical details are included in the main text: we omit those whose details are tedious and not enlightening. However, all these proofs are included in full details in Appendices~\ref{ch:app:uai}--\ref{ch:app:pdr}.

\paragraph*{Origins of material.}
The work on under-approximation abstract domains was first published at the FoSSaCS conference~\cite{ABG22}, and its extended version appeared in the TOPLAS journal~\cite{ABG24}. The taxonomy of program logics and Sufficient Incorrectness Logic is under review at a conference, and is available on ArXiV~\cite{ABGL24}.
The chapter on $\LCLA$ extensions includes results from a paper presented at the ESOP conference~\cite{ABG23} and from its extended version currently submitted to a journal.
Our new PDR-like algorithms were first presented at the CAV conference~\cite{KABBGH23}, where the paper was selected as one of the distinguished papers of the conference. An extended version was invited to the conference special issue.
