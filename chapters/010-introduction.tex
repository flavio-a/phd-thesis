% !TEX root = ../phd-thesis.tex

\chapter{Introduction}\label{ch:intro}
\todo{Expand motivations}
Static program analyses are techniques used to infer properties of programs directly from their source code, without executing them. They have been studied and successfully applied for over 50 years to produce effective methods and tools to support the development of correct software.
For all these years, the main focus of static analysis was to prove \emph{correctness} property of software such as, for instance, the absence of bugs or a security requirement. To this end, static analyses compute \emph{over-approximations}, ie. supersets of all possible behaviours, of the semantics of programs: the absence of unwanted behaviour in the over-approximation guarantees the correctness of the program. However, over-approximation cannot be used to disprove correctness, eg. by exposing real bugs, since any alert raised by the analyser may be caused by the over-approximation rather than by the program: it can be a so called false alarm.

Hoare logic~\cite{Hoare69} is perhaps the first example of formal static analysis, and indeed is an over-approximation one, that correctly fits its goal of proving the absence of errors. Maybe early works like this and influential opinions such as Dijkstra's renown quote ``\textit{Program testing can be used to show the presence of bugs, but never to show their absence!}" \cite{EWD249} directed the focus towards over-approximation.
However, from the point of view of a software developer, false alarms are undesirable because they undermine the credibility and usefulness of the analysis.
Following this observation, O'Hearn recently argued for the relevance of formal methods for bug catching, and more in general for disproving correctness -- in other words, proving \emph{incorrectness}. With this in mind, he proposed Incorrectness Logic~\cite{OHearn20}, a dual version of Hoare logic thought from the ground up for scalable bug-finding.
Incorrectness Logic computes an \emph{under-approximation} of the semantics, ie. a subset of all possible behaviours of a program. Dually to over-approximation, under-approximation can then expose defects in the code, but it is unable to show their absence.
Given the recent introduction of this new idea, we believe the field has many interesting topics yet to study.

\fromhere
Roadmap: paragraph "the problem", with over + under motivation and stuff. Paragraph contribution: list what are the contributions, chapter by chapter.

When these two paradigms are taken separately, it is possible to either prove or disprove correctness. However, their combination proved to be very effective. The insight is that even partial information about correctness can help the search for an incorrectness proof and vice versa. The goal of the proposed research is then to study the effect of this interaction, especially with respect to abstract interpretation-based techniques.
Abstract interpretation \cite{CC77,RY20} is a general framework based on constructive approximations to define, compare and combine sound static analysis. Given its flexibility, it has been applied to many fields of computer science, such as program transformation, semantics and security \cite{CC14}. Because of this, we believe it can turn out as an effective tool also in this new setting of combining over- and under-approximations, which hasn't been fully explored yet.

This proposal is organized as follows. Chapter~\ref{ch:background} lays the background needed to understand subsequent chapters. Chapter~\ref{ch:sota} examines other works that combines over- and under-approximations: they are useful to draw a comparison with our initial results and take inspiration for future enhancements.
