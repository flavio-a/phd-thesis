% !TEX root = ../phd-thesis.tex

\chapter{Introduction}\label{ch:intro}
Static program analyses are techniques used to infer properties of programs directly from their source code, without executing them. They have been studied and successfully applied for over 50 years to produce effective methods and tools to support the development of correct software.
For all these years, the main focus of static analysis was to prove \emph{correctness} property of software such as, for instance, the absence of bugs or a security requirement. To this end, static analyses compute \emph{over-approximations}, ie. supersets of all possible behaviours, of the semantics of programs: the absence of unwanted behaviour in the over-approximation guarantees the correctness of the program. However, over-approximation cannot be used to disprove correctness, eg. by exposing real bugs, since any alert raised by the analyser may be caused by the over-approximation rather than by the program: it can be a so called false alarm.

Hoare logic~\cite{Hoare69} is perhaps the first example of formal static analysis, and indeed is an over-approximation one, that correctly fits its goal of proving the absence of errors. Maybe early works like this and influential opinions such as Dijkstra's renown quote ``\textit{Program testing can be used to show the presence of bugs, but never to show their absence!}" \cite{EWD249} directed the focus towards over-approximation.
However, from the point of view of a software developer, false alarms are undesirable because they undermine the credibility and usefulness of the analysis. Therefore, in practice static analysis technique are deployed to \emph{find true bugs} rather than proving their absence~\cite{BKY05}.
This idea gained mainstream interest with O'Hearn's introduction of Incorrectness Logic~\cite{OHearn20}. In his paper, he argues for the relevance of formal methods for bug catching, and more in general for disproving correctness -- in other words, proving \emph{incorrectness}. He proposes Incorrectness Logic, a dual version of Hoare logic thought from the ground up for scalable bug-finding, which serves as the theoretical basis for Pulse, a tool in production at Meta which catches thousands of bugs on programs with millions of lines of code~\cite{DFLO19}.
Incorrectness Logic computes an \emph{under-approximation}, ie. a subset of all possible behaviours of a program. Dually to over-approximation, under-approximation can then expose defects in the code, but it is unable to show their absence.
Given the recent interest in this idea, we believe the field has many interesting topics yet to study.

\paragraph*{The problem.}
Most past works focused on over-approximation to prove correctness. Some recent works focus on under-approximation to prove incorrectness. We believe an interesting avenue lies in the possibility of combining the two paradigms to improve both.
The fundamental insight is that even partial information about correctness can help the search for an incorrectness proof and vice versa. This means that a technique employing both at the same time can, in principle, be substantially more effective than a disjoint application of the two for deciding correctness and incorrectness of a program, and possibly even improving on just one for a single task (eg. being more effective at proving correctness than a tool based solely on over-approximation).
However, designing an effective cooperation of over and under-approximation techniques is not a trivial task. It requires an instantiation for the over and under-approximation analysis, and insightful understanding of both to identify the kind of information that can be taken from one to help the other.

\paragraph*{Contributions.}
The goal of the thesis is to study the effect of this interaction. To this end, we first consider over and under-approximation separately to understand how they can effectively communicate: we do so by drawing analogies and differences between the two. Then, we consider some known approaches that combine over and under-approximation and try to extend them using the insight we gained from the previous study.

After giving some background (to be used as a reference) in Chapter~\ref{ch:background}, we examine other works that combines over and under-approximations in Chapter~\ref{ch:sota} to get a baseline.

Chapter~\ref{ch:uai} consider the possibility of using Abstract Interpretation for under\hyp{}approximation. While in principle this should be an easy task thanks to duality, it turns out the apparent symmetry between over and under-approximation in not so sharp. Thus, defining an effective under-approximation abstract domain is a challenging task. In this chapter we explore intuitive and technical reasons for this, pointing out non-trivial asymmetries between over and under-approximation.

Since it is hard to use Abstract Interpretation for under\hyp{}approximation, in Chapter~\ref{ch:sil} we study logical frameworks instead, such as Incorrectness Logic. Particularly, we follow duality considerations along two main axes --direction of the approximation and of the computation-- to draw connections between known program logics. Moreover, this leads use to define the new Sufficient Incorrectness Logic, a backward under-approximation logic tailored to backward analysis, and study its properties and its extension to handle pointers and dynamic memory allocation.

After identifying the formalisms we want to use for under-approximation, in Chapter~\ref{ch:lcla} we extend $\LCLA$~\cite{BGGR23}, a technique that combines the under-approximating Incorrectness Logic and over-approximating Abstract Interpretation to ensure that the analysis, if it finishes, can decide both correctness and incorrectness of a program. Particularly, we extend it in different ways to relax the hypothesis of the original formulation. First, we allow the use of different abstract domains for different parts of the program, which let the logic prove all the properties that are true of the semantics of the program (irrespective of its syntax). Second, we propose a framework to apply it even when there is no best abstraction in the abstract domain (a feature that some practical domains lack). Third, we combine it with Sufficient Incorrectness Logic to obtain an $\LCLA$ that is suitable for backward analysis.

Chapter~\ref{ch:pdr} discuss a new PDR-like algorithm~\cite{Bradley11}, which combines an over\hyp{}approximation of the initial Kleene chain with an under-approximation of a set of possible counterexamples to the safety of the chain. The novelty of our algorithm lies in the thorough use of adjoints, both in the forward/backward duality that in the form of abstraction/concretization functions. Our approach yields a framework to understand and compare heuristics. Moreover, we propose and extension which relaxes some of the hypothesis, and we compare our algorithm with other tools.

We draw conclusions in Chapter~\ref{ch:conclusions}. Not all proofs and technical details are included in the main text for presentation purposes: we omit those whose details are tedious and not enlightening. However, all these proofs are included in full details in Appendices~\ref{ch:app:uai}--\ref{ch:app:pdr}.
